<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TensorRT(6)-INT8 inference]]></title>
    <url>%2Farticles%2F95d15d89%2F</url>
    <content type="text"><![CDATA[这一节通过官方例程 介绍 INT8 inference mode. 例程位于 /usr/src/tensorrt/samples/sampleINT8 ，是基于mnist的，大体流程是一致的。 流程同样是 build(Calibration )-&gt;deploy，只不过在build时多了一个校准的操作。 注意以下几点： 1 网络定义定义网络时，注意这个地方传进去的dataType，如果使用FP16 inference 则传进去的是FP16，也就是kHALF；但如果是使用INT8 inference的话，这个地方传进去的是kFLOAT，也就是 FP32，这是因为INT8 需要先用FP32的精度来确定转换系数，TensorRT自己会在内部转换成INT8。 12345const IBlobNameToTensor* blobNameToTensor = parser-&gt;parse(locateFile(deployFile).c_str(), locateFile(modelFile).c_str(), *network, DataType::kFLOAT); 这个看起来就跟使用FP32是一样的流程，INT8 MODE inference的输入和输出都是 FP32的。 （After the network has been built, it can be used just like an FP32 network, for example, inputs and outputs remain in 32-bit floating point.） 2 校准网络-Calibrating The Network校准网络时，比较麻烦的是校准集的构建，作者定义了一个BatchStream class来完成这个操作。BatchStream类有个成员函数getBatch ()是为了依次读取 batch file 中的数据的。 还有个校准类 Int8EntropyCalibrator，继承自 NvInfer.h 中的 IInt8EntropyCalibrator 1class Int8EntropyCalibrator : public IInt8EntropyCalibrator 这个类里面也有个 getBatch () 成员函数，实际上调用的是 BatchStream类的getBatch () ，然后将数据从内存搬到了显存，如下： 12345678910bool getBatch(void* bindings[], const char* names[], int nbBindings) override&#123; if (!mStream.next()) return false; CHECK(cudaMemcpy(mDeviceInput, mStream.getBatch(), mInputCount * sizeof(float), cudaMemcpyHostToDevice)); assert(!strcmp(names[0], INPUT_BLOB_NAME)); bindings[0] = mDeviceInput; return true;&#125; 这个getBatch () 成员函数在校准时会被反复调用。 生成校准集时，校准集的样本应该是已经进行过一系列预处理的图片而不是原始图片。 校准类 Int8EntropyCalibrator 和 BatchStream 类的实现说起来比较麻烦，在后面源码解读部分直接结合注释看源码吧。 3 builder的配置-Configuring The Builder只需要在原来builder的基础上添加以下： 12builder-&gt;setInt8Mode(true);builder-&gt;setInt8Calibrator(calibrator); 4 batch file的生成-Batch Files For Calibration例程使用的batch file 已经制作好了，位于&lt;TensorRT&gt;/data/mnist/batches 这是一系列二进制文件，每个文件包含了 N 个图片样本，格式如下： 首先是4个32 bit的整形值，代表 {N, C, H, W},batchsize和图片dims 然后是N个 {C, H, W}维度的浮点数据，代表N个样本 batch file二进制文件的生成有两种方式： 4.1 使用caffe生成主要对于使用caffe的用户，这里干脆直接将官方文档上的说明拷贝过来好了，比较简单： Navigate to the samples data directory and create an INT8 mnist directory: 1234&gt; cd &lt;TensorRT&gt;/samples/data&gt; mkdir -p int8/mnist&gt; cd int8/mnist&gt; &gt; Note: If Caffe is not installed anywhere, ensure you clone, checkout, patch, and build Caffe at the specific commit: 12345678910&gt; git clone https://github.com/BVLC/caffe.git&gt; cd caffe&gt; git checkout 473f143f9422e7fc66e9590da6b2a1bb88e50b2f&gt; patch -p1 &lt; &lt;TensorRT&gt;/samples/mnist/int8_caffe.patch&gt; mkdir build&gt; pushd build&gt; cmake -DUSE_OPENCV=FALSE -DUSE_CUDNN=OFF ../&gt; make -j4&gt; popd&gt; &gt; Download the mnist dataset from Caffe and create a link to it: 12345&gt; bash data/mnist/get_mnist.sh&gt; bash examples/mnist/create_mnist.sh&gt; cd .. &gt; ln -s caffe/examples .&gt; &gt; Set the directory to store the batch data, execute Caffe, and link the mnist files: 1234567&gt; mkdir batches&gt; export TENSORRT_INT8_BATCH_DIRECTORY=batches&gt; caffe/build/tools/caffe test -gpu 0 -iterations 1000 -model examples/mnist/lenet_train_test.prototxt -weights&gt; &lt;TensorRT&gt;/samples/mnist/mnist.caffemodel&gt; ln -s &lt;TensorRT&gt;/samples/mnist/mnist.caffemodel .&gt; ln -s &lt;TensorRT&gt;/samples/mnist/mnist.prototxt .&gt; &gt; Execute sampleINT8 from the bin directory after being built with the following command: 12&gt; ./sample_int8 mnist&gt; 4.2 其他方式生成对于不用caffe或者模型难以转换成caffemode的用户，首先要进行一系列预处理，然后按照前面提到的batch file格式生成二进制batch file文件，但这个生成过程要自己写了，不过写的话应该也比较简单，可以参考caffe中的patch文件中的核心部分： 12345678910111213141516171819#define LOG_BATCHES_FOR_INT8_TESTING 1#if LOG_BATCHES_FOR_INT8_TESTING static int sBatchId = 0; char* batch_dump_dir = getenv("TENSORRT_INT8_BATCH_DIRECTORY"); if(batch_dump_dir != 0) &#123; char buffer[1000]; sprintf(buffer, "batches/batch%d", sBatchId++); FILE* file = fopen(buffer, "w"); if(file==0) abort(); int s[4] = &#123; top_shape[0], top_shape[1], top_shape[2], top_shape[3] &#125;; fwrite(s, sizeof(int), 4, file); fwrite(top_data, sizeof(float), top_shape[0]*top_shape[1]*top_shape[2]*top_shape[3], file); fwrite(&amp;top_label[0], sizeof(int), top_shape[0], file); fclose(file); &#125;+#endif 添加上数据集的读取，划分和预处理就可以了。 5 校准算法从INT8的例程来看，TensorRT 支持两种方式的校准，一种就是上节我们讲过的使用相对熵的方式，还有一种是废弃的校准算法，校准时需要设置两个参数 cutoff 和 quantile，以下是 在GTC2017 上对INT8校准原理进行讲解的 Szymon Migacz 对废弃的校准算法的解读： https://devtalk.nvidia.com/default/topic/1015108/cutoff-and-quantile-parameters-in-tensorrt/ Parameters cutoff and quantile have to be specified only for “legacy” calibrator. It’s difficult to set values of cutoff and quantile without running experiments. Our recommended way was to run 2D grid search and look for optimal combination of (cutoff, quantile) for a given network on a given dataset. This was implemented in sampleINT8 shipped with TensorRT 2.0 EA. New entropy calibrator doesn’t require any external hyperparameters, and it determines quantization thresholds automatically based on the distributions of activations on calibration dataset. In my presentation at GTC I was talking only about the new entropy calibrator, it’s available in TensorRT 2.1 GA. Szymon Migacz并没有充分的解释这两个参数，而是说这是 “legacy” calibrator中才会用到的参数，而且在没有做充分的试验的情况下，是很难合理地设置这两个参数的。他推荐的做法是 针对特定的网络结构和数据集使用 2D 网格搜索 来确定这两个参数的取值。而 entropy calibrator ，就是使用相对熵的校准方法，不需要任何超参数，而且能够根据校准集上的激活值分布自动确定量化阈值。NVIDIA官方也推荐使用使用相对熵校准的方式。所以 “legacy” calibrator 就不深入研究了。 6 源码解读sampleINT8.cpp: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475#include &lt;assert.h&gt;#include &lt;fstream&gt;#include &lt;sstream&gt;#include &lt;iostream&gt;#include &lt;cmath&gt;#include &lt;sys/stat.h&gt;#include &lt;cmath&gt;#include &lt;time.h&gt;#include &lt;cuda_runtime_api.h&gt;#include &lt;unordered_map&gt;#include &lt;algorithm&gt;#include &lt;float.h&gt;#include &lt;string.h&gt;#include &lt;chrono&gt;#include &lt;iterator&gt;#include "NvInfer.h"#include "NvCaffeParser.h"#include "common.h"#include "BatchStream.h"#include "LegacyCalibrator.h"using namespace nvinfer1;using namespace nvcaffeparser1;static Logger gLogger;// stuff we know about the network and the caffe input/output blobsconst char* INPUT_BLOB_NAME = "data";const char* OUTPUT_BLOB_NAME = "prob";const char* gNetworkName&#123;nullptr&#125;;std::string locateFile(const std::string&amp; input)&#123; std::vector&lt;std::string&gt; dirs; dirs.push_back(std::string("data/int8/") + gNetworkName + std::string("/")); dirs.push_back(std::string("data/") + gNetworkName + std::string("/")); return locateFile(input, dirs);&#125;bool caffeToTRTModel(const std::string&amp; deployFile, // name for caffe prototxt const std::string&amp; modelFile, // name for model const std::vector&lt;std::string&gt;&amp; outputs, // network outputs unsigned int maxBatchSize, // batch size - NB must be at least as large as the batch we want to run with) DataType dataType, IInt8Calibrator* calibrator, nvinfer1::IHostMemory *&amp;trtModelStream)&#123; //创建一个builder，传入自己实现的 gLogger 对象，为了打印信息用 // create the builder IBuilder* builder = createInferBuilder(gLogger); //创建一个 network 对象，并创建一个 ICaffeParser 对象，这个对象是用来进行模型转换的；此时的 network 对象里面还是空的 // parse the caffe model to populate the network, then set the outputs INetworkDefinition* network = builder-&gt;createNetwork(); ICaffeParser* parser = createCaffeParser(); //判断当前的硬件平台是否支持 INT8 精度和 FP16 精度，两者都不支持的话，直接返回 false if((dataType == DataType::kINT8 &amp;&amp; !builder-&gt;platformHasFastInt8()) || (dataType == DataType::kHALF &amp;&amp; !builder-&gt;platformHasFastFp16())) return false; // caffemodel到tensorrt的转换, 注意这个地方传进去的dataType， // 如果使用FP16 inference 则传进去的是FP16，也就是kHALF // 如果是使用INT8 inference的话，这个地方传进去的是kFLOAT也就是 FP32, // 因为INT8 需要先用FP32的精度来确定转换系数，TensorRT自己会在内部转换成INT8 const IBlobNameToTensor* blobNameToTensor = parser-&gt;parse(locateFile(deployFile).c_str(), locateFile(modelFile).c_str(), *network, dataType == DataType::kINT8 ? DataType::kFLOAT : dataType); //标志输出tensor // specify which tensors are outputs for (auto&amp; s : outputs) network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str())); // Build the engine // 设置最大 batchsize和工作空间大小 2^30 ,这里是1G builder-&gt;setMaxBatchSize(maxBatchSize); builder-&gt;setMaxWorkspaceSize(1 &lt;&lt; 30); // 设置平均迭代次数和最小迭代次数，这是测量每一层时间的一种策略，即多次迭代求平均值，不过这里只迭代一次 builder-&gt;setAverageFindIterations(1); builder-&gt;setMinFindIterations(1); //同步调试 builder-&gt;setDebugSync(true); //INT8 MODE or/and FP16 MODE builder-&gt;setInt8Mode(dataType == DataType::kINT8); builder-&gt;setFp16Mode(dataType == DataType::kHALF); //设置INT8校准接口 builder-&gt;setInt8Calibrator(calibrator); // 创建engine ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); assert(engine); //销毁无用对象 // we don't need the network any more, and we can destroy the parser network-&gt;destroy(); parser-&gt;destroy(); //序列化到磁盘上，这里实际上是在内存中，没有保存到磁盘 // serialize the engine, then close everything down trtModelStream = engine-&gt;serialize(); engine-&gt;destroy(); builder-&gt;destroy(); return true;&#125;float doInference(IExecutionContext&amp; context, float* input, float* output, int batchSize)&#123; //从context恢复engine const ICudaEngine&amp; engine = context.getEngine(); //创建engine的时候，会把输入blob和输出blob指针放进去，engine.getNbBindings() 就是为了获取输入和输出的blob数目，以便于做检查 //比如这里，就只有一个输入和一个输出，所以 检查时可以这样检查 assert(engine.getNbBindings() == 2); // input and output buffer pointers that we pass to the engine - the engine requires exactly IEngine::getNbBindings(), // of these, but in this case we know that there is exactly one input and one output. assert(engine.getNbBindings() == 2); //每个输入和输出blob都需要申请显存，故：void* buffers[engine.getNbBindings()]; void* buffers[2]; float ms&#123; 0.0f &#125;; //为了将 buffer中的成员(指针或者地址)分别与输入/输出的blob相关联，需要分别获取输入输出blob在engine中的索引 // In order to bind the buffers, we need to know the names of the input and output tensors. // note that indices are guaranteed to be less than IEngine::getNbBindings() int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME), outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME); //计算输入输出shape // create GPU buffers and a stream Dims3 inputDims = static_cast&lt;Dims3&amp;&amp;&gt;(context.getEngine().getBindingDimensions(context.getEngine().getBindingIndex(INPUT_BLOB_NAME))); Dims3 outputDims = static_cast&lt;Dims3&amp;&amp;&gt;(context.getEngine().getBindingDimensions(context.getEngine().getBindingIndex(OUTPUT_BLOB_NAME))); //计算实际的输入输出大小,申请显存 size_t inputSize = batchSize*inputDims.d[0]*inputDims.d[1]*inputDims.d[2] * sizeof(float), outputSize = batchSize * outputDims.d[0] * outputDims.d[1] * outputDims.d[2] * sizeof(float); CHECK(cudaMalloc(&amp;buffers[inputIndex], inputSize)); CHECK(cudaMalloc(&amp;buffers[outputIndex], outputSize)); //从Host (CPU) 拷贝输入数据到 Device(GPU)，也就是从内存到显存 CHECK(cudaMemcpy(buffers[inputIndex], input, inputSize, cudaMemcpyHostToDevice)); //创建一个 cuda 异步流 cudaStream_t stream; CHECK(cudaStreamCreate(&amp;stream)); //创建一个cuda事件 cudaEvent_t start, end; CHECK(cudaEventCreateWithFlags(&amp;start, cudaEventBlockingSync)); CHECK(cudaEventCreateWithFlags(&amp;end, cudaEventBlockingSync)); //标记stream流，start cudaEventRecord(start, stream); //异步执行inference，//标记stream流，end context.enqueue(batchSize, buffers, stream, nullptr); cudaEventRecord(end, stream); //事件同步 cudaEventSynchronize(end); //计算start事件和end事件之间的运行时间 cudaEventElapsedTime(&amp;ms, start, end); //销毁事件 cudaEventDestroy(start); cudaEventDestroy(end); //从Device(GPU) 拷贝输出数据到 Host (CPU)，也就是从显存到内存 CHECK(cudaMemcpy(output, buffers[outputIndex], outputSize, cudaMemcpyDeviceToHost)); //释放显存 CHECK(cudaFree(buffers[inputIndex])); CHECK(cudaFree(buffers[outputIndex])); //销毁流对象 CHECK(cudaStreamDestroy(stream)); //返回inference时间 return ms;&#125;//计算一个batch 中 top-1或top-5的正确的图片数量//对于输出来说，一张图片的输出对应一个 outputSize 维的向量（比如mnist是10维的）//然而对于标签来说一张图片的标签是一个0-9之间的数字//batchProb是一个batch中的标签向量按顺序叠加到一个vector中的，10个数字一组对应一张图片//label就这这个batch的标签向量，一个数字对应一张图片//outputsize是输出维度（比如mnist的outputsize=10）//threshold：两个取值：1，对应top-1；5对应top-5int calculateScore(float* batchProb, float* labels, int batchSize, int outputSize, int threshold)&#123; int success = 0; for (int i = 0; i &lt; batchSize; i++) &#123; //获取每个batch的地址，并获取预测向量中与标签相同位置上的真实概率 //举个例子：假设threshold=1 //i=0时，prob[0]-prob[9]是batch中的第一张图片的预测输出向量， //假设prob[0]-prob[9]的值为&#123;0.1, 0.5, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05&#125;，这张图片的label是1. //那么correct = prob[(int)labels[i]]=prob[1]=0.5，之后判断的是这个correct是否在top-1或者top-5范围内 //做法是：统计 prob[0]-prob[9]之间比correct更大的值的个数 better，因为如果比correct大的话，最终输出的肯定是错的预测结果； //但是由于top-1，top-5允许你出错的次数分别为1次和5次，所以只要 better &lt; threshold，就认为预测准确，success++； //最后返回success，代表这个batch中按照 top-1 或 top-5的精度来算，预测对了几张图片。 float* prob = batchProb + outputSize*i, correct = prob[(int)labels[i]]; int better = 0; for (int j = 0; j &lt; outputSize; j++) if (prob[j] &gt;= correct) better++; if (better &lt;= threshold) success++; &#125; return success;&#125;class Int8EntropyCalibrator : public IInt8EntropyCalibrator&#123;public: Int8EntropyCalibrator(BatchStream&amp; stream, int firstBatch, bool readCache = true) : mStream(stream), mReadCache(readCache) &#123; DimsNCHW dims = mStream.getDims(); mInputCount = mStream.getBatchSize() * dims.c() * dims.h() * dims.w(); //为 mDeviceInput 申请显存，跳过前面 firstBatch 个batch CHECK(cudaMalloc(&amp;mDeviceInput, mInputCount * sizeof(float))); mStream.reset(firstBatch); &#125; /** * 析构函数，释放显存 */ virtual ~Int8EntropyCalibrator() &#123; CHECK(cudaFree(mDeviceInput)); &#125; int getBatchSize() const override &#123; return mStream.getBatchSize(); &#125; bool getBatch(void* bindings[], const char* names[], int nbBindings) override &#123; if (!mStream.next()) return false; //将mStream.getBatch()获取到的数据拷贝到 mDeviceInput 中，也就是从内存到显存 CHECK(cudaMemcpy(mDeviceInput, mStream.getBatch(), mInputCount * sizeof(float), cudaMemcpyHostToDevice)); assert(!strcmp(names[0], INPUT_BLOB_NAME)); bindings[0] = mDeviceInput; return true; &#125; /** * 从文件中读取校准数据，返回校准表缓存地址 * @param length 读取长度 */ const void* readCalibrationCache(size_t&amp; length) override &#123; //首先清空mCalibrationCache mCalibrationCache.clear(); //从文件中读取内容并放到 mCalibrationCache vector中 std::ifstream input(calibrationTableName(), std::ios::binary); input &gt;&gt; std::noskipws; if (mReadCache &amp;&amp; input.good()) std::copy(std::istream_iterator&lt;char&gt;(input), std::istream_iterator&lt;char&gt;(), std::back_inserter(mCalibrationCache)); //返回 mCalibrationCache 地址或 空指针 length = mCalibrationCache.size(); return length ? &amp;mCalibrationCache[0] : nullptr; &#125; /** * 将校准数据存储到文件中 * @param cache 校准数据内存地址 * @param length 数据长度 */ void writeCalibrationCache(const void* cache, size_t length) override &#123; std::ofstream output(calibrationTableName(), std::ios::binary); output.write(reinterpret_cast&lt;const char*&gt;(cache), length); &#125;private: /** * 存储校准数据的文件 * @return 文件名称 */ static std::string calibrationTableName() &#123; assert(gNetworkName); return std::string("CalibrationTable") + gNetworkName; &#125; //batch流 BatchStream mStream; //是否从文件中读取校准数据 bool mReadCache&#123; true &#125;; //校准时 GPU接受 的 数据量mInputCount 和 数据内容 mDeviceInput size_t mInputCount; void* mDeviceInput&#123; nullptr &#125;; //存放从文件中读取到的校准数据，也就是scale_factor 缩放系数 std::vector&lt;char&gt; mCalibrationCache;&#125;;/** * 用于模型评分，包含了caffe模型向ensorRT的转化以及inference的执行 * @param batchSize 批尺寸 * @param firstBatch 跳过初始的一些batch * @param nbScoreBatches 测试的 batch总数 * @param datatype 以何种精度inference * @param calibrator 校准接口 * @param quiet 是否输出调试信息 */std::pair&lt;float, float&gt; scoreModel(int batchSize, int firstBatch, int nbScoreBatches, DataType datatype, IInt8Calibrator* calibrator, bool quiet = false)&#123; IHostMemory *trtModelStream&#123; nullptr &#125;; // 调用 caffeToTRTModel 将caffe模型解析为TensorRT bool valid = false; if (gNetworkName == std::string("mnist")) valid = caffeToTRTModel("deploy.prototxt", "mnist_lenet.caffemodel", std::vector &lt; std::string &gt; &#123; OUTPUT_BLOB_NAME &#125;, batchSize, datatype, calibrator, trtModelStream); else valid = caffeToTRTModel("deploy.prototxt", std::string(gNetworkName) + ".caffemodel", std::vector &lt; std::string &gt; &#123; OUTPUT_BLOB_NAME &#125;, batchSize, datatype, calibrator, trtModelStream); // 如果GPU不支持某种精度类型，比如FP16/INT8，则返回（0,0） if(!valid) &#123; std::cout &lt;&lt; "Engine could not be created at this precision" &lt;&lt; std::endl; return std::pair&lt;float, float&gt;(0,0); &#125; assert(trtModelStream != nullptr); // 恢复创建engine，创建上下文环境 // Create engine and deserialize model. IRuntime* infer = createInferRuntime(gLogger); assert(infer != nullptr); ICudaEngine* engine = infer-&gt;deserializeCudaEngine(trtModelStream-&gt;data(), trtModelStream-&gt;size(), nullptr); assert(engine != nullptr); trtModelStream-&gt;destroy(); IExecutionContext* context = engine-&gt;createExecutionContext(); assert(context != nullptr); //创建 batch 流对象，并跳过开始的一些batch，共firstBatch个，此处等于100 BatchStream stream(batchSize, nbScoreBatches); stream.skip(firstBatch); // output tensor 维度 Dims3 outputDims = static_cast&lt;Dims3&amp;&amp;&gt;(context-&gt;getEngine().getBindingDimensions(context-&gt;getEngine().getBindingIndex(OUTPUT_BLOB_NAME))); //确定输出 tensor 数据量大小 int outputSize = outputDims.d[0]*outputDims.d[1]*outputDims.d[2]; int top1&#123; 0 &#125;, top5&#123; 0 &#125;; float totalTime&#123; 0.0f &#125;; //每张图片都有一个 outputSize 大小的向量(比如 mnist 分类大小为10)，那么一个batch的输出应该为 batchSize * outputSize std::vector&lt;float&gt; prob(batchSize * outputSize, 0); //依次对不同的batch进行inference，stream.next()获取下一个batch while (stream.next()) &#123; //输入数据：stream.getBatch()，输出数据：prob 每循环一次就对一个batch的数据进行测试，这个batch的输出放在 prob 中 totalTime += doInference(*context, stream.getBatch(), &amp;prob[0], batchSize); //对每个batch，按照top-1和top-5精度来计算准确率 top1 += calculateScore(&amp;prob[0], stream.getLabels(), batchSize, outputSize, 1); top5 += calculateScore(&amp;prob[0], stream.getLabels(), batchSize, outputSize, 5); //读取10个batch输出一个点，读取800个输出一个换行符 std::cout &lt;&lt; (!quiet &amp;&amp; stream.getBatchesRead() % 10 == 0 ? "." : "") &lt;&lt; (!quiet &amp;&amp; stream.getBatchesRead() % 800 == 0 ? "\n" : "") &lt;&lt; std::flush; &#125; //统计总共读到了多少张图片，并计算top-1和top-5正确率 int imagesRead = stream.getBatchesRead()*batchSize; float t1 = float(top1) / float(imagesRead), t5 = float(top5) / float(imagesRead); // 精度和时间，结果输出 if (!quiet) &#123; std::cout &lt;&lt; "\nTop1: " &lt;&lt; t1 &lt;&lt; ", Top5: " &lt;&lt; t5 &lt;&lt; std::endl; std::cout &lt;&lt; "Processing " &lt;&lt; imagesRead &lt;&lt; " images averaged " &lt;&lt; totalTime / imagesRead &lt;&lt; " ms/image and " &lt;&lt; totalTime / stream.getBatchesRead() &lt;&lt; " ms/batch." &lt;&lt; std::endl; &#125; //销毁无用对象，返回准确率 context-&gt;destroy(); engine-&gt;destroy(); infer-&gt;destroy(); return std::make_pair(t1, t5);&#125;int main(int argc, char** argv)&#123; if (argc &lt; 2) &#123; std::cout &lt;&lt; "Please provide the network as the first argument." &lt;&lt; std::endl; exit(0); &#125; gNetworkName = argv[1]; //前 firstScoreBatch 个 batch是用来作为校准集的，因此在测试时这些是不进行测试的 int batchSize = 100, firstScoreBatch = 100, nbScoreBatches = 400; // by default we score over 40K images starting at 10000, so we don't score those used to search calibration //search变量是LEGACY_CALIBRATION校准算法中使用的变量，具体作用要看 LegacyCalibrator.h 源码，因为这个校准算法nvidia已经不推荐使用了，所以这里不深究了 bool search = false; //校准算法 选择参考 Nvinfer.h 文件，kENTROPY_CALIBRATION：使用信息熵进行校准；kLEGACY_CALIBRATION，使用以前遗留下来的算法进行校准 // enum class CalibrationAlgoType : int // &#123; // kLEGACY_CALIBRATION = 0, // kENTROPY_CALIBRATION = 1 // &#125;; CalibrationAlgoType calibrationAlgo = CalibrationAlgoType::kENTROPY_CALIBRATION; // 处理命令行参数 for (int i = 2; i &lt; argc; i++) &#123; if (!strncmp(argv[i], "batch=", 6)) batchSize = atoi(argv[i] + 6); else if (!strncmp(argv[i], "start=", 6)) firstScoreBatch = atoi(argv[i] + 6); else if (!strncmp(argv[i], "score=", 6)) nbScoreBatches = atoi(argv[i] + 6); else if (!strncmp(argv[i], "search", 6)) search = true; else if (!strncmp(argv[i], "legacy", 6)) calibrationAlgo = CalibrationAlgoType::kLEGACY_CALIBRATION; else &#123; std::cout &lt;&lt; "Unrecognized argument " &lt;&lt; argv[i] &lt;&lt; std::endl; exit(0); &#125; &#125; if (calibrationAlgo == CalibrationAlgoType::kENTROPY_CALIBRATION) &#123; search = false; &#125; //batchsize不能大于128，这是为何？ if (batchSize &gt; 128) &#123; std::cout &lt;&lt; "Please provide batch size &lt;= 128" &lt;&lt; std::endl; exit(0); &#125; //感觉这里写错了，应该是 50000 if ((firstScoreBatch + nbScoreBatches)*batchSize &gt; 500000) &#123; std::cout &lt;&lt; "Only 50000 images available" &lt;&lt; std::endl; exit(0); &#125; //设置标准输出流输出的精度 std::cout.precision(6); //用于构建校准集的batch流 //CAL_BATCH_SIZE = 50;NB_CAL_BATCHES = 10; 定义在 LegacyCalibrator.h文件中, 既然废弃了 LegacyCalibrator，为什么不把常量定义在本文件中 BatchStream calibrationStream(CAL_BATCH_SIZE, NB_CAL_BATCHES); //FP32精度不需要校准集，因此最后一个参数传入 nullptr std::cout &lt;&lt; "\nFP32 run:" &lt;&lt; nbScoreBatches &lt;&lt; " batches of size " &lt;&lt; batchSize &lt;&lt; " starting at " &lt;&lt; firstScoreBatch &lt;&lt; std::endl; scoreModel(batchSize, firstScoreBatch, nbScoreBatches, DataType::kFLOAT, nullptr); //FP16精度不需要校准集，因此最后一个参数传入 nullptr std::cout &lt;&lt; "\nFP16 run:" &lt;&lt; nbScoreBatches &lt;&lt; " batches of size " &lt;&lt; batchSize &lt;&lt; " starting at " &lt;&lt; firstScoreBatch &lt;&lt; std::endl; scoreModel(batchSize, firstScoreBatch, nbScoreBatches, DataType::kHALF, nullptr); std::cout &lt;&lt; "\nINT8 run:" &lt;&lt; nbScoreBatches &lt;&lt; " batches of size " &lt;&lt; batchSize &lt;&lt; " starting at " &lt;&lt; firstScoreBatch &lt;&lt; std::endl; if (calibrationAlgo == CalibrationAlgoType::kENTROPY_CALIBRATION) &#123; //先构建校准集，然后调用scoreModel进行模型评估，创建engine时传入了Int8EntropyCalibrator对象calibrator //FIRST_CAL_SCORE_BATCH = 100; 定义在 LegacyCalibrator.h文件中 Int8EntropyCalibrator calibrator(calibrationStream, FIRST_CAL_BATCH); scoreModel(batchSize, firstScoreBatch, nbScoreBatches, DataType::kINT8, &amp;calibrator); &#125; else &#123; //被废弃的校准算法，不解释了 std::pair&lt;double, double&gt; parameters = getQuantileAndCutoff(gNetworkName, search); Int8LegacyCalibrator calibrator(calibrationStream, FIRST_CAL_BATCH, parameters.first, parameters.second); scoreModel(batchSize, firstScoreBatch, nbScoreBatches, DataType::kINT8, &amp;calibrator); &#125; shutdownProtobufLibrary(); return 0;&#125; BatchStream.h，这个源码看起来还是稍微有点费劲的，还是我C++功底不够啊，得补。。。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173#ifndef BATCH_STREAM_H#define BATCH_STREAM_H#include &lt;vector&gt;#include &lt;assert.h&gt;#include &lt;algorithm&gt;#include "NvInfer.h"std::string locateFile(const std::string&amp; input);class BatchStream&#123;public: //构造函数，使用 batchSize 和 maxBatches 初始化 BatchStream 中的 mBatchSize(批尺寸) 和 mMaxBatches(批数量) BatchStream(int batchSize, int maxBatches) : mBatchSize(batchSize), mMaxBatches(maxBatches) &#123; //读取第一个batch文件的shape，用于一系列初始化操作 FILE* file = fopen(locateFile(std::string("batches/batch0")).c_str(), "rb"); int d[4]; fread(d, sizeof(int), 4, file); mDims = nvinfer1::DimsNCHW&#123; d[0], d[1], d[2], d[3] &#125;; fclose(file); //单张图片的大小（总的像素个数） mImageSize = mDims.c()*mDims.h()*mDims.w(); //根据batch文件中的单张图片大小mImageSize初始化 BatchStream 中的 mBatch 的内存空间，初值为0；同理根据mBatchSize初始化mLabels //mBatch指的是BatchStream中的batch，batch的个数为mBatchSize，所以数据量总数为mBatchSize*mImageSize， //mLabels是BatchStream中的label，总数就是 mBatchSize mBatch.resize(mBatchSize*mImageSize, 0); mLabels.resize(mBatchSize, 0); //有两块专门的内存区域用于存储读取到的batch&#123;i&#125; 文件内容，就是下面两个。这两块内存区域里的内容在后面会被复制到 mBatch和mLabels中 //mFileBatch指的是读取到的 batch&#123;i&#125; 文件中的batch，因此总数为mDims.n()*mDims.c()*mDims.h()*mDims.w()=mDims.n()*mImageSize //mFileLabels指的是读取到的 batch&#123;i&#125; 文件中的label，因此总数为 mDims.n() mFileBatch.resize(mDims.n()*mImageSize, 0); mFileLabels.resize(mDims.n(), 0); reset(0); &#125; // reset操作 void reset(int firstBatch) &#123; mBatchCount = 0; mFileCount = 0; mFileBatchPos = mDims.n(); skip(firstBatch); &#125; /** * stream.next()每调用一次，就使用batch file中的数据(读取后首先是变量名为mFileBatch的buffer)填充一个mBatch * @return 是否填充成功 */ bool next() &#123; //已经读取到 最大 批数量 了，返回false if (mBatchCount == mMaxBatches) return false; // 将mFileBatch（相当于buffer）中的内容拷贝到mBatch中， //由于mFileBatch和mBatch大小有可能不一样,所以才这么写 for (int csize = 1, batchPos = 0; batchPos &lt; mBatchSize; batchPos += csize, mFileBatchPos += csize) &#123; assert(mFileBatchPos &gt; 0 &amp;&amp; mFileBatchPos &lt;= mDims.n()); //调用update函数，读取batches文件夹中的 batch&#123;i&#125; 文件，读取失败的话直接在这里返回false， //调用update函数会使 mFileBatchPos=0，这是合理的，因为还没有开始往 mBatch 拷贝数据 if (mFileBatchPos == mDims.n() &amp;&amp; !update()) return false; //一次从batch文件中读取 csize 张图片， //由于mFileBatch和mBatch大小有可能不一样所以借助 mFileBatchPos 和 batchpos 来指示batch文件和mbatch中的当前操作(读取或存储)位置 //所以csize取二者之间较小值 // copy the smaller of: elements left to fulfill the request, or elements left in the file buffer. csize = std::min(mBatchSize - batchPos, mDims.n() - mFileBatchPos); //将 mFileBatch 和 mFileLabels 中存放的batch文件的内容复制到 mBatch 和 mLabels 中 std::copy_n(getFileBatch() + mFileBatchPos * mImageSize, csize * mImageSize, getBatch() + batchPos * mImageSize); std::copy_n(getFileLabels() + mFileBatchPos, csize, getLabels() + batchPos); &#125; // mBatchCount自增，指示当前填充了多少个mBatch mBatchCount++; return true; &#125; /** * 跳过前面多少个batch * @param skipCount 跳过的batch的个数 */ void skip(int skipCount) &#123; //如果mBatchSize 大于等于 mDims.n()，并且 mBatchSize%mDims.n() == 0， //换句话说batchsteam中的batchsize(比如100)，比batch&#123;i&#125;文件的batchsize(比如50)大，并且能整除. //那么batchstream中一个 batch， 相当于 mBatchSize / mDims.n()个batch 个batch&#123;i&#125;文件 //举个例子：batchsteam中batchsize=100，batch&#123;i&#125;文件中batchsize=50，那么batchsteam中一个batch相当于 两个batch&#123;i&#125;文件 //那么在batchstream中跳过一个 batch， 相当于跳过 mBatchSize / mDims.n() 个 batch&#123;i&#125;文件 //所以才有 mFileCount += skipCount * mBatchSize / mDims.n(); //这时直接通过修改mFileCount的数值来读取剩下的batch文件 if (mBatchSize &gt;= mDims.n() &amp;&amp; mBatchSize%mDims.n() == 0 &amp;&amp; mFileBatchPos == mDims.n()) &#123; mFileCount += skipCount * mBatchSize / mDims.n(); return; &#125; //其他情况：batchsteam中的batchsize不能整除batch&#123;i&#125;文件的batchsize //循环调用 next() 读取batch&#123;i&#125;文件，读取skipCount个，由于next() 会改变 mBatchCount 的值，所以先暂存，再取出 int x = mBatchCount; for (int i = 0; i &lt; skipCount; i++) next(); mBatchCount = x; &#125; //获取batchsteam中的 batch 和 label 的首地址， batch文件中的内容读取后首先是放在 mFileBatch 和 mFileLabels 中， //但最终会被复制到 mBatch和mLabels中，校准使用的就是 mBatch 和mLabels，而不是直接从batch file中读取进来的mFileBatch和mFileLabels float *getBatch() &#123; return &amp;mBatch[0]; &#125; float *getLabels() &#123; return &amp;mLabels[0]; &#125; //mBatchCount表示填充了多少个 mBatch 的数量 //mBatchSize表示填充mBatch时使用的batchsize int getBatchesRead() const &#123; return mBatchCount; &#125; int getBatchSize() const &#123; return mBatchSize; &#125; //获取图片的shape信息，这个在mBatch和mFileBatch中是一样的 nvinfer1::DimsNCHW getDims() const &#123; return mDims; &#125;private: //batch文件（如batch0）中的图像数据和标签数据存放在 mFileBatch 和 mFileLabels 中，此处返回他们的地址 float* getFileBatch() &#123; return &amp;mFileBatch[0]; &#125; float* getFileLabels() &#123; return &amp;mFileLabels[0]; &#125; //此函数用于依次读取 batches文件夹下的 batch&#123;i&#125; 文件，并将读取到的内容存放在mFileBatch和mFileLabels中，读取成功返回true，否则返回false bool update() &#123; //依次读取 batches文件夹下的 batch&#123;i&#125; 文件，mFileCount变量自增，指向下一个batch文件也就是 batch&#123;i+1&#125; 文件 std::string inputFileName = locateFile(std::string("batches/batch") + std::to_string(mFileCount++)); FILE * file = fopen(inputFileName.c_str(), "rb"); if (!file) return false; //从batch文件读取当前 batch 的 shape 信息（图像数据的shape） int d[4]; fread(d, sizeof(int), 4, file); assert(mDims.n() == d[0] &amp;&amp; mDims.c() == d[1] &amp;&amp; mDims.h() == d[2] &amp;&amp; mDims.w() == d[3]); //从batch文件读取图像数据（精度为float，大小为mDims.n()*mImageSize ），存放到 mFileBatch 中 //从batch文件读取标签数据（精度为float，大小为mDims.n()），存放到mFileLabels中 size_t readInputCount = fread(getFileBatch(), sizeof(float), mDims.n()*mImageSize, file); size_t readLabelCount = fread(getFileLabels(), sizeof(float), mDims.n(), file);; assert(readInputCount == size_t(mDims.n()*mImageSize) &amp;&amp; readLabelCount == size_t(mDims.n())); fclose(file); //每读取一个batch文件，mFileBatchPos置零，也就是说新读取的batch文件内容 mFileBatch 还没有开始往 mBatch 拷贝 mFileBatchPos = 0; //读取成功返回true return true; &#125; //stream中的批尺寸和最大批数量，每填充一个mBatch，mBatchCount 自增1 int mBatchSize&#123; 0 &#125;; int mMaxBatches&#123; 0 &#125;; int mBatchCount&#123; 0 &#125;; //mFileCount指向batches文件夹中的batch文件，就跟指针一样，读完一个batch，自增1 //mFileBatchPos在一个batch中当前操作的位置 int mFileCount&#123; 0 &#125;, mFileBatchPos&#123; 0 &#125;; //batchstream中的图片大小，一般要求跟batch文件中的大小一致，初值为0 int mImageSize&#123; 0 &#125;; //batch文件中的数据的shape nvinfer1::DimsNCHW mDims; // 从 batch文件 中读到的图像数据和标签数据最终要放到这里来，这个是最终校准时使用的 std::vector&lt;float&gt; mBatch; std::vector&lt;float&gt; mLabels; //用以存取 从 batch文件 中读到的图像数据和标签数据，相当于buffer std::vector&lt;float&gt; mFileBatch; std::vector&lt;float&gt; mFileLabels;&#125;;#endif 7 结果12345678910111213myself@admin:~/workspace/study/tensorrt/bin$ ./sample_int8 mnistFP32 run:400 batches of size 100 starting at 100........................................Top1: 0.9904, Top5: 1Processing 40000 images averaged 0.00167893 ms/image and 0.167893 ms/batch.FP16 run:400 batches of size 100 starting at 100Engine could not be created at this precisionINT8 run:400 batches of size 100 starting at 100........................................Top1: 0.9908, Top5: 1Processing 40000 images averaged 0.0013438 ms/image and 0.13438 ms/batch. 从这例程中也忽然发现在TensorRT中 1080ti GPU竟然不支持 FP16 mode，虽然1080ti官方的参数上是支持 float16的，但是在TensorRT中竟然不能使用。查了一下，是因为 1080ti的float16 吞吐量太低（throughput），效率太低，应该是TensorRT对float16也进行了条件限制，吞吐量太低的不支持。 从资料中得知，只有 Tesla P100, Quadro GP100, and Jetson TX1/TX2 支持 full-rate FP16 performance，应该也就只有这些才支持 TensorRT的FP16吧。新出的 TITAN V 加了tensor core，float16半精度性能有很大提升，应该也支持？不过有意思的是jetson TX1和 TX2 却能支持 FP16，反而不支持INT8. 可以参考下面资料： FP16 –half=true option doesn’t work on GTX 1080 TI although it runs ./sample_int8 INT8FP16 support on gtx 1060 and 1080 The only GPUs with full-rate FP16 performance are Tesla P100, Quadro GP100, and Jetson TX1/TX2. All GPUs with compute capability 6.1 (e.g. GTX 1050, 1060, 1070, 1080, Pascal Titan X, Titan Xp, Tesla P40, etc.) have low-rate FP16 performance. It’s not the fast path on these GPUs. All of these GPUs should support “full rate” INT8 performance, however. 从结果上看： INT8 MODE：Top 1 0.9908， 速度：0.0013438 ms/image ； FP32 MODE : Top 1 0.9904，速度：0.00167893 ms/image； 准确率竟然还高那么一点点，速度上大概快了20%。 参考 TensorRT Developer Guide cutoff and quantile parameters in TensorRT FP16 –half=true option doesn’t work on GTX 1080 TI although it runs ./sample_int8 INT8 FP16 support on gtx 1060 and 1080]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>TensorRT</tag>
        <tag>inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT(5)-INT8校准原理]]></title>
    <url>%2Farticles%2F923e2c40%2F</url>
    <content type="text"><![CDATA[本次讲一下 tensorRT 的 INT8 低精度推理模式。主要参考 GTC 2017，Szymon Migacz 的PPT 。 1 Low Precision Inference现有的深度学习框架 比如：TensorFlow，Caffe， MixNet等，在训练一个深度神经网络时，往往都会使用 float 32（Full Precise ，简称FP32）的数据精度来表示，权值、偏置、激活值等。但是如果一个网络很深的话，比如像VGG，ResNet这种，网络参数是极其多的，计算量就更多了（比如VGG 19.6 billion FLOPS, ResNet-152 11.3 billion FLOPS）。如此多的计算量，如果中间值都使用 FP 32的精度来计算的话，势必会很费时间。而这对于嵌入式设备或者移动设备来说，简直就是噩梦，因为他们的计算能力和内存数量是不能与PC相比的。 因此解决此问题的方法之一就是在部署推理时（inference）使用低精度数据，比如INT8。除此之外，当然还有模型压缩之类的方法，不过此处不做探究。注意此处只是针对 推理阶段，训练时仍然使用 FP32的精度。 从经验上来分析一下低精度推理的可行性： 实际上有些人认为，即便在推理时使用低精度的数据（比如INT8），在提升速度的同时，也并不会造成太大的精度损失，比如 Why are Eight Bits Enough for Deep Neural Networks? 以及Low Precision Inference with TensorRT 这两篇博文。 文章的作者认为网络在训练的过程中学习到了数据样本的模式可分性，同时由于数据中存在的噪声，使得网络具有较强的鲁棒性，也就是说在输入样本中做轻微的变动并不会过多的影响结果性能。与图像上目标间的位置，姿态，角度等的变化程度相比，这些噪声引进的变动只是很少的一部分，但实际上这些噪声引进的变动同样会使各个层的激活值输出发生变动，然而却对结果影响不大，也就是说训练好的网络对这些噪声具有一定的容忍度（tolerance ）。 正是由于在训练过程中使用高精度（FP32）的数值表示，才使得网络具有一定的容忍度。训练时使用高精度的数值表示，可以使得网络以很小的计算量修正参数，这在网络最后收敛的时候是很重要的，因为收敛的时候要求修正量很小很小（一般训练初始 阶段学习率稍大，越往后学习率越小）。 那么如果使用低精度的数据来表示网络参数以及中间值的话，势必会存在误差，这个误差某种程度上可以认为是一种噪声。那也就是说，使用低精度数据引进的差异是在网络的容忍度之内的，所以对结果不会产生太大影响。 以上分析都是基于经验的，理论上的分析比较少，不过文章提到了两篇 paper，如下： Improving the speed of neural networks on CPUs Training deep neural networks with low precision multiplications 这里不对这两篇paper做探究。 TensorRT 的INT8模式只支持计算能力为6.1的GPU（Compute Capability 6.1 ），比如： GP102 (Tesla P40 and NVIDIA Titan X), GP104 (Tesla P4), and GP106 GPUs，主要根源是这些GPU支持 DP4A硬件指令。DP4A下面会稍微介绍一下。 2 TensorRT INT8 Inference首先看一下不同精度的动态范围： 动态范围 最小正数 FP32 $-3.4×10^{38} ~ +3.4×10^{38}$ $1.4 × 10^{−45}$ FP16 $-65504 ~ +65504$ $5.96 × 10^{-8}$ INT8 $-128 ~ +127$ $1$ 实际上将FP32的精度降为INT8还是比较具有挑战性的。 2.1 Quantization将FP32降为INT8的过程相当于信息再编码（re-encoding information ），就是原来使用32bit来表示一个tensor，现在使用8bit来表示一个tensor，还要求精度不能下降太多。 将FP32转换为 INT8的操作需要针对每一层的输入张量（tensor）和 网络学习到的参数（learned parameters）进行。 首先能想到的最简单的映射方式就是线性映射（或称线性量化，linear quantization）, 就是说映射前后的关系满足下式： $$\text{FP32 Tensor (T) = scale_factor(sf) * 8-bit Tensor(t) + FP32_bias (b)}$$ 试验证明，偏置实际上是不需要的，因此去掉偏置，也就是$$T = sf * t$$$sf$ 是每一层上每一个tensor的换算系数或称比例因子（scaling factor），因此现在的问题就变成了如何确定比例因子。然后最简单的方法是下图这样的： 简单的将一个tensor 中的 -|max| 和 |max| FP32 value 映射为 -127 和 127 ，中间值按照线性关系进行映射。 称这种映射关系为不饱和的（No saturation ），对称的。 但是试验结果显示这样做会导致比较大的精度损失。 下面这张图展示的是不同网络结构的不同layer的激活值分布，有卷积层，有池化层，他们之间的分布很不一样，因此合理的 量化方式 应该适用于不同的激活值分布，并且减小 信息损失。因为从FP32到INT8其实就是一种信息再编码的过程。 我个人理解的直接使用线性量化的方式导致精度损失比较大的原因是： 上图是一些网络模型中间层的 激活值统计，横坐标是激活值，纵坐标是统计数量的归一化表示，这里是归一化表示，不是绝对数值统计； 这个激活值统计 针对的是一批图片，不同的图片输出的激活值不完全相同。所以图上并不是一条曲线而是多条曲线（一张图片对应一条曲线，或者称为散点图更好一点），只不过前面一部分重复在一块了（红色虚线圈起来的部分），说明对于不同图片生成的大部分激活值其分布是相似的；但是在激活值比较大时（红色实线圈起来的部分），曲线不重复了，一个激活值对应多个不同的统计量，这时的激活值分布就比较乱了。 后面这一部分在整个层中是占少数的（占比很小，比如10^-9, 10^-7, 10^-3），因此后面这一段完全可以不考虑到映射关系中去，保留激活值分布的主方向。开始我以为网络之所以能把不同类别的图片分开是由于后面实线部分的差异导致的，后来想了一下：这个并不包含空间位置的分布，只是数值上的分布，所以后面的应该对结果影响不大。 因此TensorRT的做法是： 这种做法不是将 ±|max| 映射为 ±127，而是存在一个 阈值 |T| ，将 ±|T| 映射为±127，显然这里 |T|&lt;|max|。 超出 阈值 ±|T| 外的直接映射为阈值 ±127。比如上图中的三个红色点，直接映射为-127。 称这种映射关系为饱和的（Saturate ），不对称的。 只要 阈值 选取得当，就能将分布散乱的较大的激活值舍弃掉，也就有可能使精度损失不至于降低太多。 网络的前向计算涉及到两部分数值：权值和激活值（weights 和activation，二者要做乘法运算），Szymon Migacz 也提到他们曾经做过实验，说对weights 做saturation 没有什么变化，因此 对于weights的int8量化就使用的是不饱和的方式；而对activation做saturation就有比较显著的性能提升，因此对activation使用的是饱和的量化方式。 那现在的问题是 如何确定|T|？我们来思考一下，现在有一个FP32的tensor，FP32肯定是能够表达这个tensor的最佳分布。现在我们要用一个不同的分布（INT8）来表达这个tensor，这个 INT8 分布不是一个最佳的分布。饱和的INT8分布由于阈值 |T|的取值会有很多种情况（$128-|max|$），其中肯定有一种情况是相对其他最接近FP32的，我们就是要把这种情况找出来。 既然如此，我们就需要一个衡量指标来衡量不同的 INT8 分布与原来的FP3F2分布之间的差异程度。这个衡量指标就是 相对熵（relative entropy），又称为KL散度（Kullback–Leibler divergence，简称KLD），信息散度（information divergence），信息增益（information gain）。叫法实在太多了，最常见的就是相对熵。跟交叉熵也是有关系的。 假设我们要给一个信息进行完美编码，那么最短平均编码长度就是信息熵。 如果编码方案不一定完美（由于对概率分布的估计不一定正确），这时的平均编码长度就是交叉熵。 平均编码长度 = 最短平均编码长度 + 一个增量 交叉熵在深度学习中广泛使用，衡量了测试集标签分布和模型预测分布之间的差异程度。 编码方法不一定完美时，平均编码长度相对于最小值的增加量（即上面那个增量）是相对熵。 即 交叉熵=信息熵+相对熵 通俗的理解 信息熵，交叉熵，相对熵，参考：知乎：如何通俗的解释交叉熵与相对熵? 如何理解信息熵用来表示最短平均编码长度，参考： 如何理解用信息熵来表示最短的平均编码长度 详细的不说了，请看参考链接。 在这里，FP32的tensor就是我们要表达的信息量，FP32也是最佳分布（可以认为最短编码长度32bit），现在要做的是使用INT8 来编码FP32的信息，同时要求INT8编码后差异尽可能最小。考虑两个分布 P（FP32）、Q（INT8）KL散度计算如下：$$\text{KL_divergence(P,Q):= SUM(P[i] * log(P[i] / Q[i] ), i)}$$P，Q分别称为 reference_distribution、 quantize _distribution 实际上这里也说明了每一层的tensor 的 |T| 值都是不一样的。 确定每一层的 |T|值的过程称为 校准（Calibration ）。 2.2 Calibration上面已经说了 KL散度越小代表 INT8编码后的信息损失越少。这一节来看看如何根据KL散度寻找最佳INT8分布。其实前面我们也已经提到了，如果要让最后的精度损失不大，是要考虑一些先验知识的，这个先验知识就是每一层在 FP32精度下的激活值分布，只有根据这个才能找到更加合理的 阈值|T|。也就是说首先得有一个以FP32精度训练好的模型。基本上现有的深度学习框架都是默认 FP32精度的，有些模型还支持FP16精度训练，貌似 Caffe2和MXNet是支持FP16的，其他的不太清楚。所以基本上只要没有特别设定，训练出来的模型肯定是 FP32 的。 那激活值分布如何得到？难道我们要将FP32的模型先在所有的测试集（或验证集）上跑一边记录下每一层的FP32激活值，然后再去推断 |T|? 这里的做法是 从验证集 选取一个子集作为校准集（Calibration Dataset ），校准集应该具有代表性，多样性，最好是验证集的一个子集，不应该只是分类类别的一小部分。激活值分布就是从校准集中得到的。 按照NVIDIA 官方的说法： Note: The calibration set must be representative of the input provided to TensorRT at runtime; for example, for image classification networks, it should not consist of images from just a small subset of categories. For ImageNet networks, around 500 calibration images is adequate. 对于ImageNet 数据集来说 校准集大小一般500张图片就够了（Szymon Migacz的演讲说用1000张），这里有点怀疑也有点震惊，没想到 ImageNet 1000个分类，100多万张图片，500张就够了，不过从2.5节的图表中的结果可以看出500张确实够了。 然后要做的是： 首先在 校准集上 进行 FP32 inference 推理； 对于网络的每一层（遍历）： 收集这一层的激活值，并做 直方图（histograms ），分成几个组别（bins）（官方给的一个说明使用的是2048组），分组是为了下面遍历 |T| 时，减少遍历次数； 对于不同的 阈值 |T| 进行遍历，因为这里 |T|的取值肯定在 第128-2047 组之间，所以就选取每组的中间值进行遍历； 选取使得 KL_divergence(ref_distr, quant_distr) 取得最小值的 |T|。 返回一系列 |T|值，每一层都有一个 |T|。创建 CalibrationTable 。 上面解释一下：假设 最后 使得 KL散度最小的|T|值是第200组的中间值，那么就把原来 第 0-200组的 数值线性映射到 0-128之间，超出范围的直接映射到128。 校准的过程可以参考一下这个：https://www.jianshu.com/p/43318a3dc715， 这篇文章提供了一个详细的根据KL散度来将原始信息进行编码的例子，包括直方图的使用。跟这里的校准过程极为相像。 下面是一个官方 GTC2017 PPT 中给的校准的伪代码： 123456789101112131415161718192021222324252627//首先分成 2048个组，每组包含多个数值（基本都是小数）Input: FP32 histogram H with 2048 bins: bin[ 0 ], …, bin[ 2047 ] For i in range( 128 , 2048 ): // |T|的取值肯定在 第128-2047 组之间,取每组的中点 reference_distribution_P = [ bin[ 0 ] , ..., bin[ i-1 ] ] // 选取前 i 组构成P，i&gt;=128 outliers_count = sum( bin[ i ] , bin[ i+1 ] , … , bin[ 2047 ] ) //边界外的组 reference_distribution_P[ i-1 ] += outliers_count //边界外的组加到边界P[i-1]上，没有直接丢掉 P /= sum(P) // 归一化 // 将前面的P（包含i个组，i&gt;=128），映射到 0-128 上，映射后的称为Q，Q包含128个组， // 一个整数是一组 candidate_distribution_Q = quantize [ bin[ 0 ], …, bin[ i-1 ] ] into 128 levels //这时的P（包含i个组，i&gt;=128）和Q向量（包含128个组）的大小是不一样的，无法直接计算二者的KL散度 //因此需要将Q扩展为 i 个组，以保证跟P大小一样 expand candidate_distribution_Q to ‘ i ’ bins Q /= sum(Q) // 归一化 //计算P和Q的KL散度 divergence[ i ] = KL_divergence( reference_distribution_P, candidate_distribution_Q)End For//找出 divergence[ i ] 最小的数值，假设 divergence[m] 最小，//那么|T|=( m + 0.5 ) * ( width of a bin )Find index ‘m’ for which divergence[ m ] is minimalthreshold = ( m + 0.5 ) * ( width of a bin ) 解释一下第16行： 计算KL散度 KL_divergence(P, Q) 时，要求序列P和Q的长度一致，即 len(P) == len(Q)； Candidate_distribution_Q 是将 P 线性映射到 128个bins得到的，长度为128。而reference_distribution_P 包含 i （i&gt;=128）个 bins （bin[0] - bin[i-1] ），二者长度不等； 需要将 candidate_distribution_Q 扩展回 i 个bins 然后才能与 i个bins 的 reference_distribution_P计算KL散度。 举个简单的栗子： 假设reference_distribution_P 包含 8 个bins（这里一个bin就只包含一个数据）: P = [ 1, 0, 2, 3, 5, 3, 1, 7] 我们想把它映射为 2 个bins，于是 4个一组合并： [1 + 0 + 2 + 3 , 5 + 3 + 1 + 7] = [6, 16] 然后要成比例的 扩展回到 8个组，保留原来是0的组： Q = [ 6/3, 0, 6/3, 6/3, 16/4, 16/4, 16/4, 16/4] = [ 2, 0, 2, 2, 4, 4, 4, 4] 然后对 P和Q进行标准化： P /= sum(P) 、Q /= sum(Q) 最后计算散度： result = KL_divergence(P, Q) 我们来看看 ResNet-152中 res4b30层校准前后的结果对比： 图中那个白线就是 |T|的取值，不过怎么还小于128了，有点没搞明白。 再看看其他几种网络的校准情况： 2.3 DP4A(Dot Product of 4 8-bits Accumulated to a 32-bit)TensorRT 进行优化的方式是 DP4A (Dot Product of 4 8-bits Accumulated to a 32-bit)，如下图： 这是PASCAL 系列GPU的硬件指令，INT8卷积就是使用这种方式进行的卷积计算。 这个没搞太明白是怎么回事，参考这篇博客获取详细信息Mixed-Precision Programming with CUDA 8 下面是 官方 GTC2017 PPT 中给的INT8卷积计算的伪代码： 12345678910111213141516171819202122232425262728// I8 input tensors: I8_input, I8_weights, INT8输入tensor// I8 output tensors: I8_output， INT8输出tensor// F32 bias (original bias from the F32 model),FP32的偏置// F32 scaling factors: input_scale, output_scale, weights_scale[K], 这个是前面说的缩放因子sfI32_gemm_out = I8_input * I8_weights // Compute INT8 GEMM (DP4A)，卷积计算，INT32输出F32_gemm_out = (float)I32_gemm_out // Cast I32 GEMM output to F32 float，强制转换为FP32//前面计算I8_input * I8_weights时，总的缩放系数为 input_scale * weights_scale[K]//但是输出的缩放系数为output_scale，所以为了保证缩放程度匹配，要将F32_gemm_out乘以 //output_scale / (input_scale * weights_scale[ i ] ) // At this point we have F32_gemm_out which is scaled by ( input_scale * weights_scale[K] ),// but to store the final result in int8 we need to have scale equal to "output_scale", so we have to rescale:// (this multiplication is done in F32, *_gemm_out arrays are in NCHW format)For i in 0, ... K-1:rescaled_F32_gemm_out[ :, i, :, :] = F32_gemm_out[ :, i, :, :] * [ output_scale /(input_scale * weights_scale[ i ] ) ] //将FP32精度的偏置 乘上缩放因子，加到前面的计算结果中// Add bias, to perform addition we have to rescale original F32 bias so that it's scaled with "output_scale"rescaled_F32_gemm_out _with_bias = rescaled_F32_gemm_out + output_scale * bias//ReLU 激活// Perform ReLU (in F32)F32_result = ReLU(rescaled_F32_gemm_out _with_bias)//重新转换为 INT8// Convert to INT8 and save to globalI8_output = Saturate( Round_to_nearest_integer( F32_result ) ) 它这个INT8卷积的计算是这样的，虽然输入的tensor已经降为 INT8，但是在卷积计算的时候用了DP4A的计算模式，卷积计算完之后是INT32的，然后又要转成 FP32，然后激活，最后再将FP32的转为INT8. 只知道这么计算会快很多，但不知道为什么，详情还是看Mixed-Precision Programming with CUDA 8 这个吧，我看的也是糊里糊涂的。 不过这个对于tensorRT的使用没啥影响，这个是很底层的东西，涉及到硬件优化。 2.4 Typical workflow in TensorRT典型的工作流还是直接使用 GTC2017 PPT 原文说法吧： You will need: Model trained in FP32. Calibration dataset. TensorRT will: Run inference in FP32 on calibration dataset. Collect required statistics. Run calibration algorithm → optimal scaling factors. Quantize FP32 weights → INT8. Generate “CalibrationTable” and INT8 execution engine. 2.5 Results - Accuracy &amp; Performance精度并没有损失太多 速度提升还蛮多的，尤其是当 batch_size 大于1时，提升更明显 TITAN X GPU优化效果 DRIVE PX 2, dGPU 优化效果 2.6 Open challenges / improvements一些开放式的提升和挑战： Unsigned int8 for activations after ReLU. 无符号 INT8 的映射。 RNNs → open research problem. TensorRT 3.0开始已经支持RNN了。 Fine tuning of saturation thresholds. 对阈值 |T|的 微调方法。 Expose API for accepting custom, user provided scale factors. 开放API，使用户可以自定义 换算系数（比例因子） 这几个开放问题还是很值得研究的。 3 Conclusion 介绍了一种自动化，无参数的 FP32 到 INT8 的转换方法； 对称的，不饱和的线性量化，会导致精度损失较大； 通过最小化 KL散度来选择 饱和量化中的 阈值 |T|; FP32完全可以降低为INT8推理，精度几乎持平，速度有很大提升。 参考 Why are Eight Bits Enough for Deep Neural Networks? Low Precision Inference with TensorRT GTC 2017 Presentation: 8-Bit Inference with TensorRT PPT和演讲视频 维基百科中文版：相对熵 Kullback-Leibler Divergence Explained 引用4的中文版：如何理解K-L散度（相对熵） 知乎：信息熵是什么？ 相对熵（KL散度） 知乎：如何通俗的解释交叉熵与相对熵? 如何理解用信息熵来表示最短的平均编码长度 TensorRT Developer Guide]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>TensorRT</tag>
        <tag>inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT(4)-Profiling and 16-bit Inference]]></title>
    <url>%2Farticles%2Ffda11be6%2F</url>
    <content type="text"><![CDATA[前面几节以 LeNet 为例主要介绍了 tensorRT 的简单使用流程。包括，使用 tensorRT 的 NvCaffeParser 工具以及底层 C++ API 来 模型 caffe 解析，构建 tensorRT 模型并部署等。 本节以 GooLeNet 为例，来展示 tensorRT 的优化方法。 例程位于 /usr/src/tensorrt/samples/sampleGoogleNet 这个例程展示的是 TensorRT的layer-based profiling和 half2mode 和 FP16 使用方法。 1 Key Concepts首先了解几个概念： Profiling a network ：就是测量网络每一层的运行时间，可以很方便的看出：使用了TensorRT和没使用TensorRT在时间上的差别。 FP16 ：FP32 是指 Full Precise Float 32 ，FP 16 就是 float 16。更省内存空间，更节约推理时间。 Half2Mode ：tensorRT 的一种执行模式（execution mode ），这种模式下 图片上相邻区域的 tensor 是 以16位 交叉存储的方式 存在的。而且在 batchsize 大于 1的情况下，这种模式的运行速度是最快的。（Half2Mode is an execution mode where internal tensors interleave 16-bits fromadjacent pairs of images, and is the fastest mode of operation for batch sizes greaterthan one. ） 这是计算机组成原理中涉及到存储方式的选择，不是很懂。大概是下图这样的： 以下分别是 2D和3D情况： ​ 参考这个 顺序存储和交叉存储 ，这样做可以提升存储器带宽。更多详细内容参考文末参考资料。 2 具体做法2.1 配置 builderTensorRT3.0的官方文档上说，如果只是使用 float 16 的数据精度代替 float-32 ， 实际上并不会有多大的性能提升。真正提升性能的是 half2mode ，也就是上述使用了交叉存存储方式的模式。 如何使用half2mode ？ 首先 使用float 16 精度的数据 来初始化 network 对象，主要做法就是 在调用NvCaffeParser 工具解析 caffe模型时，使用 DataType::kHALF 参数，如下： 12345const IBlobNameToTensor *blobNameToTensor = parser-&gt;parse(locateFile(deployFile).c_str(), locateFile(modelFile).c_str(), *network, DataType::kHALF); 配置builder 使用 half2mode ，这个很简单，就一个语句就完成了： 1builder-&gt;setFp16Mode(true); 2.2 Profilingprofiling 一个网络 ,要创建一个 IProfiler 接口并且添加 profiler 到 execution context 中: 1context.profiler = &amp;gProfiler; 然后执行时，Profiling不支持异步方式，只支持同步方式，因此要使用 tensorRT的同步执行函数 execute() ： 12for (int i = 0; i &lt; TIMING_ITERATIONS;i++) engine-&gt;execute(context, buffers); 执行过程中，每一层都会调用 profiler 回调函数，存储执行时间。 因为TensorRT进行了层间融合和张量融合的优化方式，一些层在 TensorRT 中会被合并，如上图。 比如原来网络中的 inception_5a/3x3 和 inception_5a/ relu_3x3 等这样的层会被合并成 inception_5a/3x3 + inception_5a/relu_3x3 ，因此输出 每一层的时间时，也是按照合并之后的输出。因此TensorRT优化之后的网络结构是跟原来的网络结构不是一一对应的。 3 官方例程例程位于 /usr/src/tensorrt/samples/sampleGoogleNet 这个例程展示的是 TensorRT的layer-based profiling和 half2mode 和 FP16 使用方法。相比于前面说过的mnist的例程只添加了一些借口和修改了一部分参数，还是贴个完整代码吧，虽然比较占篇幅。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193#include &lt;assert.h&gt;#include &lt;fstream&gt;#include &lt;sstream&gt;#include &lt;iostream&gt;#include &lt;cmath&gt;#include &lt;algorithm&gt;#include &lt;sys/stat.h&gt;#include &lt;cmath&gt;#include &lt;time.h&gt;#include &lt;cuda_runtime_api.h&gt;#include "NvInfer.h"#include "NvCaffeParser.h"#include "common.h"static Logger gLogger;using namespace nvinfer1;using namespace nvcaffeparser1;// stuff we know about the network and the caffe input/output blobsstatic const int BATCH_SIZE = 4;static const int TIMING_ITERATIONS = 1000;const char* INPUT_BLOB_NAME = "data";const char* OUTPUT_BLOB_NAME = "prob";std::string locateFile(const std::string&amp; input)&#123; std::vector&lt;std::string&gt; dirs&#123;"data/samples/googlenet/", "data/googlenet/"&#125;; return locateFile(input, dirs);&#125;// profile类，继承自 IProfilerstruct Profiler : public IProfiler&#123; typedef std::pair&lt;std::string, float&gt; Record; std::vector&lt;Record&gt; mProfile; // 将每一层的运行时间存放到 vector中 virtual void reportLayerTime(const char* layerName, float ms) &#123; // find_if找到第一个 r.first 与 layerName 相同的层，返回一个迭代器 auto record = std::find_if(mProfile.begin(), mProfile.end(), [&amp;](const Record&amp; r)&#123; return r.first == layerName; &#125;); // 如果是新的层就push_back进vector if (record == mProfile.end()) mProfile.push_back(std::make_pair(layerName, ms)); // 如果是vector中已有的层就直接累加时间，因为他是迭代1000次的，肯定会重复，所以要累加时间 else record-&gt;second += ms; &#125; // 打印各层的运行时间，打印时要除掉 总的迭代次数 void printLayerTimes() &#123; float totalTime = 0; for (size_t i = 0; i &lt; mProfile.size(); i++) &#123; printf("%-40.40s %4.3fms\n", mProfile[i].first.c_str(), mProfile[i].second / TIMING_ITERATIONS); totalTime += mProfile[i].second; &#125; printf("Time over all layers: %4.3f\n", totalTime / TIMING_ITERATIONS); &#125;&#125; gProfiler;void caffeToTRTModel(const std::string&amp; deployFile, // name for caffe prototxt const std::string&amp; modelFile, // name for model const std::vector&lt;std::string&gt;&amp; outputs, // network outputs unsigned int maxBatchSize, // batch size - NB must be at least as large as the batch we want to run with) IHostMemory *&amp;trtModelStream)&#123; // create API root class - must span the lifetime of the engine usage IBuilder* builder = createInferBuilder(gLogger); INetworkDefinition* network = builder-&gt;createNetwork(); // parse the caffe model to populate the network, then set the outputs ICaffeParser* parser = createCaffeParser(); bool useFp16 = builder-&gt;platformHasFastFp16(); // 判断当前的GPU设备是否支持 FP16的精度 DataType modelDataType = useFp16 ? DataType::kHALF : DataType::kFLOAT; // create a 16-bit model if it's natively supported const IBlobNameToTensor *blobNameToTensor = parser-&gt;parse(locateFile(deployFile).c_str(), // caffe deploy file locateFile(modelFile).c_str(), // caffe model file *network, // network definition that the parser will populate modelDataType); assert(blobNameToTensor != nullptr); // the caffe file has no notion of outputs, so we need to manually say which tensors the engine should generate for (auto&amp; s : outputs) network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str())); // Build the engine builder-&gt;setMaxBatchSize(maxBatchSize); builder-&gt;setMaxWorkspaceSize(16 &lt;&lt; 20); // 设置half2mode // set up the network for paired-fp16 format if available if(useFp16) builder-&gt;setFp16Mode(true); ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); assert(engine); // we don't need the network any more, and we can destroy the parser network-&gt;destroy(); parser-&gt;destroy(); // serialize the engine, then close everything down trtModelStream = engine-&gt;serialize(); engine-&gt;destroy(); builder-&gt;destroy(); shutdownProtobufLibrary();&#125;void timeInference(ICudaEngine* engine, int batchSize)&#123; // input and output buffer pointers that we pass to the engine - the engine requires exactly ICudaEngine::getNbBindings(), // of these, but in this case we know that there is exactly one input and one output. assert(engine-&gt;getNbBindings() == 2); void* buffers[2]; // In order to bind the buffers, we need to know the names of the input and output tensors. // note that indices are guaranteed to be less than ICudaEngine::getNbBindings() int inputIndex = engine-&gt;getBindingIndex(INPUT_BLOB_NAME), outputIndex = engine-&gt;getBindingIndex(OUTPUT_BLOB_NAME); // allocate GPU buffers // 自动获取输入输出的维度 Dims3 inputDims = static_cast&lt;Dims3&amp;&amp;&gt;(engine-&gt;getBindingDimensions(inputIndex)), outputDims = static_cast&lt;Dims3&amp;&amp;&gt;(engine-&gt;getBindingDimensions(outputIndex)); size_t inputSize = batchSize * inputDims.d[0] * inputDims.d[1] * inputDims.d[2] * sizeof(float); size_t outputSize = batchSize * outputDims.d[0] * outputDims.d[1] * outputDims.d[2] * sizeof(float); CHECK(cudaMalloc(&amp;buffers[inputIndex], inputSize)); CHECK(cudaMalloc(&amp;buffers[outputIndex], outputSize)); IExecutionContext* context = engine-&gt;createExecutionContext(); // 设置profiler context-&gt;setProfiler(&amp;gProfiler); // zero the input buffer CHECK(cudaMemset(buffers[inputIndex], 0, inputSize)); for (int i = 0; i &lt; TIMING_ITERATIONS;i++) context-&gt;execute(batchSize, buffers); // release the context and buffers context-&gt;destroy(); CHECK(cudaFree(buffers[inputIndex])); CHECK(cudaFree(buffers[outputIndex]));&#125;int main(int argc, char** argv)&#123; std::cout &lt;&lt; "Building and running a GPU inference engine for GoogleNet, N=4..." &lt;&lt; std::endl; // parse the caffe model and the mean file IHostMemory *trtModelStream&#123;nullptr&#125;; caffeToTRTModel("googlenet.prototxt", "googlenet.caffemodel", std::vector &lt; std::string &gt; &#123; OUTPUT_BLOB_NAME &#125;, BATCH_SIZE, trtModelStream); assert(trtModelStream != nullptr); // create an engine IRuntime* infer = createInferRuntime(gLogger); assert(infer != nullptr); ICudaEngine* engine = infer-&gt;deserializeCudaEngine(trtModelStream-&gt;data(), trtModelStream-&gt;size(), nullptr); assert(engine != nullptr); printf("Bindings after deserializing:\n"); for (int bi = 0; bi &lt; engine-&gt;getNbBindings(); bi++) &#123; if (engine-&gt;bindingIsInput(bi) == true) &#123; printf("Binding %d (%s): Input.\n", bi, engine-&gt;getBindingName(bi)); &#125; else &#123; printf("Binding %d (%s): Output.\n", bi, engine-&gt;getBindingName(bi)); &#125; &#125; // run inference with null data to time network performance timeInference(engine, BATCH_SIZE); engine-&gt;destroy(); infer-&gt;destroy(); trtModelStream-&gt;destroy(); // 打印profing 结果 gProfiler.printLayerTimes(); std::cout &lt;&lt; "Done." &lt;&lt; std::endl; return 0;&#125; 4 结果分析TensorRT 的profiling执行结果： batch=4, iterations=1000， GPU=1080 ti 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273myself@admin:~/workspace/study/tensorrt/bin$ ./sample_googlenetBuilding and running a GPU inference engine for GoogleNet, N=4...Bindings after deserializing:Binding 0 (data): Input.Binding 1 (prob): Output.conv1/7x7_s2 + conv1/relu_7x7 0.128mspool1/3x3_s2 0.054mspool1/norm1 0.031msconv2/3x3_reduce + conv2/relu_3x3_reduce 0.029msconv2/3x3 + conv2/relu_3x3 0.193msconv2/norm2 0.084mspool2/3x3_s2 0.045msinception_3a/1x1 + inception_3a/relu_1x1 0.040msinception_3a/3x3 + inception_3a/relu_3x3 0.062msinception_3a/5x5 + inception_3a/relu_5x5 0.044msinception_3a/pool 0.020msinception_3a/pool_proj + inception_3a/re 0.031msinception_3a/1x1 copy 0.008msinception_3b/1x1 + inception_3b/relu_1x1 0.075msinception_3b/3x3 + inception_3b/relu_3x3 0.109msinception_3b/5x5 + inception_3b/relu_5x5 0.086msinception_3b/pool 0.026msinception_3b/pool_proj + inception_3b/re 0.040msinception_3b/1x1 copy 0.012mspool3/3x3_s2 0.032msinception_4a/1x1 + inception_4a/relu_1x1 0.056msinception_4a/3x3 + inception_4a/relu_3x3 0.034msinception_4a/5x5 + inception_4a/relu_5x5 0.044msinception_4a/pool 0.014msinception_4a/pool_proj + inception_4a/re 0.048msinception_4a/1x1 copy 0.007msinception_4b/1x1 + inception_4b/relu_1x1 0.059msinception_4b/3x3 + inception_4b/relu_3x3 0.037msinception_4b/5x5 + inception_4b/relu_5x5 0.059msinception_4b/pool 0.014msinception_4b/pool_proj + inception_4b/re 0.051msinception_4b/1x1 copy 0.006msinception_4c/1x1 + inception_4c/relu_1x1 0.059msinception_4c/3x3 + inception_4c/relu_3x3 0.052msinception_4c/5x5 + inception_4c/relu_5x5 0.061msinception_4c/pool 0.014msinception_4c/pool_proj + inception_4c/re 0.051msinception_4c/1x1 copy 0.006msinception_4d/1x1 + inception_4d/relu_1x1 0.059msinception_4d/3x3 + inception_4d/relu_3x3 0.057msinception_4d/5x5 + inception_4d/relu_5x5 0.072msinception_4d/pool 0.014msinception_4d/pool_proj + inception_4d/re 0.051msinception_4d/1x1 copy 0.005msinception_4e/1x1 + inception_4e/relu_1x1 0.063msinception_4e/3x3 + inception_4e/relu_3x3 0.063msinception_4e/5x5 + inception_4e/relu_5x5 0.071msinception_4e/pool 0.014msinception_4e/pool_proj + inception_4e/re 0.052msinception_4e/1x1 copy 0.008mspool4/3x3_s2 0.014msinception_5a/1x1 + inception_5a/relu_1x1 0.079msinception_5a/3x3 + inception_5a/relu_3x3 0.040msinception_5a/5x5 + inception_5a/relu_5x5 0.071msinception_5a/pool 0.009msinception_5a/pool_proj + inception_5a/re 0.072msinception_5a/1x1 copy 0.004msinception_5b/1x1 + inception_5b/relu_1x1 0.075msinception_5b/3x3 + inception_5b/relu_3x3 0.046msinception_5b/5x5 + inception_5b/relu_5x5 0.097msinception_5b/pool 0.009msinception_5b/pool_proj + inception_5b/re 0.072msinception_5b/1x1 copy 0.005mspool5/7x7_s1 0.012msloss3/classifier 0.019msprob 0.007msTime over all layers: 2.978Done. 这个速度很快的整个网络一次前向过程只有3ms左右。 我们再来看看不用TensorRT的googlenet的profiling结果，这个googlenet使用的是caffe代码中自带的模型文件，profiling用的是caffe 自己的time命令。 将deploy.prototxt 中的batch改为4，迭代次数的话因为这个没有使用TensorRT优化，所以比较费时间，就跑50个iterations，不过也能说明问题了。 同样因为没有使用TensorRT优化，原来的网络结构中是没有进行层间融合的，而且caffe的time命令是把forward和backward都测了时间的，因此输出比较多，所以下面删除了一部分，只保留了inception_5*。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576myself@admin:~/caffe$ ./build/tools/caffe time --model=models/bvlc_googlenet/deploy.prototxt --iterations=50Average time per layer: data forward: 0.00454 ms. data backward: 0.00204 ms.………………太长了省略一部分inception_5a/1x1 forward: 4.43762 ms.inception_5a/1x1 backward: 1.5149 ms.inception_5a/relu_1x1 forward: 0.10942 ms.inception_5a/relu_1x1 backward: 0.00126 ms.inception_5a/3x3_reduce forward: 2.88932 ms.inception_5a/3x3_reduce backward: 1.17394 ms.inception_5a/relu_3x3_reduce forward: 0.0859 ms.inception_5a/relu_3x3_reduce backward: 0.0012 ms.inception_5a/3x3 forward: 9.88662 ms.inception_5a/3x3 backward: 3.98626 ms.inception_5a/relu_3x3 forward: 0.22092 ms.inception_5a/relu_3x3 backward: 0.00116 ms.inception_5a/5x5_reduce forward: 0.90482 ms.inception_5a/5x5_reduce backward: 0.66332 ms.inception_5a/relu_5x5_reduce forward: 0.01554 ms.inception_5a/relu_5x5_reduce backward: 0.00128 ms.inception_5a/5x5 forward: 2.50424 ms.inception_5a/5x5 backward: 1.49614 ms.inception_5a/relu_5x5 forward: 0.05624 ms.inception_5a/relu_5x5 backward: 0.00108 ms.inception_5a/pool forward: 10.9052 ms.inception_5a/pool backward: 0.00168 ms.inception_5a/pool_proj forward: 2.41494 ms.inception_5a/pool_proj backward: 1.23424 ms.inception_5a/relu_pool_proj forward: 0.05614 ms.inception_5a/relu_pool_proj backward: 0.00124 ms.inception_5a/output forward: 0.20292 ms.inception_5a/output backward: 0.01312 ms.inception_5a/output_inception_5a/output_0_split forward: 0.00384 ms.inception_5a/output_inception_5a/output_0_split backward: 0.00156 ms. inception_5b/1x1 forward: 6.4108 ms. inception_5b/1x1 backward: 2.19984 ms. inception_5b/relu_1x1 forward: 0.16204 ms. inception_5b/relu_1x1 backward: 0.00146 ms.inception_5b/3x3_reduce forward: 3.16198 ms.inception_5b/3x3_reduce backward: 1.70668 ms.inception_5b/relu_3x3_reduce forward: 0.08388 ms.inception_5b/relu_3x3_reduce backward: 0.00146 ms.inception_5b/3x3 forward: 13.2323 ms.inception_5b/3x3 backward: 5.93336 ms.inception_5b/relu_3x3 forward: 0.16636 ms.inception_5b/relu_3x3 backward: 0.00118 ms.inception_5b/5x5_reduce forward: 1.01018 ms.inception_5b/5x5_reduce backward: 0.82398 ms.inception_5b/relu_5x5_reduce forward: 0.02294 ms.inception_5b/relu_5x5_reduce backward: 0.00118 ms.inception_5b/5x5 forward: 4.08472 ms.inception_5b/5x5 backward: 2.8564 ms.inception_5b/relu_5x5 forward: 0.05658 ms.inception_5b/relu_5x5 backward: 0.0011 ms.inception_5b/pool forward: 10.9437 ms.inception_5b/pool backward: 0.00116 ms.inception_5b/pool_proj forward: 2.21102 ms.inception_5b/pool_proj backward: 2.23458 ms.inception_5b/relu_pool_proj forward: 0.05634 ms.inception_5b/relu_pool_proj backward: 0.00124 ms.inception_5b/output forward: 0.26758 ms.inception_5b/output backward: 0.01492 ms.pool5/7x7_s1 forward: 2.37076 ms.pool5/7x7_s1 backward: 0.00188 ms.pool5/drop_7x7_s1 forward: 0.06108 ms.pool5/drop_7x7_s1 backward: 0.00134 ms.loss3/classifier forward: 2.74434 ms.loss3/classifier backward: 2.75442 ms. prob forward: 0.28054 ms. prob backward: 0.06392 ms.Average Forward pass: 1046.79 ms.Average Backward pass: 676.121 ms.Average Forward-Backward: 1723.54 ms.Total Time: 86177 ms.*** Benchmark ends *** 首先是一次前向的总耗时： 没有使用TensorRT优化的googlenet 是 1046.79ms，使用TensorRT优化的是2.98ms 其次可以看其中的某一层的对比： inception_5b/1x1 + inception_5b/relu_1x1 优化前： 12inception_5b/1x1 forward: 6.4108 ms.inception_5b/relu_1x1 forward: 0.16204 ms. 总耗时：6.57ms 优化后： 1inception_5b/1x1 + inception_5b/relu_1x1 0.075ms 总耗时：0.075ms inception_5b/3x3 + inception_5b/relu_3x3： 优化前： 12inception_5b/3x3 forward: 13.2323 ms.inception_5b/relu_3x3 forward: 0.16636 ms. 总耗时：13.40ms 优化后： 1inception_5b/3x3 + inception_5b/relu_3x3 0.046ms 总耗时：0.046ms inception_5b/5x5 + inception_5b/relu_5x5 优化前： 12inception_5b/5x5 forward: 4.08472 ms.inception_5b/relu_5x5 forward: 0.05658 ms. 总耗时：4.14ms 优化后： 1inception_5b/5x5 + inception_5b/relu_5x5 0.097ms 总耗时：0.079ms 此外还有这些层： 优化前： 12345inception_5b/pool forward: 10.9437 ms.inception_5b/pool_proj forward: 2.21102 ms.inception_5b/relu_pool_proj forward: 0.05634 ms.inception_5b/output forward: 0.26758 ms.pool5/7x7_s1 forward: 2.37076 ms. 优化后： 1234inception_5b/pool 0.009msinception_5b/pool_proj + inception_5b/re 0.072msinception_5b/1x1 copy 0.005mspool5/7x7_s1 0.012ms 前面 3×3 卷积比 5×5 卷积还耗时间是因为 3×3 卷积的channel比 5×5 卷积的channel多很多，但是经过TensorRT优化之后二者差别就不是很大了，甚至 5×5 卷积比 3×3 卷积 耗时间。 TensorRT确实极大的降低了前向传播时间，一次前向传播时间只有优化之前的 0.2%，不过这只是分类问题，并且网络也都是传统卷积堆起来的。对于那些复杂结构的网络，比如用于检测的网络或者使用了非经典卷积的比如 dilated conv 或者 deformable conv 的，应该就不会有这么大幅度的提升效果了。不过从英伟达公布的测试数据来看，提升幅度还是蛮大的。 参考 Morton Coding Overview：http://ashtl.sourceforge.net/morton_overview.html Interleaving Explained：http://www.kitz.co.uk/adsl/interleaving.htm interleave bits the obvious way：https://stackoverflow.com/questions/3203764/bit-twiddling-hacks-interleave-bits-the-obvious-way Bitwise operation：https://en.wikipedia.org/wiki/Bitwise_operation 计算机组成原理 顺序存储和交叉存储：https://wenku.baidu.com/view/43f5d1d333d4b14e8524687b 执行时间计算： https://devtalk.nvidia.com/default/topic/1027443/print-time-unit-is-not-ms-in-samplegooglenet-cpp-of-tensorrt-3-0-sdk-/ tensorRT API : https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_profiler.html tensorRT 用户手册：http://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#googlenet_sample]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>TensorRT</tag>
        <tag>inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT(3)-C++ API使用：mnist手写体识别]]></title>
    <url>%2Farticles%2F7629a20%2F</url>
    <content type="text"><![CDATA[本节将介绍如何使用tensorRT C++ API 进行网络模型创建。 1 使用C++ API 进行 tensorRT 模型创建还是通过 tensorRT官方给的一个例程来学习。 还是mnist手写体识别的例子。上一节主要是用 tensorRT提供的NvCaffeParser来将 Caffe中的model 转换成tensorRT中特有的模型结构。NvCaffeParser是tensorRT封装好的一个用以解析Caffe模型的工具 （较顶层的API），同样的还有 NvUffPaser是用于解析TensorFlow的工具。 除了以上两个封装好的工具之外，还可以使用tensorRT提供的C++ API（底层的API）来直接在tensorRT中创建模型。这时 tensorRT 相当于是一个独立的深度学习框架了，这个框架和其他框架（Caffe, TensorFlow，MXNet等）一样都具备搭建网络模型的能力（只有前向计算没有反向传播）。 不同之处在于： 这个框架不能用于训练，模型的权值参数要人为给定； 可以针对设定网络模型（自己使用API创建网络模型）或给定模型（使用NvCaffeParser或NvUffPaser导入其他深度学习框架训练好的模型）做一系列优化，以加快推理速度（inference） 使用C++ API函数部署网络主要分为四个步骤： 创建网络； 为网络添加输入； 添加各种各样的层； 设定网络输出； 以上，第1,2,4步骤在使用 NvCaffeParser 时也是有的。只有第3步是本节所讲的方法中特有的，其实对于NvCaffeParser 工具来说，他只是把 第 3步封装起来了而已。 如下，对比一下 NvCaffeParser 的使用方法，下面的代码中只列出了关键部分的代码。完整代码请看上一节。 12345678910111213141516171819202122232425262728//build phaseINetworkDefinition* network = builder-&gt;createNetwork(); //1. 创建网络CaffeParser* parser = createCaffeParser();std::unordered_map&lt;std::string, infer1::Tensor&gt; blobNameToTensor;const IBlobNameToTensor* blobNameToTensor = //3. 添加各种各样的层 parser-&gt;parse(locateFile(deployFile).c_str(), //NvCaffeParser 工具 locateFile(modelFile).c_str(), //把添加层的内容封装起来了 *network, DataType::kFLOAT);for (auto&amp; s : outputs) network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str())); // 4. 设定网络输出ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); //创建engine//省略一些内容………………//execution phaseIExecutionContext *context = engine-&gt;createExecutionContext(); //创建 contextint inputIndex = engine-&gt;getBindingIndex(INPUT_BLOB_NAME), outputIndex = engine-&gt;getBindingIndex(OUTPUT_BLOB_NAME); //2.为网络添加输入//省略一些内容………………context.enqueue(batchSize, buffers, stream, nullptr); //调用cuda核计算cudaStreamSynchronize(stream); //同步cuda 流 上述四个步骤对应部分已在注释标出。可见 NvCaffeParser 工具中最主要的是 parse 函数，这个函数接受网络模型文件（deploy.prototxt）、权值文件（net.caffemodel）为参数，这两个文件是caffe的模型定义文件和训练参数文件。parse 函数会解析这两个文件并对应生成 tensorRT的模型结构。 对于NvCaffeParser 工具来说，是需要三个文件的，分别是： 网络模型文件（比如，caffe的deploy.prototxt） 训练好的权值文件（比如，caffe的net.caffemodel） 标签文件（这个主要是将模型产生的数字标号分类，与真实的名称对应起来） 以下分步骤说明四个步骤： 1.1 创建网络先创建一个tensorRT的network，这个network 现在只是个空架子，比较简单： 1INetworkDefinition* network = builder-&gt;createNetwork(); 1.2 为网络添加输入所有的网络都需要明确输入是哪个blob，因为这是数据传送的入口。 12// Create input of shape &#123; 1, 1, 28, 28 &#125; with name referenced by INPUT_BLOB_NAME auto data = network-&gt;addInput(INPUT_BLOB_NAME, dt, DimsCHW&#123; 1, INPUT_H, INPUT_W&#125;); INPUT_BLOB_NAME 是为输入 blob起的名字; dt是指数据类型，有kFLOAT(float 32), kHALF(float 16), kINT8(int 8)等类型; 12345678//位于 NvInfer.h 文件 enum class DataType : int&#123; kFLOAT = 0, //!&lt; FP32 format. kHALF = 1, //!&lt; FP16 format. kINT8 = 2, //!&lt; INT8 format. kINT32 = 3 //!&lt; INT32 format. 这个是TensorRT新增的&#125;; DimsCHW{ 1, INPUT_H, INPUT_W} 是指，batch为1（省略），channel 为1，输入height 和width分别为 INPUT_H, INPUT_W的blob； 1.3 添加各种各样的层 以下示例是添加一个 scale layer 123456// Create a scale layer with default power/shift and specified scale parameter. floatscale_param = 0.0125f; Weights power&#123;DataType::kFLOAT, nullptr, 0&#125;; Weights shift&#123;DataType::kFLOAT, nullptr, 0&#125;; Weights scale&#123;DataType::kFLOAT, &amp;scale_param, 1&#125;; auto scale_1 = network-&gt;addScale(*data, ScaleMode::kUNIFORM, shift, scale, power); 主要就是 addScale 函数，后面接受的参数是这一层需要设置的参数。 scale 层的作用是对每个输入数据进行幂运算 f(x)= (shift + scale * x) ^ power 层类型：Power 可选参数： power: 默认为1 scale: 默认为1 shift: 默认为0 就是一种激活层。 Weights 类的定义如下： 123456789//NvInfer.h 文件class Weights&#123;public: DataType type; //!&lt; the type of the weights const void* values; //!&lt; the weight values, in a contiguous array int64_t count; //!&lt; the number of weights in the array&#125;; 以上是不包含训练参数的层，还有 Relu层，Pooling层等。 包含训练参数的层，比如卷积层，全连接层，要先加载权值文件。 以下示例是添加一个卷积层 123456789// Add convolution layer with 20 outputs and a 5x5 filter.// 加载权值文件，加载一次即可std::map&lt;std::string, Weights&gt; weightMap = loadWeights(locateFile("mnistapi.wts"));//添加卷积层IConvolutionLayer* conv1 = network-&gt;addConvolution(*scale_1-&gt;getOutput(0), 20, DimsHW&#123;5, 5&#125;, weightMap["conv1filter"], weightMap["conv1bias"]);//设置步长conv1-&gt;setStride(DimsHW&#123;1, 1&#125;); 第6行添加卷积层： 1IConvolutionLayer* conv1 = network-&gt;addConvolution(*scale_1-&gt;getOutput(0), 20, DimsHW&#123;5, 5&#125;, weightMap["conv1filter"], weightMap["conv1bias"]); *scale_1-&gt;getOutput(0) ：获取上一层 scale层的输出 20：卷积核个数，或者输出feature map 层数 DimsHW{5, 5}：卷积核大小 weightMap[&quot;conv1filter&quot;], weightMap[&quot;conv1bias&quot;]：权值系数矩阵 上面的 mnistapi.wts 文件，是用于存放网络中各个层间的权值系数的，该文件位于 /usr/src/tensorrt/data 文件夹中。 可以用notepad打开看一下，如下： 可见每一行都是一层的一些参数，比如 conv1bias 是指第一个卷积层的偏置系数，后面的0 指的是 kFLOAT 类型，也就是 float 32；后面的20是系数的个数，因为输出是20，所以偏置是20个；下面一行是 卷积核的系数，因为是20个 5×5的卷积核，所以有 20×5×5=500个参数。其它层依次类推。 这个文件是例程中直接给的，感觉像是 用caffe等工具训练后，将weights系数从caffemodel 中提取出来的。直接读取caffemodel应该也是可以的，稍微改一下接口：解析caffemodel文件然后将层名和权值参数键值对存到一个map中，网上大概找了一下，比如 这个 ，解析后的caffemodel如下所示： conv1 最下面有一个 blobs结构，这个是weights系数；每一个包含参数的层（卷积，全连接等；激活层，池化层没有参数）都有一个 blobs结构。只需将这些参数提取出来，保存到一个map中。 除此之外也可以添加很多其他的层，比如反卷积层，池化层，全连接层等，具体参考 英伟达官方API 。 添加层的过程就相当于 NvCaffeParser 工具中 parse 函数解析 deploy.prototxt 文件的过程。 1.4 设定网络输出网络必须知道哪一个blob是输出的。 如下代码，在网络的最后添加了一个softmax层，并将这个层命名为 OUTPUT_BLOB_NAME，之后指定为输出层。 1234// Add a softmax layer to determine the probability. auto prob = network-&gt;addSoftMax(*ip2-&gt;getOutput(0)); prob-&gt;getOutput(0)-&gt;setName(OUTPUT_BLOB_NAME); network-&gt;markOutput(*prob-&gt;getOutput(0)); 那直接使用底层API有什么好处呢？看下表 Feature C++ Python NvCaffeParser NvUffParser CNNs yes yes yes yes RNNs yes yes no no INT8 Calibration yes yes NA NA Asymmetric Padding yes yes no no 上表列出了 tensorRT 的不同特点与 API 对应的情况。可以看到对于 RNN，int8校准（float 32 转为 int8），不对称 padding 来说，NvCaffeParser是不支持的，只有 C++ API 和 Python API，才是支持的。 所以说如果是针对很复杂的网络结构使用tensorRT，还是直接使用底层的 C++ API，和Python API 较好。底层C++ API还可以解析像 darknet 这样的网络模型，因为它需要的就只是一个层名和权值参数对应的map文件。 2 官方例程例程位于 /usr/src/tensorrt/samples/sampleMNISTAPI 2.1 build phase12345//这个是main函数中的代码片段// create a model using the API directly and serialize it to a streamIHostMemory *modelStream&#123;nullptr&#125;;//调用APIToModel函数，手动创建网络模型APIToModel(1, &amp;modelStream); APIToModel函数： 1234567891011121314151617void APIToModel(unsigned int maxBatchSize, IHostMemory** modelStream)&#123; // Create builder IBuilder* builder = createInferBuilder(gLogger); //下面这个createMNISTEngine函数才是真正手动创建网络的过程 // Create model to populate the network, then set the outputs and create an engine ICudaEngine* engine = createMNISTEngine(maxBatchSize, builder, DataType::kFLOAT); assert(engine != nullptr); // Serialize the engine (*modelStream) = engine-&gt;serialize(); // Close everything down engine-&gt;destroy(); builder-&gt;destroy();&#125; createMNISTEngine函数如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// Creat the engine using only the API and not any parser.ICudaEngine* createMNISTEngine(unsigned int maxBatchSize, IBuilder* builder, DataType dt)&#123; INetworkDefinition* network = builder-&gt;createNetwork(); // Create input tensor of shape &#123; 1, 1, 28, 28 &#125; with name INPUT_BLOB_NAME ITensor* data = network-&gt;addInput(INPUT_BLOB_NAME, dt, Dims3&#123;1, INPUT_H, INPUT_W&#125;); assert(data); // Create scale layer with default power/shift and specified scale parameter. const float scaleParam = 0.0125f; const Weights power&#123;DataType::kFLOAT, nullptr, 0&#125;; const Weights shift&#123;DataType::kFLOAT, nullptr, 0&#125;; const Weights scale&#123;DataType::kFLOAT, &amp;scaleParam, 1&#125;; IScaleLayer* scale_1 = network-&gt;addScale(*data, ScaleMode::kUNIFORM, shift, scale, power); assert(scale_1); // Add convolution layer with 20 outputs and a 5x5 filter. // 加载权值文件，加载一次即可 std::map&lt;std::string, Weights&gt; weightMap = loadWeights(locateFile("mnistapi.wts")); // 添加卷积层 IConvolutionLayer* conv1 = network-&gt;addConvolution(*scale_1-&gt;getOutput(0), 20, DimsHW&#123;5, 5&#125;, weightMap["conv1filter"], weightMap["conv1bias"]); assert(conv1); //设置步长 conv1-&gt;setStride(DimsHW&#123;1, 1&#125;); // Add max pooling layer with stride of 2x2 and kernel size of 2x2. IPoolingLayer* pool1 = network-&gt;addPooling(*conv1-&gt;getOutput(0), PoolingType::kMAX, DimsHW&#123;2, 2&#125;); assert(pool1); pool1-&gt;setStride(DimsHW&#123;2, 2&#125;); // Add second convolution layer with 50 outputs and a 5x5 filter. IConvolutionLayer* conv2 = network-&gt;addConvolution(*pool1-&gt;getOutput(0), 50, DimsHW&#123;5, 5&#125;, weightMap["conv2filter"], weightMap["conv2bias"]); assert(conv2); conv2-&gt;setStride(DimsHW&#123;1, 1&#125;); // Add second max pooling layer with stride of 2x2 and kernel size of 2x3&gt; IPoolingLayer* pool2 = network-&gt;addPooling(*conv2-&gt;getOutput(0), PoolingType::kMAX, DimsHW&#123;2, 2&#125;); assert(pool2); pool2-&gt;setStride(DimsHW&#123;2, 2&#125;); // Add fully connected layer with 500 outputs. IFullyConnectedLayer* ip1 = network-&gt;addFullyConnected(*pool2-&gt;getOutput(0), 500, weightMap["ip1filter"], weightMap["ip1bias"]); assert(ip1); // Add activation layer using the ReLU algorithm. IActivationLayer* relu1 = network-&gt;addActivation(*ip1-&gt;getOutput(0), ActivationType::kRELU); assert(relu1); // Add second fully connected layer with 20 outputs. IFullyConnectedLayer* ip2 = network-&gt;addFullyConnected(*relu1-&gt;getOutput(0), OUTPUT_SIZE, weightMap["ip2filter"], weightMap["ip2bias"]); assert(ip2); // Add softmax layer to determine the probability. ISoftMaxLayer* prob = network-&gt;addSoftMax(*ip2-&gt;getOutput(0)); assert(prob); prob-&gt;getOutput(0)-&gt;setName(OUTPUT_BLOB_NAME); network-&gt;markOutput(*prob-&gt;getOutput(0)); // Build engine builder-&gt;setMaxBatchSize(maxBatchSize); builder-&gt;setMaxWorkspaceSize(1 &lt;&lt; 20); ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); // Don't need the network any more network-&gt;destroy(); // Release host memory for (auto&amp; mem : weightMap) &#123; free((void*) (mem.second.values)); &#125; return engine;&#125; 可见里面包含了很多 add* 函数，都是用于添加各种各样的层的。可参考英伟达官方API 。 2.2 deploy phasedeploy阶段基本与之前的无异。 12345678910111213141516171819202122int main(int argc, char** argv)&#123; ……………… ……………… // Deserialize engine we serialized earlier // 创建运行时环境 IRuntime对象，传入 gLogger 用于打印信息 IRuntime* runtime = createInferRuntime(gLogger); assert(runtime != nullptr); ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(trtModelStream-&gt;data(), trtModelStream-&gt;size(), nullptr); assert(engine != nullptr); trtModelStream-&gt;destroy(); //创建上下文环境，主要用于inference 函数中启动cuda核 IExecutionContext* context = engine-&gt;createExecutionContext(); assert(context != nullptr); //2.deploy 阶段：调用 inference 函数，进行推理过程 // Run inference on input data float prob[OUTPUT_SIZE]; doInference(*context, data, prob, 1); ……………… ………………&#125; doInference函数如下： 123456789101112131415161718192021222324252627282930313233void doInference(IExecutionContext&amp; context, float* input, float* output, int batchSize)&#123; const ICudaEngine&amp; engine = context.getEngine(); // Pointers to input and output device buffers to pass to engine. // Engine requires exactly IEngine::getNbBindings() number of buffers. assert(engine.getNbBindings() == 2); void* buffers[2]; // In order to bind the buffers, we need to know the names of the input and output tensors. // Note that indices are guaranteed to be less than IEngine::getNbBindings() const int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME); const int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME); // Create GPU buffers on device CHECK(cudaMalloc(&amp;buffers[inputIndex], batchSize * INPUT_H * INPUT_W * sizeof(float))); CHECK(cudaMalloc(&amp;buffers[outputIndex], batchSize * OUTPUT_SIZE * sizeof(float))); // Create stream cudaStream_t stream; CHECK(cudaStreamCreate(&amp;stream)); // DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host CHECK(cudaMemcpyAsync(buffers[inputIndex], input, batchSize * INPUT_H * INPUT_W * sizeof(float), cudaMemcpyHostToDevice, stream)); context.enqueue(batchSize, buffers, stream, nullptr); CHECK(cudaMemcpyAsync(output, buffers[outputIndex], batchSize * OUTPUT_SIZE * sizeof(float), cudaMemcpyDeviceToHost, stream)); cudaStreamSynchronize(stream); // Release stream and buffers cudaStreamDestroy(stream); CHECK(cudaFree(buffers[inputIndex])); CHECK(cudaFree(buffers[outputIndex]));&#125; 参考资料 caffe中的一些激活函数：http://www.cnblogs.com/denny402/p/5072507.html caffemodel 解析：http://www.cnblogs.com/zjutzz/p/6185452.html caffemodel 解析：http://www.cnblogs.com/zzq1989/p/4439429.html tensorRT C++ API：https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/index.html tensorRT python API：https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/index.html tensorRT 开发者指南：https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html NVIDIA Deep Learning SDK：https://docs.nvidia.com/deeplearning/sdk/index.html]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>TensorRT</tag>
        <tag>inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT(2)-基本使用：mnist手写体识别]]></title>
    <url>%2Farticles%2Fc17471cb%2F</url>
    <content type="text"><![CDATA[结合 tensorRT官方给出的一个例程，介绍tensorRT的使用。 这个例程是mnist手写体识别。例程位于目录： /usr/src/tensorrt/samples/sampleMNIST 文件结构： 12345tensorrt/samples/sampleMNIST -common.cpp -common.h -Makefile -sampleMNIST.cpp 主要是 sampleMNIST.cpp 文件， common.cpp 文件主要提供 读取文件的函数和 Logger对象。 main123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127#include &lt;algorithm&gt;#include &lt;assert.h&gt;#include &lt;cmath&gt;#include &lt;cuda_runtime_api.h&gt;#include &lt;fstream&gt;#include &lt;iostream&gt;#include &lt;sstream&gt;#include &lt;sys/stat.h&gt;#include &lt;time.h&gt;#include "NvCaffeParser.h"#include "NvInfer.h"#include "common.h"using namespace nvinfer1;using namespace nvcaffeparser1;//定义输入输出大小，创建Logger对象//Logger是一个日志类，在common.h文件中定义static Logger gLogger;// Attributes of MNIST Caffe modelstatic const int INPUT_H = 28;static const int INPUT_W = 28;static const int OUTPUT_SIZE = 10;//指定输入输出blob，和资源文件夹const char* INPUT_BLOB_NAME = "data";const char* OUTPUT_BLOB_NAME = "prob";const std::vector&lt;std::string&gt; directories&#123;"data/samples/mnist/", "data/mnist/"&#125;;//查找文件std::string locateFile(const std::string&amp; input)&#123; return locateFile(input, directories);&#125;//读取图片// Simple PGM (portable greyscale map) readervoid readPGMFile(const std::string&amp; fileName, uint8_t buffer[INPUT_H * INPUT_W])&#123; readPGMFile(fileName, buffer, INPUT_H, INPUT_W);&#125;………………………………int main(int argc, char** argv)&#123; if (argc &gt; 1) &#123; std::cout &lt;&lt; "This sample builds a TensorRT engine by importing a trained MNIST Caffe model.\n"; std::cout &lt;&lt; "It uses the engine to run inference on an input image of a digit.\n"; return EXIT_SUCCESS; &#125; // Create TRT model from caffe model and serialize it to a stream // 创建tensorRT流对象 trtModelStream，这个就跟文件流中的 ifstream 是类似的。 // trtModelStream是一块内存区域，用于保存序列化的plan文件。 IHostMemory* trtModelStream&#123;nullptr&#125;; //1. build阶段：调用caffeToTRTModel函数，传入caffe模型文件和权值文件，创建 Ibuilder对象，调用模型解析函数， //生成的plan文件保存在 gieModelStream 中 caffeToTRTModel("mnist.prototxt", "mnist.caffemodel", std::vector&lt;std::string&gt;&#123;OUTPUT_BLOB_NAME&#125;, 1, trtModelStream); assert(trtModelStream != nullptr); // 随机读取一张图片 // Read a random digit file srand(unsigned(time(nullptr))); uint8_t fileData[INPUT_H * INPUT_W]; const int num = rand() % 10; readPGMFile(locateFile(std::to_string(num) + ".pgm", directories), fileData); //将原始图片中的像素用二进制文本 “.:-=+*#%@”来输出 // Print ASCII representation of digit std::cout &lt;&lt; "\nInput:\n" &lt;&lt; std::endl; for (int i = 0; i &lt; INPUT_H * INPUT_W; i++) std::cout &lt;&lt; (" .:-=+*#%@"[fileData[i] / 26]) &lt;&lt; (((i + 1) % INPUT_W) ? "" : "\n"); // 加载均值文件，将读取的图片统一减去平均值。 // Parse mean file ICaffeParser* parser = createCaffeParser(); IBinaryProtoBlob* meanBlob = parser-&gt;parseBinaryProto(locateFile("mnist_mean.binaryproto", directories).c_str()); parser-&gt;destroy(); // Subtract mean from image const float* meanData = reinterpret_cast&lt;const float*&gt;(meanBlob-&gt;getData()); float data[INPUT_H * INPUT_W]; for (int i = 0; i &lt; INPUT_H * INPUT_W; i++) data[i] = float(fileData[i]) - meanData[i]; meanBlob-&gt;destroy(); // Deserialize engine we serialized earlier // 创建运行时环境 IRuntime对象，传入 gLogger 用于打印信息 IRuntime* runtime = createInferRuntime(gLogger); assert(runtime != nullptr); ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(trtModelStream-&gt;data(), trtModelStream-&gt;size(), nullptr); assert(engine != nullptr); trtModelStream-&gt;destroy(); //创建上下文环境，主要用于inference 函数中启动cuda核 IExecutionContext* context = engine-&gt;createExecutionContext(); assert(context != nullptr); //2.deploy 阶段：调用 doInference 函数，进行推理过程 // Run inference on input data float prob[OUTPUT_SIZE]; doInference(*context, data, prob, 1); //销毁无用对象 // Destroy the engine context-&gt;destroy(); engine-&gt;destroy(); runtime-&gt;destroy(); //输出分类结果 // Print histogram of the output distribution std::cout &lt;&lt; "\nOutput:\n\n"; float val&#123;0.0f&#125;; int idx&#123;0&#125;; for (unsigned int i = 0; i &lt; 10; i++) &#123; val = std::max(val, prob[i]); if (val == prob[i]) idx = i; std::cout &lt;&lt; i &lt;&lt; ": " &lt;&lt; std::string(int(std::floor(prob[i] * 10 + 0.5f)), '*') &lt;&lt; "\n"; &#125; std::cout &lt;&lt; std::endl; return (idx == num &amp;&amp; val &gt; 0.9f) ? EXIT_SUCCESS : EXIT_FAILURE;&#125; 实际上从第93行创建 IRuntime对象时，就可以认为是属于deploy了。 最后输出是这样的：读进一张9，输出一个结果： 其中最重要的两个函数 caffeToTRTModel( ) 和 doInference( )分别完成的是build和deploy的功能。 Build Phase 将Caffe model 转换为 TensorRT object，首先使用其他深度学习框架训练好模型，然后丢进tensorRT优化器中进行优化，优化后会产生一个文件，这个文件可以认为是优化后的模型，接着使用序列化方法将这个优化好后的模型存储在磁盘上，存储到磁盘上的文件称为 plan file。 这个阶段需要给tensorRT提供两个文件，分别是 网络模型文件（比如，caffe的deploy.prototxt） 训练好的权值文件（比如，caffe的net.caffemodel） 除此之外，还需要明确 batch size，并指明输出层。 mnist例程中的caffe模型解析代码：标志是创建 IBuilder对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// 解析caffemodel到tensorrtvoid caffeToTRTModel(const std::string&amp; deployFile, // Path of Caffe prototxt file const std::string&amp; modelFile, // Path of Caffe model file const std::vector&lt;std::string&gt;&amp; outputs, // Names of network outputs unsigned int maxBatchSize, // Note: Must be at least as large as the batch we want to run with IHostMemory*&amp; trtModelStream) // Output buffer for the TRT model&#123; // 1. Create builder //创建一个 IBuilder，传进gLogger参数是为了方便打印信息。 //builder 这个地方感觉像是使用了建造者模式。 IBuilder* builder = createInferBuilder(gLogger); // Parse caffe model to populate network, then set the outputs const std::string deployFpath = locateFile(deployFile, directories); const std::string modelFpath = locateFile(modelFile, directories); std::cout &lt;&lt; "Reading Caffe prototxt: " &lt;&lt; deployFpath &lt;&lt; "\n"; std::cout &lt;&lt; "Reading Caffe model: " &lt;&lt; modelFpath &lt;&lt; "\n"; //创建一个 network对象，但是这个network对象只是一个空架子，里面的属性还没有具体的数值。 INetworkDefinition* network = builder-&gt;createNetwork(); //创建一个caffe模型解析对象，parser,并调用解析函数，填充network对象， //将caffe模型中的blob解析为tensorRT中的tensor，赋给blob_name_to_tensor变量。 //此处使用了模型文件和权值文件。 ICaffeParser* parser = createCaffeParser(); const IBlobNameToTensor* blobNameToTensor = parser-&gt;parse(deployFpath.c_str(), modelFpath.c_str(), *network, DataType::kFLOAT); //标记输出blob. // Specify output tensors of network for (auto&amp; s : outputs) network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str())); // 设置batch size；设置工作空间 size。 builder-&gt;setMaxBatchSize(maxBatchSize); builder-&gt;setMaxWorkspaceSize(1 &lt;&lt; 20); // 2.Build engine //使用network创建 CudaEngine，优化方法在这里执行。 //至此，caffe模型已转换为tensorRT object。 ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); assert(engine); //销毁没用的network对象和parser对象。 // Destroy parser and network network-&gt;destroy(); parser-&gt;destroy(); //将转换好的tensorRT object序列化到内存中，trtModelStream是一块内存空间。 //这里也可以序列化到磁盘中。 // Serialize engine and destroy it trtModelStream = engine-&gt;serialize(); //销毁无用对象 engine-&gt;destroy(); builder-&gt;destroy(); //关闭protobuf库 shutdownProtobufLibrary();&#125; Deploy Phase Deploy 阶段需要文件如下： 标签文件（这个主要是将模型产生的数字标号分类，与真实的名称对应起来），不过这个例子中就不需要了，因为MNIST的真实分类就是数字标号。 Deploy 阶段可以认为从主函数中就已经开始了。标志是创建 IRuntime 对象。 12345678910111213141516171819202122int main(int argc, char** argv)&#123; ……………… ……………… // Deserialize engine we serialized earlier // 创建运行时环境 IRuntime对象，传入 gLogger 用于打印信息 IRuntime* runtime = createInferRuntime(gLogger); assert(runtime != nullptr); ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(trtModelStream-&gt;data(), trtModelStream-&gt;size(), nullptr); assert(engine != nullptr); trtModelStream-&gt;destroy(); //创建上下文环境，主要用于inference 函数中启动cuda核 IExecutionContext* context = engine-&gt;createExecutionContext(); assert(context != nullptr); //2.deploy 阶段：调用 inference 函数，进行推理过程 // Run inference on input data float prob[OUTPUT_SIZE]; doInference(*context, data, prob, 1); ……………… ………………&#125; 其中 doInference函数的详细内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647void doInference(IExecutionContext&amp; context, float* input, float* output, int batchSize)&#123; //使用传进来的context恢复engine。 const ICudaEngine&amp; engine = context.getEngine(); //engine.getNbBindings()是为了获取与这个engine相关的输入输出tensor的数量。 //这个地方，输入+输出 总共就2个，所以做个验证。 // Pointers to input and output device buffers to pass to engine. // Engine requires exactly IEngine::getNbBindings() number of buffers. assert(engine.getNbBindings() == 2); //void* 型数组，主要用于下面GPU开辟内存。 void* buffers[2]; //获取与这个engine相关的输入输出tensor的索引。 // In order to bind the buffers, we need to know the names of the input and output tensors. // Note that indices are guaranteed to be less than IEngine::getNbBindings() const int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME); const int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME); //为输入输出tensor开辟显存。 // Create GPU buffers on device CHECK(cudaMalloc(&amp;buffers[inputIndex], batchSize * INPUT_H * INPUT_W * sizeof(float))); CHECK(cudaMalloc(&amp;buffers[outputIndex], batchSize * OUTPUT_SIZE * sizeof(float))); //创建cuda流，用于管理数据复制，存取，和计算的并发操作 // Create stream cudaStream_t stream; CHECK(cudaStreamCreate(&amp;stream)); //从内存到显存，从CPU到GPU，将输入数据拷贝到显存中 //input是读入内存中的数据；buffers[inputIndex]是显存上的存储区域，用于存放输入数据 // DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host CHECK(cudaMemcpyAsync(buffers[inputIndex], input, batchSize * INPUT_H * INPUT_W * sizeof(float), cudaMemcpyHostToDevice, stream)); //启动cuda核，异步执行推理计算 context.enqueue(batchSize, buffers, stream, nullptr); //从显存到内存，将计算结果拷贝回内存中 //output是内存中的存储区域;buffers[outputIndex]是显存中的存储区域，存放模型输出. CHECK(cudaMemcpyAsync(output, buffers[outputIndex], batchSize * OUTPUT_SIZE * sizeof(float), cudaMemcpyDeviceToHost, stream)); //这个是为了同步不同的cuda流。 cudaStreamSynchronize(stream); //销毁流对象和释放显存 // Release stream and buffers cudaStreamDestroy(stream); CHECK(cudaFree(buffers[inputIndex])); CHECK(cudaFree(buffers[outputIndex]));&#125; 辅助函数用到 common.cpp 文件中的辅助函数：locateFile( ) 和 readPGMFile( ) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include "common.h"// Locate path to file, given its filename or filepath suffix and possible dirs it might lie in// Function will also walk back MAX_DEPTH dirs from CWD to check for such a file path//查找文件inline std::string locateFile(const std::string&amp; filepathSuffix, const std::vector&lt;std::string&gt;&amp; directories)&#123; const int MAX_DEPTH&#123;10&#125;; bool found&#123;false&#125;; std::string filepath; for (auto&amp; dir : directories) &#123; filepath = dir + filepathSuffix; for (int i = 0; i &lt; MAX_DEPTH &amp;&amp; !found; i++) &#123; std::ifstream checkFile(filepath); found = checkFile.is_open(); if (found) break; filepath = "../" + filepath; // Try again in parent dir &#125; if (found) &#123; break; &#125; filepath.clear(); &#125; if (filepath.empty()) &#123; std::string directoryList = std::accumulate(directories.begin() + 1, directories.end(), directories.front(), [](const std::string&amp; a, const std::string&amp; b) &#123; return a + "\n\t" + b; &#125;); throw std::runtime_error("Could not find " + filepathSuffix + " in data directories:\n\t" + directoryList); &#125; return filepath;&#125;//读取图片inline void readPGMFile(const std::string&amp; fileName, uint8_t* buffer, int inH, int inW)&#123; std::ifstream infile(fileName, std::ifstream::binary); assert(infile.is_open() &amp;&amp; "Attempting to read from a file that is not open."); std::string magic, h, w, max; infile &gt;&gt; magic &gt;&gt; h &gt;&gt; w &gt;&gt; max; infile.seekg(1, infile.cur); infile.read(reinterpret_cast&lt;char*&gt;(buffer), inH * inW);&#125; 日志类common.h文件中有个日志类： class Logger : public nvinfer1::ILogger 这是一个日志类，继承自 nvinfer1::ILogger 123456789101112131415161718192021222324252627// Logger for TensorRT info/warning/errorsclass Logger : public nvinfer1::ILogger&#123;public: Logger(): Logger(Severity::kWARNING) &#123;&#125; Logger(Severity severity): reportableSeverity(severity) &#123;&#125; void log(Severity severity, const char* msg) override &#123; // suppress messages with severity enum value greater than the reportable if (severity &gt; reportableSeverity) return; switch (severity) &#123; case Severity::kINTERNAL_ERROR: std::cerr &lt;&lt; "INTERNAL_ERROR: "; break; case Severity::kERROR: std::cerr &lt;&lt; "ERROR: "; break; case Severity::kWARNING: std::cerr &lt;&lt; "WARNING: "; break; case Severity::kINFO: std::cerr &lt;&lt; "INFO: "; break; default: std::cerr &lt;&lt; "UNKNOWN: "; break; &#125; std::cerr &lt;&lt; msg &lt;&lt; std::endl; &#125; Severity reportableSeverity&#123;Severity::kWARNING&#125;;&#125;; nvinfer1::ILogger 这个类位于 tensorRT头文件 NvInfer.h 中，此文件路径： /usr/include/x86_64-linux-gnu/NvInfer.h 把 ILogger 类摘出来： 123456789101112131415161718192021222324252627class ILogger&#123;public: //! //! \enum Severity //! //! The severity corresponding to a log message. //! enum class Severity &#123; kINTERNAL_ERROR = 0, //!&lt; An internal error has occurred. Execution is unrecoverable. kERROR = 1, //!&lt; An application error has occurred. kWARNING = 2, //!&lt; An application error has been discovered, but TensorRT has recovered or fallen back to a default. kINFO = 3 //!&lt; Informational messages. &#125;; //! //! A callback implemented by the application to handle logging messages; //! //! \param severity The severity of the message. //! \param msg The log message, null terminated. //! virtual void log(Severity severity, const char* msg) = 0;protected: virtual ~ILogger() &#123;&#125;&#125;; 可见这个类 是 builder, engine and runtime 的一个日志接口，这个类应该以单例模式使用。即当有多个IRuntime 和/或 IBuilder 对象时，也只能使用同一个ILogger接口。 这个接口中有个枚举类 enum class Severity 定义了日志报告级别，分别为 kINTERNAL_ERROR，kERROR，kWARNING和kINFO；然后还有一个纯虚函数 log( ) ，用户可以自定义这个函数，以实现不同效果的打印。 比如common.h 文件中Logger类的 log()函数，就是根据不同的报告级别向标准错误输出流输出带有不同前缀的信息。这个地方是可以自己定义的，比如你可以设置为输出信息到文件流然后把信息保存到txt文件中等。 以上就是使用tensorRT优化MNIST的LeNet的一个简单的例子，其实对于mnist来说，使用tensorRT加速的意义不大，因为这个模型本来就比较小，这里使用这个例子主要是为了学习tensorRT的用法。 参考 http://wiki.jikexueyuan.com/project/java-design-pattern/builder-pattern.html 史上最全设计模式导学目录（完整版） NVIDIA TensorRT | NVIDIA Developer Deploying Deep Neural Networks with NVIDIA TensorRT TensorRT Developer Guide TensorRT C++ API]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>TensorRT</tag>
        <tag>inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT(1)-介绍-使用-安装]]></title>
    <url>%2Farticles%2F7f4b25ce%2F</url>
    <content type="text"><![CDATA[1 简介TensorRT是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来，能在几乎所有的框架中进行快速和高效的部署推理。 TensorRT 是一个C++库，从 TensorRT 3 开始提供C++ API和Python API，主要用来针对 NVIDIA GPU进行 高性能推理（Inference）加速。现在最新版TensorRT是4.0版本。 TensorRT 之前称为GIE。 关于推理（Inference）： 由以上两张图可以很清楚的看出，训练（training）和 推理（inference）的区别： 训练（training）包含了前向传播和后向传播两个阶段，针对的是训练集。训练时通过误差反向传播来不断修改网络权值（weights）。 推理（inference）只包含前向传播一个阶段，针对的是除了训练集之外的新数据。可以是测试集，但不完全是，更多的是整个数据集之外的数据。其实就是针对新数据进行预测，预测时，速度是一个很重要的因素。 一般的深度学习项目，训练时为了加快速度，会使用多GPU分布式训练。但在部署推理时，为了降低成本，往往使用单个GPU机器甚至嵌入式平台（比如 NVIDIA Jetson）进行部署，部署端也要有与训练时相同的深度学习环境，如caffe，TensorFlow等。 由于训练的网络模型可能会很大（比如，inception，resnet等），参数很多，而且部署端的机器性能存在差异，就会导致推理速度慢，延迟高。这对于那些高实时性的应用场合是致命的，比如自动驾驶要求实时目标检测，目标追踪等。 所以为了提高部署推理的速度，出现了很多轻量级神经网络，比如squeezenet，mobilenet，shufflenet等。基本做法都是基于现有的经典模型提出一种新的模型结构，然后用这些改造过的模型重新训练，再重新部署。 而tensorRT 则是对训练好的模型进行优化。 tensorRT就只是 推理优化器。当你的网络训练完之后，可以将训练模型文件直接丢进tensorRT中，而不再需要依赖深度学习框架（Caffe，TensorFlow等），如下： 可以认为tensorRT是一个只有前向传播的深度学习框架，这个框架可以将 Caffe，TensorFlow的网络模型解析，然后与tensorRT中对应的层进行一一映射，把其他框架的模型统一全部 转换到tensorRT中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略，并进行部署加速。 目前TensorRT4.0 几乎可以支持所有常用的深度学习框架，对于caffe和TensorFlow来说，tensorRT可以直接解析他们的网络模型；对于caffe2，pytorch，mxnet，chainer，CNTK等框架则是首先要将模型转为 ONNX 的通用深度学习模型，然后对ONNX模型做解析。而tensorflow和MATLAB已经将TensorRT集成到框架中去了。 ONNX（Open Neural Network Exchange ）是微软和Facebook携手开发的开放式神经网络交换工具，也就是说不管用什么框架训练，只要转换为ONNX模型，就可以放在其他框架上面去inference。这是一种统一的神经网络模型定义和保存方式，上面提到的除了tensorflow之外的其他框架官方应该都对onnx做了支持，而ONNX自己开发了对tensorflow的支持。从深度学习框架方面来说，这是各大厂商对抗谷歌tensorflow垄断地位的一种有效方式；从研究人员和开发者方面来说，这可以使开发者轻易地在不同机器学习工具之间进行转换，并为项目选择最好的组合方式，加快从研究到生产的速度。 上面图中还有一个 Netwok Definition API 这个是为了给那些使用自定义的深度学习框架训练模型的人提供的TensorRT接口。举个栗子：比如 YOLO 作者使用的darknet要转tensorrt估计得使用这个API，不过一般网上有很多使用其他框架训练的YOLO，这就可以使用对应的caffe/tensorflow/onnx API了。 ONNX / TensorFlow / Custom deep-learning frame模型的工作方式： 现在tensorRT支持的层有： Activation: ReLU, tanh and sigmoid Concatenation : Link together multiple tensors across the channel dimension. Convolution: 3D，2D Deconvolution Fully-connected: with or without bias ElementWise: sum, product or max of two tensors Pooling: max and average Padding Flatten LRN: cross-channel only SoftMax: cross-channel only RNN: RNN, GRU, and LSTM Scale: Affine transformation and/or exponentiation by constant values Shuffle: Reshuffling of tensors , reshape or transpose data Squeeze: Removes dimensions of size 1 from the shape of a tensor Unary: Supported operations are exp, log, sqrt, recip, abs and neg Plugin: integrate custom layer implementations that TensorRT does not natively support. 基本上比较经典的层比如，卷积，反卷积，全连接，RNN，softmax等，在tensorRT中都是有对应的实现方式的，tensorRT是可以直接解析的。 但是由于现在深度学习技术发展日新月异，各种不同结构的自定义层（比如：STN）层出不穷，所以tensorRT是不可能全部支持当前存在的所有层的。那对于这些自定义的层该怎么办？ tensorRT中有一个 Plugin 层，这个层提供了 API 可以由用户自己定义tensorRT不支持的层。 如下图： 这就解决了适应不同用户的自定义层的需求。 2 优化方式TentsorRT 优化方式： TensorRT优化方法主要有以下几种方式，最主要的是前面两种。 层间融合或张量融合（Layer &amp; Tensor Fusion） 如下图左侧是GoogLeNetInception模块的计算图。这个结构中有很多层，在部署模型推理时，这每一层的运算操作都是由GPU完成的，但实际上是GPU通过启动不同的CUDA（Compute unified device architecture）核心来完成计算的，CUDA核心计算张量的速度是很快的，但是往往大量的时间是浪费在CUDA核心的启动和对每一层输入/输出张量的读写操作上面，这造成了内存带宽的瓶颈和GPU资源的浪费。TensorRT通过对层间的横向或纵向合并（合并后的结构称为CBR，意指 convolution, bias, and ReLU layers are fused to form a single layer），使得层的数量大大减少。横向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心。纵向合并可以把结构相同，但是权值不同的层合并成一个更宽的层，也只占用一个CUDA核心。合并之后的计算图（图4右侧）的层次更少了，占用的CUDA核心数也少了，因此整个模型结构会更小，更快，更高效。 数据精度校准（Weight &amp;Activation Precision Calibration） 大部分深度学习框架在训练神经网络时网络中的张量（Tensor）都是32位浮点数的精度（Full 32-bit precision，FP32），一旦网络训练完成，在部署推理的过程中由于不需要反向传播，完全可以适当降低数据精度，比如降为FP16或INT8的精度。更低的数据精度将会使得内存占用和延迟更低，模型体积更小。 如下表为不同精度的动态范围： Precision Dynamic Range FP32 $-3.4×10^{38} ~ +3.4×10^{38}$ FP16 $-65504 ~ +65504$ INT8 $-128 ~ +127$ INT8只有256个不同的数值，使用INT8来表示 FP32精度的数值，肯定会丢失信息，造成性能下降。不过TensorRT会提供完全自动化的校准（Calibration ）过程，会以最好的匹配性能将FP32精度的数据降低为INT8精度，最小化性能损失。关于校准过程，后面会专门做一个探究。 Kernel Auto-Tuning 网络模型在推理计算时，是调用GPU的CUDA核进行计算的。TensorRT可以针对不同的算法，不同的网络模型，不同的GPU平台，进行 CUDA核的调整（怎么调整的还不清楚），以保证当前模型在特定平台上以最优性能计算。 TensorRT will pick the implementation from a library of kernels that delivers the best performance for the target GPU, input data size, filter size, tensor layout, batch size and other parameters. Dynamic Tensor Memory 在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率。 Multi-Stream Execution Scalable design to process multiple input streams in parallel，这个应该就是GPU底层的优化了。 3 安装这里 是英伟达提供的安装指导，如果有仔细认真看官方指导，基本上按照官方的指导肯定能安装成功。 问题是肯定有很多人不愿意认真看英文指导，比如说我就是，我看那个指导都是直接找到命令行所在，直接敲命令，然后就出了很多问题，然后搜索好长时间，最后才发现，原来官方install guide里是有说明的。 这里使用的是 deb 包安装的方式，以下是安装过程，我是cuda 8.0 ，cuda9.0也是类似的。 进行下面三步时最好先将后面记录的遇到的问题仔细看看，然后回过头来按照 一二三 步来安装。 第一步： 1234$ sudo dpkg -i nv-tensorrt-repo-ubuntu1604-ga-cuda8.0-trt3.0-20171128_1-1_amd64.deb $ sudo apt-get update$ sudo apt-get install tensorrt 其中的deb包要换成与自己 cuda和系统 对应的版本。 第二步： 使用python2则安装如下依赖 1$ sudo apt-get install python-libnvinfer-doc 这个是为了安装一些依赖的：比如 python-libnvinfer python-libnvinfer-dev swig3.0 如果是python3则安装如下依赖 1$ sudo apt-get install python3-libnvinfer-doc 第三步： 1$ sudo apt-get install uff-converter-tf 这个是安装通用文件格式转换器，主要用在 TensorRT 与TensorFlow 交互使用的时候。 不过我安装的时候还是出问题了： 安装tensorRT之前要将cuda的两个deb包添加上，因为TensorRT依赖好多cuda的一些东西比如 cuda-cublas-8-0 ，我之前cuda是用runfile安装的，所以TensorRT安装时有些依赖库找不到导致出错，如下图： ​ 上面提示缺少依赖包，但是实际上 libnvinfer4 的包是tensorRT安装了之后才有的，那现在反而成了依赖包了，不管他，缺什么安装什么，但是还是出错，如下： 哇，还是缺少依赖包，这次是缺 cuda-cublas-8-0 ，现在知道了，缺的是cuda的相关组件。 后来把 cuda 的两个deb包安装之后就没问题了，cuda 8.0 的deb包 在这里 ，如下图，下载红框里的两个deb包。 如果用的是 runfile 的方式安装的cuda的话，很容易出错，因为网上大部分cuda安装教程都是用runfile的方式安装的。所以如果cuda就是用deb包安装的话，就没有这个问题，如果使用runfile安装的话，安装tensorRT之前要把这两个deb包安装上，安装方式如下： 12$ sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64.deb $ sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb 以上是自己摸索出来的，折腾了一番之后才发现原来官方的 install guide已经说明了，如下： The debian installation automatically installs any dependencies, but: requires sudo root privileges to install provides no flexibility as to which location TensorRT is installed into requires that the CUDA Toolkit has also been installed with a debian package. 注意最后一条，意思是如果用deb包安装TensorRT，那么前提是 你的CUDA也是用deb包安装的。 怪自己没有认真看，要是多花个5分钟仔细看一下，就不用折腾这么久了，由此深有感触，文档还是官方英文原版的最好，而且要认真看。 不过不知道用 runfile cuda+Tar File Installation tensorRT的组合安装方式是怎么样的，没试过。 tensorRT 3 支持CUDA 8 和 CUDA 9，但是只支持 cuDNN 7，我第一次安装的时候cuDNN是5.1的，结果总是出错，错误是啥忘记了，反正换成cuDNN 7就好了，这个官方指导也有说明，不过比较隐蔽，他是放在 4.2 Tar File Installation 一节说明的： Install the following dependencies, if not already present:‣ Install the CUDA Toolkit v8.0, 9.0 or 9.2‣ cuDNN 7.1.3‣ Python 2 or Python 3 我试过只要大版本是 cudnn7就可以。这个也容易忽略。 安装好后，使用 $ dpkg -l | grep TensorRT 命令检测是否成功，输出如下所示即为成功 安装后会在 /usr/src 目录下生成一个 tensorrt 文件夹，里面包含 bin , data , python , samples 四个文件夹， samples 文件夹中是官方例程的源码； data , python 文件中存放官方例程用到的资源文件，比如caffemodel文件，TensorFlow模型文件，一些图片等；bin 文件夹用于存放编译后的二进制文件。 可以把 tensorrt 文件夹拷贝到用户目录下，方便自己修改测试例程中的代码。 进入 samples 文件夹直接 make，会在 bin 目录中生成可执行文件，可以一一进行测试学习。 另外tensorRT是不开源的， 它的头文件位于 /usr/include/x86_64-linux-gnu 目录下，共有七个，分别为： 1234567/usr/include/x86_64-linux-gnu/NvCaffeParser.h/usr/include/x86_64-linux-gnu/NvInfer.h/usr/include/x86_64-linux-gnu/NvInferPlugin.h/usr/include/x86_64-linux-gnu/NvOnnxConfig.h/usr/include/x86_64-linux-gnu/NvOnnxParser.h/usr/include/x86_64-linux-gnu/NvUffParser.h/usr/include/x86_64-linux-gnu/NvUtils.h TensorRT4.0相比于3.0新增了对ONNX的支持。 tensorRT的库文件位于 /usr/lib/x86_64-linux-gnu 目录下，如下(筛选出来的，掺杂了一些其他nvidia库)： 12345678910111213141516171819202122232425262728293031323334/usr/lib/x86_64-linux-gnu/libnvinfer.so/usr/lib/x86_64-linux-gnu/libnvToolsExt.so/usr/lib/x86_64-linux-gnu/libnvinfer_plugin.a/usr/lib/x86_64-linux-gnu/libnvinfer_plugin.so.4/usr/lib/x86_64-linux-gnu/libnvcaffe_parser.so/usr/lib/x86_64-linux-gnu/libnvparsers.so.4.1.2/usr/lib/x86_64-linux-gnu/stubs/libnvrtc.so/usr/lib/x86_64-linux-gnu/libnvcaffe_parser.a/usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1/usr/lib/x86_64-linux-gnu/libnvvm.so/usr/lib/x86_64-linux-gnu/libnvinfer.a/usr/lib/x86_64-linux-gnu/libnvvm.so.3/usr/lib/x86_64-linux-gnu/libnvToolsExt.so.1/usr/lib/x86_64-linux-gnu/libnvrtc.so.7.5/usr/lib/x86_64-linux-gnu/libnvparsers.a/usr/lib/x86_64-linux-gnu/libnvblas.so.7.5/usr/lib/x86_64-linux-gnu/libnvToolsExt.so.1.0.0/usr/lib/x86_64-linux-gnu/libnvcaffe_parser.so.4.1.2/usr/lib/x86_64-linux-gnu/libnvinfer_plugin.so/usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so/usr/lib/x86_64-linux-gnu/libnvparsers.so/usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.7.5.18/usr/lib/x86_64-linux-gnu/libnvblas.so.7.5.18/usr/lib/x86_64-linux-gnu/libnvvm.so.3.0.0/usr/lib/x86_64-linux-gnu/libnvrtc.so/usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.7.5/usr/lib/x86_64-linux-gnu/libnvinfer.so.4.1.2/usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.390.30/usr/lib/x86_64-linux-gnu/libnvrtc.so.7.5.17/usr/lib/x86_64-linux-gnu/libnvblas.so/usr/lib/x86_64-linux-gnu/libnvinfer.so.4/usr/lib/x86_64-linux-gnu/libnvparsers.so.4/usr/lib/x86_64-linux-gnu/libnvinfer_plugin.so.4.1.2/usr/lib/x86_64-linux-gnu/libnvcaffe_parser.so.4 编译 将 /usr/src/tensorrt 文件夹拷贝到用户目录下，假设路径为 &lt;tensorrt_srcpath&gt; 。 第一个问题： 在 &lt;tensorrt_srcpath&gt;/tensorrt/samples 文件夹中有个 Makefile.config 文件，里面第4行： 1CUDA_VER?=cuda-$(shell dpkg-query -f '$$&#123;version&#125;\n' -W 'cuda-cudart-[0-9]*' | cut -d . -f 1,2 | sort -n | tail -n 1) 这一句是为了获取cuda版本的，我的机器是 CUDA 8.0 。我记得我第一次安装时，后面dpkg命令 输出的不是8.0，是一个很奇怪的数字，导致我不能编译 tensorRT 例程。 后来我直接在这句后面添加了一句： CUDA_VER=cuda-8.0 ，简单粗暴解决问题了。 这个问题好像是还是因为我之前安装 cuda 时是用 runfile 的方式安装的，用这种方式安装的cuda不会安装cuda的deb包，所以上面语句输出的是不对的，导致找不到cuda库目录，编译不能进行。 可以使用命令sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb ，安装deb包，就可以了。或者像我那样添加 CUDA_VER=cuda-8.0 也可以。 如果安装cuda就是使用deb包安装的话，就不会出现这个问题。 第二个问题： 如果机器上安装了多个cuda版本，像我这个机器上 cuda8.0，9.0，9.1都装上了，上面语句得到的就只是 CUDA_VER=9.1，如果安装的是其他版本cuda的TensorRT的话肯定是不对的。 可以直接在第4行下面添加： 1CUDA_INSTALL_DIR=/usr/local/cuda-9.0 3 TensorRT 使用流程这是个很简单的流程，先简单了解一下，以后会深入研究更高级的用法。 在使用tensorRT的过程中需要提供以下文件（以caffe为例）： A network architecture file (deploy.prototxt), 模型文件 Trained weights (net.caffemodel), 权值文件 A label file to provide a name for each output class. 标签文件 前两个是为了解析模型时使用，最后一个是推理输出时将数字映射为有意义的文字标签。 tensorRT的使用包括两个阶段， build and deployment： build：Import and optimize trained models to generate inference engines build阶段主要完成模型转换（从caffe或TensorFlow到TensorRT），在模型转换时会完成前述优化过程中的层间融合，精度校准。这一步的输出是一个针对特定GPU平台和网络模型的优化过的TensorRT模型，这个TensorRT模型可以序列化存储到磁盘或内存中。存储到磁盘中的文件称之为 plan file。 下面代码是一个简单的build过程： 1234567891011121314151617181920212223242526//创建一个builderIBuilder* builder = createInferBuilder(gLogger);// parse the caffe model to populate the network, then set the outputs// 创建一个network对象，不过这时network对象只是一个空架子INetworkDefinition* network = builder-&gt;createNetwork();//tensorRT提供一个高级别的API：CaffeParser，用于解析Caffe模型//parser.parse函数接受的参数就是上面提到的文件，和network对象//这一步之后network对象里面的参数才被填充，才具有实际的意义CaffeParser parser;auto blob_name_to_tensor = parser.parse(“deploy.prototxt”, trained_file.c_str(), *network, DataType::kFLOAT); // 标记输出 tensors// specify which tensors are outputsnetwork-&gt;markOutput(*blob_name_to_tensor-&gt;find("prob"));// Build the engine// 设置batchsize和工作空间，然后创建inference enginebuilder-&gt;setMaxBatchSize(1);builder-&gt;setMaxWorkspaceSize(1 &lt;&lt; 30); //调用buildCudaEngine时才会进行前述的层间融合或精度校准优化方式ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); 上面的过程使用了一个高级别的API：CaffeParser，直接读取 caffe的模型文件，就可以解析，也就是填充network对象。解析的过程也可以直接使用一些低级别的C++API，比如： 12ITensor* in = network-&gt;addInput(“input”, DataType::kFloat, Dims3&#123;…&#125;);IPoolingLayer* pool = network-&gt;addPooling(in, PoolingType::kMAX, …); 解析caffe模型之后，必须要指定输出tensor，设置batchsize，和设置工作空间。设置batchsize就跟使用caffe测试是一样的，设置工作空间是进行前述层间融合和张量融合的必要措施。层间融合和张量融合的过程是在调用builder-&gt;buildCudaEngine时才进行的。 deploy：Generate runtime inference engine for inference deploy阶段主要完成推理过程，Kernel Auto-Tuning 和 Dynamic Tensor Memory 应该是在这里完成的。将上面一个步骤中的plan文件首先反序列化，并创建一个 runtime engine，然后就可以输入数据（比如测试集或数据集之外的图片），然后输出分类向量结果或检测结果。 tensorRT的好处就是不需要安装其他深度学习框架，就可以实现部署和推理。 以下是一个简单的deploy代码：这里面没有包含反序列化过程和测试时的batch流获取 1234567891011121314151617181920212223242526272829303132333435363738394041// The execution context is responsible for launching the // compute kernels 创建上下文环境 context，用于启动kernelIExecutionContext *context = engine-&gt;createExecutionContext();// In order to bind the buffers, we need to know the names of the // input and output tensors. //获取输入，输出tensor索引int inputIndex = engine-&gt;getBindingIndex(INPUT_LAYER_NAME),int outputIndex = engine-&gt;getBindingIndex(OUTPUT_LAYER_NAME);//申请GPU显存// Allocate GPU memory for Input / Output datavoid* buffers = malloc(engine-&gt;getNbBindings() * sizeof(void*));cudaMalloc(&amp;buffers[inputIndex], batchSize * size_of_single_input);cudaMalloc(&amp;buffers[outputIndex], batchSize * size_of_single_output);//使用cuda 流来管理并行计算// Use CUDA streams to manage the concurrency of copying and executingcudaStream_t stream;cudaStreamCreate(&amp;stream);//从内存到显存，input是读入内存中的数据；buffers[inputIndex]是显存上的存储区域，用于存放输入数据// Copy Input Data to the GPUcudaMemcpyAsync(buffers[inputIndex], input, batchSize * size_of_single_input, cudaMemcpyHostToDevice, stream);//启动cuda核计算// Launch an instance of the GIE compute kernelcontext.enqueue(batchSize, buffers, stream, nullptr);//从显存到内存，buffers[outputIndex]是显存中的存储区，存放模型输出；output是内存中的数据// Copy Output Data to the HostcudaMemcpyAsync(output, buffers[outputIndex], batchSize * size_of_single_output, cudaMemcpyDeviceToHost, stream));//如果使用了多个cuda流，需要同步// It is possible to have multiple instances of the code above// in flight on the GPU in different streams.// The host can then sync on a given stream and use the resultscudaStreamSynchronize(stream); 可见使用了挺多的CUDA 编程，所以要想用好tensorRT还是要熟练 GPU编程。 4 Performance Results来看一看使用以上优化方式之后，能获得怎样的加速效果： 可见使用tensorRT与使用CPU相比，获得了40倍的加速，与使用TensorFlow在GPU上推理相比，获得了18倍的加速。效果还是很明显的。 以下两图，是使用了INT8低精度模式进行推理的结果展示：包括精度和速度。 来自：GTC 2017，Szymon Migacz 的PPT 可见精度损失很少，速度提高很多。 上面还是17年 TensorRT2.1的性能，这里 是一个最新的TensorRT4.0.1的性能表现，有很详细的数据展示来说明TensorRT在inference时的强劲性能。 后面的博客中会进一步学习 tensorRT，包括官方例程和做一些实用的优化。 参考资料 What’s the Difference Between Deep Learning Training and Inference? Discover the Difference Between Deep Learning Training and Inference GTC 2017，Szymon Migacz 的PPT NVIDIA TensorRT | NVIDIA Developer Deploying Deep Neural Networks with NVIDIA TensorRT TensorRT 3: Faster TensorFlow Inference and Volta Support tensorRT installation guide cuda installation guide NVIDIA TensorRT Performance Guide TensorRT 4 Accelerates Neural Machine Translation, Recommenders, and Speech ONNX]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>TensorRT</tag>
        <tag>inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础DL模型-Dilated Residual Networks-论文笔记]]></title>
    <url>%2Farticles%2F47648ee6%2F</url>
    <content type="text"><![CDATA[论文：Dilated Residual Networks CVPR2017代码：https://github.com/fyu/drn pytorch 现在用于分类的大多数卷积神经网络，都是将图片分辨率降低了32倍的。在输入固定的情况下（分类一般都是224×224），最后的输出是7×7的feature map，选择这个倍数，在减少模型的参数量和计算量的同时保证最后的feature map不至于丢失太多信息。这对于分类问题来说是比较合理的，但是对于分割或者定位或者检测来说，毕竟属于像素级别的分类任务，如果还是选择32的总步长，势必会丢掉一些信息。但是如果选择小的步长，又会使感受野变小。所以现在有些文章会采用 up-convolutions, skip connections, and posthoc dilation等操作在降低步长的同时，维持感受野。 这篇文章使用了 Dilated Convolution ，也就是空洞卷积，或者膨胀卷积，或者带孔卷积。空洞卷积的使用使得网络可以在减少步长的情况下，保持感受野不变。关于空洞卷积的原理见 [参考资料 1]，这里不再叙述。 Dilated Convolution 与ResNet的结合，所以本文叫做Dilated Residual NetWorks，简称DRN。 1 DRNResNet的总步长为32，由于直接降低步长会影响感受野，所以作者使用 Dilated Convolution 来改进ResNet。 如下图，DRN与ResNet的对比。 DRN中的Group4和Group5的步长stride变成了1，这导致整个DRN的总步长变为8. Group4中，第一层 dilation=1，本层感受野没有变，但是后续卷积层的感受野会变小，因此采用dilation=2，维持感受野不变 Group5与原来相比，相当于少了2个stride=2的层，为了维持感受野不变，使dilation=4 这里稍微解释下： 假设输入，输出为：Input: $(N, C_{in}, H_{in}, W_{in})$，Output: $(N, C_{out}, H_{out}, W_{out})$ 输出的计算是这样的： 感受野的计算 [参考资料 2,3]。 当然这种方法也是可以加到 Group2和3的，那样就达到总步长为1了，但是这样参数量会很多。文章的分析，步长为8已足够应对，分割这种像素级别的任务了。 以上就是对ResNet的基础改进，比较简单，只加了dilation。 2 Localization在针对定位或者分割任务时，作者还做了一点改动，如下图： 这就是将最后的average global pooling换成了 1×1 卷积，使得输出变成一个channel=n，n为类别数的feature map，这在分割中是比较常见的处理，输出的是一个28×28的score map，而其他文章中比较常用的是14×14大小的score map。28×28明显可以保留更多信息。 做Object Localization时的处理： 假设最后生成的score map是C×W×H的，C为类别，W,H为宽高，对对ImageNet来说就是 1000×28×28.令 $f(c,w,h)$ 为 坐标 $(w,h)$ 处的激活值后者叫做得分。 那么用下式表示 坐标 $(w,h)$处的类别： 坐标 $(w,h)$ 在所有C个类别中，得分最大的作为当前坐标的类别 c bounding box由下式确定： 类别为 $c_i$ ，且得分大于 阈值 $t$ 的那些坐标集合构成的bounding box 以上bounding box的确定，对于class $c_i$ ,肯定会确定下多个bounding box，使用下式找到与目标最贴切的那个。 更有趣的是，对这个 Object Localization 可以直接拿 分类网络来做，连fine-tuning都不用了。 3 DegriddingDilated Convolution 的网格化现象，这是由于空洞卷积本身的问题带来的。 比如下面 (c) 列，feature map上有很明显的网格化现象. 作者也给了一个图来解释,网格化现象产生的原因: 网格化现象会影响网络的性能，于是作者设计了去网格化（Degridding）的结构，如下图： DRN-A-18与ResNet18的结构类似，只做了stride 和 dilation的转换 DRN-B-26：为了消除网格化现象，在整个网络后面再加两个shortcut block 但是考虑到shortcut block中的shortcut会把输入层直接加到输出，还是会产生网格现象，所以取出shortcut connection 为了消除网格现象，作者的改进方法有如下三种： Removing max pooling 如上图，考虑到ResNet中 第一个卷积层后面的 max pooling，其提取到的信息高频部分占主要地位（figure 6(b)），这些高频信息会使后续的feature map中的网格化现象加重，因此作者移除了 max pooling 换成了stride=2的卷积层，figure 6(c)作为对比结果。 Adding layers 在网络后面添加 dilation 较小的卷积层，如Figure 5(b)，通过网络学习，消除网格现象。 Removing residual connections 因为shortcut connection 的存在，会使 level6（dilation=4）的输出直接加到 level7 的输出上（level7和8也是同样的），因此将shortcut connection 去掉，如Figure 5(c)。 下图是DRN-C结构中不同层的feature map可视化结果： level5（dilation=2）和level6（dilation=4）的网格现象还比较严重，而最终的 class activation 中的网格现象已经消除的很不错了。 4 Experiments4.1 Image ClassificationTraining is performed by SGD with momentum 0.9 and weight decay 10e-4. The learning rate isinitially set to 10e-1 and is reduced by a factor of 10 every 30 epochs. Training proceeds for 120 epochs total. ImageNet 分类实验，top-1和top-5错误率 同等深度情况下 DRN的效果比ResNet好 而且效果 DRN-C &gt; DRN-B &gt; DRN-A ，说明消除网格现象的措施是有用的 DRN-C-42与 ResNet101的结果相当，而深度只有不到一半，计算量也更少 4.2 Object Localization在ImageNet上的 Localization结果，也是错误率 还是DRN 好 DRN-C-26比ResNet101结果都要好，而深度只有26层 DRN-C比DRN-B和DRN-A效果更好 4.3 Semantic SegmentationCityscapes 数据集上的分割结果： baseline 是ResNet101，其结果是 66.6，DRN的效果要更好。 DRN-A-50虽然存在网格化现象，但仍然赶上 ResNet了 DRN-C-26比DRN-A-50还要好，而网络深度只有一半多一点，说明消除网格很有必要 下图是DRN不同配置的最终分割结果展示，DRN-C-26的结果已经很好了。 5 总结 对于分割任务，减少了步长，使最后用于分类的feature map更精细，并将dilated convolution与ResNet结合，保持感受野。 DRN对于分类，定位，分割的效果均好于以ResNet为backbone的模型。 大大减少了网络深度，而且减少了计算量。 参考资料 知乎：如何理解空洞卷积（dilated convolution）？ CNN中感受野的计算 卷积神经网络物体检测之感受野大小计算 A guide to receptive field arithmetic for Convolutional Neural Networks https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>basic dl models</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-An Analysis of Scale Invariance in Object Detection – SNIP-论文笔记]]></title>
    <url>%2Farticles%2F89e6ce5e%2F</url>
    <content type="text"><![CDATA[CVPR2018 the Best Student Entry in the COCO 2017 challenge论文：An Analysis of Scale Invariance in Object Detection-SNIP代码：https://github.com/bharatsingh430/snip 这篇论文看了几遍才看明白什么意思，哎，功力火候不够啊。。。不过感觉越看越有意思，文章思想还是很巧妙的。 这里还有个王乃岩大神写的 https://zhuanlan.zhihu.com/p/36431183 SNIP的剖析，写的很深入，要是看明白了这个，就不用看这篇文章了。 1 问题提出计算机视觉问题中，分类（classification）任务是最简单的，而且最近几年随着各种新的模型提出，ImageNet分类准确率逐步提高，现在已经能够超过人类的识别率，top-5准确率已经提高到98%左右（SENET）。但是对于检测（detection）来说，按照检测任务的评估方法（mAP/AP，关于mAP/AP的介绍可以参考这里），如果不考虑使用额外的数据，那么PASCAL VOC 排行榜最高为77.5（mAP），MS COCO排行榜最高为 73.0 （AP@0.5）。虽然分类与检测使用的评估标准不一样，分别是准确率（accuracy）和精度（precision），但是从发展的速度来看，明显检测任务的发展是比较缓慢的，这也说明了检测任务的复杂性和难度。检测任务为什么比分类难，一个宏观上的原因是由于 检测是一种密集型的分类任务（当然有些模型会用回归的方式来检测），除此之外还有很多其他原因，这里不详细分析。当然这里肯定要提出一个比较重要的原因，这篇论文也是针对这个原因提出的优化训练方法。 检测任务由于目标的大小，形状不一，因此在检测时需要处理的一个重要的点，是scale。首先小目标的scale太小了，其次大目标和小目标的scale跨度太大。这两个scale上的问题是检测任务难度大、制约检测精度的难点之一。关于scale的含义，可以简单的理解为 目标面积开平方，比如我们经常说某个图片是 224×224大小的，或者某个目标是 30×30 大小的，这个224和30就是scale。 接下来看一下ImageNet和COCO数据集中的图片在scale上的分布（现在的新的检测模型往往更关注COCO上的结果，VOC用的不多了）: 这个图乍看起来有点懵，横轴竖轴都是scale。横轴的scale是目标大小相对于图片大小的相对scale，竖轴是当前scale下的目标数量占数据集中所有目标数量的百分比。这应该是一个累加的统计，比如箭头所指的点代表的意思是：COCO数据集中，相对scale小于0.106的目标占所有目标数量的50%。 可知以下几点： ImageNet中目标scale分布比较均匀：有一半的目标其relative scale是 $\le 0.556$（media scale）的，大小目标数量基本平衡。 COCO中明显小目标占比更多：因为有一半的目标其relative scale是 $\le 0.106$（small scale）的（面积相当于小于原图面积的1%，注意平方关系），而且有90%的目标其relative scale是 $\le 0.472$ （media scale）的，只有剩下差不多10%才是 large scale的。 COCO中目标scale跨度大：media scale（0.472） 和 smallest scale（0.024）相差20倍。 上面的现象造成的后果是： 对于检测器来说要处理这么大跨度的scale是很困难的， scale invariance将会受到极大影响。 ImageNet与COCO数据集在scale上的分布差异，在迁移学习时将会造成domain-shift。 为了解决小目标检测困难，scale 变化跨度大等问题，有很多学者提出了很多方法： 结合深层feature map和浅层feature map预测小目标 使用dilated/deformable convolution（膨胀卷积）来增加感受野，这可以使检测器对scale跨度大的问题适应性更强（因为使用高分辨率图像或者上采样来增加小目标时，会使大目标变得更大，也不利于检测）。 在不同分辨率的图像上独立的预测不同scale的目标。 增加context信息 multi-scale training multi-scale inference on an image pyramid combined NMS 以上几种方法的代表性论文基本都是检测领域中比较重要的论文，具体看原文吧，这里不提了。 以上方法都解决了一些问题，作者提出了两个问题： 对获得更好的精度来说，图片的上采样是有必要的吗？检测用数据集中额图片一般大小都在 480×640左右，但是为了提升对小目标检测效果，现在的很多检测模型在训练时都会图片放大到 800×1000 左右的大小。能不能在ImsgeNet上训练一个总步长较小的模型，然后再将它迁移到检测数据集上检测小目标？ 在迁移学习时，是否应该将目标的大小都固定在一个范围比如从 64×64 到 256×256，是否应该将resize过的目标全部用于训练？ 2 Multi-scale Classification现在的很多检测模型为了提升对小目标的检测精度，一般都会在 800×1200的分辨率上训练图片，为了进一步提升对小目标的监测精度，在inference时会进一步放大图片，比如在1400×2000的分辨率上检测。 也就是说 train和test(inference)之间是存在domain shift 的。 当然从直观来说，放大图片确实会增加检测效果，但是作者做了很严谨的实验来证明这个现象。 如上图，是作者提出的三个用于做对照实验的模型。 CNN-B：是在高分辨率图像的数据集上训练的（ImageNet 224），然后测试时，先将ImageNet的图像下采样至 48x48, 64x64, 80x80, 96x96 and 128x128 大小，然后再上采样至224，将这个ImageNet称为 ImageNet up-sampled low resolution images to 224。 CNN-S：是在低分辨率图像的数据集上训练的（ImageNet 48x48, 64x64, 80x80, 96x96 and 128x128, 通过降采样完成），测试也是在相应分辨率上测试的。 CNN-B-FT：是在高分辨率图像的数据集上训练（ImageNet 224），而后又在上采样到224的低分辨率的数据集上fine tuning的（ImageNet up-sampled low resolution images to 224数据），测试也是在ImageNet up-sampled low resolution images to 224上。 上图是结果分析： （a）：CNN-B 在不同分辨率图像上的测试精度，注意CNN-B是在高分辨率图像上训练的，然后在 由不同低分辨率图像 上采样构成的 ImageNet 上测试的。很明显，测试图像与训练图像的分辨率差别越大，效果越差。使用网络没有训练过的图像进行测试，结果只能是次优的，至少对于分类问题来说是这样。 （b），（c）：三种训练好的模型在 48×48 (96×96)大小的低分辨率图像上的测试精度，CNN-B的效果最差，CNN-S次之，CNN-B-FT最好。 直接用低分辨率图像训练（CNN-S，减小步长），然后低分辨率测试，效果是比较好的。 这给我们的直接启发是如果检测时想要提升对小目标的检测效果，应该专门在低分辨率的ImageNet上训练一个对低分辨率图像最优的分类模型（通过减小步长），然后再迁移到检测任务上，用于检测小目标。 先从高分辨率图像训练然后再在上采样后的低分辨率图像上fine tuning 效果最好，说明分类模型在高分辨率图像中学习到了其他更有用的信息，上采样是有用的。 这给我们的直接启发是在高分辨率图像上先预训练，然后再在低分辨率图像上fine tuning，对于分类小目标是有利的，应该比前一条提到的方法更有效。 进一步： 在高分辨率图像上先预训练，然后再在低分辨率图像上fine tuning，对于分类小目标时有利的。 如果检测时的proposals与ImageNet预训练模型的分辨率相近，那么可以减少domain shift。 上采样确实是有用的 现有的检测模型也确实都是这么做的，先将图片上采样（800×1200），然后使用在高分辨率图像（ImageNet）上预训练的模型fine tuning。 3 Multi-scale Detection 前面提到train和test时分辨率不同将会存在domain shift，但是由于GPU现存的限制，这个差异一般都会存在。事实检测模型经常会在800×1000的分辨率上训练，然后再1400×2000的分辨率上测试。 下面是作者做的几个实验，作者固定了测试时的分辨率为 1400×2000 ，然后通过调整不同的输入来对比COCO数据集上小目标（小于32×32的目标）的检测精度。作者主要使用了800×1000和1400×2000这两种输入分辨率，然后再梯度反向传播时，把所有scale的ROI的梯度回传。因此下表 800all,1400all，就是代表这个意思。 首先是800all和1400all的对比：按照前面第2节的分析，关于分类问题高分辨率图片是可以提升分类效果的，所以理论上使用大分辨率的图像检测，应该也会提升小目标的检测效果的，而且1400all是不存在train和test的domain shift的，因为train和test的分辨率是一样的。但是实验发现，这里 1400all的结果竟然低于800all，作者的分析是有大分辨率图像中medium和large 的目标也会变得更大，这也是不利于检测的，所以说，虽然大分辨率有利于小目标检测，但是损害了中等和大目标的检测效果。 1400(&lt;80px)和1400all的对比：为了验证上面的解释，作者又做了一个实验，输入还是1400all，但是将目标大于80px的目标全部都忽略掉，只保留小目标，按说排除了大目标的影响，小目标的检测效果应该有所提升，但是很不幸，还是很差，比800all都差。作者的分析是，移除大目标会损失目标的多样性。这个损失掩盖了移除大目标地优势。 MST：最终作者还用 multi-scale training，做了实验（figure5.3），结果还是差不多，作者分析的原因是，小scale 图像中的小目标更小了，大scale 图像中的目标更大了，这种极端尺寸的目标会损害性能。 SNIP：性能最牛，下面再分析。 4 SNIPScale Normalization for Image Pyramids (SNIP). 前面说了那么多，这一部分才是文章的核心。 通过上面的分析，SNIP的思想是比较明显的。SNIP训练的方法是一种改进的multi scale training，而且他保证了目标实例的大小在224×224大小左右，这个与ImageNet与训练的分类网络的尺度是统一的，减少了domain shift。 SNIP在训练时只考虑了那些在合适scale范围内的ROI，那些极端尺寸的ROI都直接被忽略。结果在table 1中也展示了，效果是最好的。 作者使用了R-FCN的基础模型。 因为是multi-scale training，在某个分辨率 i 上，如果一个ROI的大小落在了一个有效范围内，比如 $[s_i^c,e_i^c]$ ，那么这个ROI回被标记为有效；其余标记为无效，无效的ROI在反向传播时不予考虑。这样就可以将极端尺寸的ROI过滤掉。 实际在操作中，ground truth box在不同分辨率的图片上会随之放大或缩小，因此过大或过小的 gt box就被标记为无效。在RPN层生成anchor或者 proposals时，anchor如果与无效gt box的IOU大于0.3，那么这些anchor就会在反向传播时被忽略。在测试或者inference时，每张分辨率不同的图片都运行这样一个检测器，对于检测出来的bounding box如果范围落在有效范围 $[s_i^c,e_i^c]$ 之外，就会被忽略。最终使用NMS。这样就保证了每个分辨率图片上的目标scale是一样的。 在 ROI pooling 时保证pooling之后的 ROI 在224×224大小（这地方应该有个resize操作或者resale操作），这样就减少了与ImageNet预训练的分类网络的domain shift。 5 训练训练使用的图片分辨率比较高（1400x2000），为了减少GPU显存，作者在原图上进行crop，裁剪出1000×1000大小的chip。裁剪过程是：先随机生成50个1000×1000大小的chip，然后选取包含包含最多目标的chip保留。如果图像分辨率是 800x1200 or 480x640 就不用这么随机裁剪了。 文章使用了三个scale的image pyramid来做multi scale training，分别是(480, 800), (800, 1200) and(1400,2000), 训练策略： 7 epochs for the classifier while RPN is trained for 6 epochs warmup learning rate of 0.00005 for 1000 iterations after which it is increased to 0.0005. Step down is performed at 4.33 epochs for RPN and 5.33 epochs otherwise. OHEM online hard examplemining implementation is in MxNet and training is performed on 8 Nvidia P6000 GPUs. Batch size is 1 per GPU and we use synchronous SGD. For efficient utilization of multiple GPUs in parallel, images of only one resolution are included in a mini-batch. 三个scale的image使用的object 的scale分别是： images of resolution (1400,2000), the valid range in the original image (without up/down sampling) is [0, 80], at a resolution of (800,1200) it is [40, 160] and at a resolution of (480,800) it is [120, 1]. 随后这些object 回被re-scale到224×224大小。 6 实验结果实验部分详细的描述不提了。 7 总结 这篇文章通过对比实验验证了上采样对提升分类性能有很大帮助，基于此分析作者提出了一种 Image Pyramid Network（IPN）用于目标检测，在训练和测试时都使用与ImageNet分类网络同样的image scale。 由于过小和过大的目标分别在 小的scale上和大的scale上较难识别，因此作者提出了一种SNIP的训练方法，在反向传播时将极端尺寸的目标忽略。 参考资料 王乃岩大神写的 https://zhuanlan.zhihu.com/p/36431183，写的很深入本质]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-SNIPER-Efficient Multi-Scale Training-论文笔记]]></title>
    <url>%2Farticles%2Ff0c1556d%2F</url>
    <content type="text"><![CDATA[论文：https://arxiv.org/abs/1805.09300代码：https://github.com/MahyarNajibi/SNIPER 1 问题提出这篇文章提出的SNIPER(Scale Normalization for Image Pyramids with Efficient Resampling)算法是一种多尺度（multi-scale）训练算法。提出的multi-scale算法不敢说很有创造性，但是是一种比较新颖的思想。 为了实现对图片上不同大小目标的检测，需要针对multi-scale提出有效方法。现在很多先进的目标检测算法，比如Faster R-CNN/Mask R-CNN，它们实现multi-scale的思路除了使用anchor的思想之外，一般还都包括一种常用的手段，那就是 image pyramid，这方法在早期的目标检测算法中是经常使用的方法。 如下图，image pyramid 和 anchor 方法实现multi-scale的思路展现的很清楚了。 现在大多数目标检测算法为了提高精度，除了使用anchor的思想外，一般都会把image pyramid的方法加进去，但是我们知道的，这种方法肯定会严重影响速度，不管是训练还是inference，不过但凡是用了这种方法的一般都是为了追求高精度，而暂时不注重速度。 image pyramid 影响速度的原因也很明了，每一个scale的图片的所有像素都参与了后续的计算。比如一个3scale的image pyramid（分别是原图的1倍，2倍，3倍），它要处理相当于原图14倍的像素. 在CVPR2018上有这样一篇文章，An Analysis of Scale Invariance in Object Detection – SNIP 。这篇文章曾经表述这样一个观点，在multi-scale training时，最好是将极端分辨率情况下的目标的梯度忽略。比如在 3scales的情况下，应该将较大分辨率图像上的较大目标和较小分辨率图像上的较小目标的梯度忽略。 关于scale的问题先回顾一下RCNN和Fast R-CNN R-CNN R-CNN是比较老的目标检测框架了，但是很多现代新型的目标检测算法都是从它演变而来的。在R-CNN检测的第二步里对通过selective search生成的候选区域（region proposals）做了warped操作，一般是warp成224x224大小的图片，这个是CNN中经常使用的输入图片尺寸。因为R-CNN相当于一个分类问题，因此将不同大小的候选区域缩放成固定大小，实际上引入了尺度不变性 (scale invariant )，这也解决了 multi-scale 的问题，任何一个唯一尺寸的候选区域都相当于一个scale。由于候选区域是由selective search生成的，因此有一定的先验知识，换句话说每个不同大小的候选区域，其scale也代表了object的scale。虽然第3步输入的是固定大小的region proposals（看起来是没有考虑scale的），但是scale已经在提取候选区域时和缩放时完成了，实际上输入的warped region proposals是有scale的。它最主要的问题在于提取特征时，CNN需要在每一个候选区域上跑一遍，候选区域之间的交叠使得特征被重复提取, 造成了严重的速度瓶颈, 降低了计算效率。 Fast R-CNN Fast RCNN的提出主要是解决了R-CNN的速度问题，它通过将selective search生成的候选区域映射到feature map上，实现了候选区域间的卷积计算共享，同时由于共同参与计算，Fast R-CNN也能更多的获取到全局信息（contex）。而同时提出的 ROI pooling也将对应到feature map上的候选区域池化到相同的大小，ROI pooling 在某种程度上也可以引进尺度不变性 (scale invariant )，因为可以将不同大小的region proposal（连同 object）在feature map层面池化到固定大小。但它的问题是前面的卷积层部分是只在一个scale上进行的，这正是为了实现卷积计算共享所带来的一点负面影响，因为是卷积计算共享，所以不同大小的目标没有办法进行不同比例的缩放操作，所以相对于原图大小来说，它们都处于一个scale。这影响了尺度不变性 (scale invariant )，因此为了弥补这一点，在训练或者inference时，会使用image pyramid的操作。 但是image pyramid有一个问题，就是在缩放原图时，object也会跟着缩放，这就导致原来比较大的object变得更大了，原来更小的object变得更小了。这在R-CNN中是不存在这个问题的。 SNIPER 相当于综合了R-CNN 在scale上的优点 和Fast R-CNN在速度上的优点。 2 SNIPERSNIPER(Scale Normalization for Image Pyramids with Efficient Resampling) chips 首先介绍一下这篇文章提出的 chips 的概念，原文的解释是 scale specific context-regions (chips) that cover maximum proposals at a particular scale. chips是某个图片的某个scale上的一系列固定大小的（比如KxK个像素）的以恒定间隔（比如d个像素）排布的小窗（window） ，每个window都可能包含一个或几个objects。 这个跟sliding window有点像。 Chip Generation 首先是chips的生成： 假设有n个scale，{s1,s2,…,si,…sn}，每个scale的大小为 $W_i × H_i$ ，将原图缩放至对应比例后，$K×K$ 大小的chip（对于COCO数据集，论文用的512×512）以等间隔（d pixels）的方式排布，注意是每个scale都会生成这些chips，而且chip的大小是固定的，变化的是图片的尺寸。这跟anchor的思想刚好相反，因为anchor中，不变的是feature map，变化的是anchor。 实际上这篇论文输进网络用于训练的是这些chip，每个chip都是原图的一部分，包含了若干个目标。 一张图的每个scale，会生成若干个Positive Chips 和 若干个 Negative Chips 。每个Positive Chip都包含若干个ground truth boxes，所有的Positive Chips则覆盖全部有效的ground truth boxes。每个 Negative Chips 则包含若干个 false positive cases。这些在下面详细讲解。 Positive Chip Selection 每个scale，都会有个area range $R^i =[r_{min}^i, r_{max}^i]，i \in [1,n]$ ，这个范围决定了这个scale上哪些ground truth box是用来训练的。所有ground truth box中落在 $R^i$ 范围内的ground truth box 称为有效的（对某个scale来说），其余为无效的，有效的gt box集合表示为 $G^i$ 。从所有chips中选取包含（完全包含）有效 gt box最多的chip，作为Positive Chip，其集合称为 $C^i_{pos}$ ，每一个gt box都会有chip包含它。因为 $R^i$ 的区间会有重叠，所以一个gt box可能会被不同scale的多个chip包含，也有可能被同一个scale的多个chip包含。被割裂的gt box（也就是部分包含）则保持残留的部分。 如下图，左侧是将gt box和不同scale上的 chip都呈现在了原图上，这里为了展示，将不同scale的图片缩放回原图大小，所以chip才有大有小，它们是属于不同scale的。右侧才是训练时真正使用的chips，它们的大小是固定的。可以看出这张图片包含了三个scale，所以存在三种chip，chip中包含有效的gt box（绿色）和无效的gt box（红色）。 从原图也可以看出，原图中有一大部分的背景区域（没被postive chip覆盖的区域）被丢弃了，而且实际上网络真正输入的是chip（比原图分辨率小很多），这对于减少计算量来说很重要。 SNIPER 只用了4张低分辨率的chips 就完成了所有objects的全覆盖和multi-scale的训练方式。 这个地方很巧妙，在不同的scale上截取相同大小的chip，而且包含的是不完全相同的目标。这个方式很接近 RCNN。对较小的目标起到一种 zoom in 的作用，而对较大的目标起到一种 zoom out的作用。而对于网络来说，输入的还是固定大小的chip，这样的结构更加接近于分类问题网络的拓扑。 所以说 SNIPER 相当于综合了R-CNN 在scale上的优点 和Fast R-CNN在速度上的优点。 这个地方太精妙了。 Negative Chip Selection 如果至包含上述的 postive chip那么由于还是有大部分的区域背景，所以对背景的识别效果不好，导致假正例率比较高。因此需要生成 negative chip，来提高网络对背景的识别效果。现在的目标检测算法因为在multi-scale训练时，所有的像素都参与了计算，所以假正例率的问题相对这个算法没那么严重（但是计算量大），但是这里因为抛弃了很多背景，所以会使假正例率增加。为了降低假正例率和维持计算量不至于上涨太多，一个简单的解决办法就是使用一个精度不是很高的rpn生成一些不太准确的proposals，这些proposals中肯定有一大部分都是假正例。连这个精度不是很高的rpn都没有生成proposals的地方则有很大可能是背景区域，那么训练时把这一部分抛弃即可。 negative chip 的生成是这样的：首先使用训练集训练一个不是很精确的RPN，生成一系列 proposals，对每个scale i，移除那些被 positive chip 所覆盖的proposals，因为这部分proposal与gt box重合在一起了。选取包含至少M个在 $R^i$ 范围内的proposals的所有chips作为 negative chip，其集合称为 $C^i_{neg}$ 。 训练时，每张图片在每个epoch中只从 negative chip 池中选择固定数量的 negative chip。这个地方训练时怎么弄的，文章解释不是很清楚，恐怕得看代码才能明白。 如下图是 negative chip 的生成示例： Label Assignment 回归时标签的赋值与 Faster R-CNN是类似的，文章说是端到端的训练，但是我个人觉得是没有把生成 positive/negative chip的过程算进去，这一部分好像是独立进行的，等chip生成后才在chip上进行 类似于 Fster R-CNN的训练。因为生成chip时用的RPN是不太精确的，跟训练检测网络时的RPN应该不能共享。不过这个地方具体是怎样的也只能看代码才能明白了。 其他的设置基本跟 Faster R-CNN 差别不大，只是正样本比例啊，OHEM啊，损失函数啊有所不同，这里不再赘述。 Benefits 许多网络为了增加精度，都会使用image pyramid而且会使用较高分辨率的图片。SNIPER 则摆脱了对较高分辨率图像的依赖。而且由于SNIPER 训练时使用的是较低分辨率的chip，所以batch也可以做的很大，比如可以做到每张卡20的batch（不过大神用的都是tesla V100）。 3 实验结果 4 总结 这篇文章的 multi-scale 训练方式很新奇，但是可能在实现上会有点复杂 证明了在高分辨率图片上进行训练这一常用技巧并不是提升精度必须要使用的策略]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-Cascade R-CNN-论文笔记]]></title>
    <url>%2Farticles%2F1c9cf9be%2F</url>
    <content type="text"><![CDATA[论文：Cascade R-CNN: Delving into High Quality Object Detection代码：https://github.com/zhaoweicai/cascade-rcnn 这是一篇针对R-CNN系列的两步检测模型 提出的改进方法。主要是针对匹配策略提出了 multi-level 的方式。 1 问题提出检测=分类+定位，对于分类来说，标签就是标签，直接计算损失。但是对于定位问题来说，它的标签处理稍微麻烦一点。现在的目标检测模型对于定位问题的解决方法主要是回归。我们来看一下在Faster R-CNN中是如何进行回归的： 现在的问题是生成的 anchor box是有很多的，那么势必会导致只有少部分是包含目标或者是与目标重叠关系比较大的，那当然只有这以少部分才是我们的重点观察对象，我们才能把他用到上述提到的回归过程中去。因为越靠近标签的default box回归的时候越容易，如果二者一个在最上边，一个在最下边，那么回归的时候难度会相当大，而且也会更耗时间。 确定这少部分重点观察对象的过程就是匹配策略。换句话说其实就是为了确定一部分对象作为正样本。 所以对于回归问题来说，需要把ground truth box转换一下，也就是这里的正样本。 确定正样本的过程普遍的做法是：计算anchor box与ground truth 之间的IOU，大于某一个阈值就认为这是一个正样本，这个阈值一般是0.5。 但是实际上严格来说使用0.5的阈值偏低，造成的结果就如 Figure 1(a)那样会有很多，“close” false positives ，通俗的说就是有很多重复的没用的框，可以称为噪声，而我们期望的是输出像 Figure 1(b)那样。 但是提高IOU阈值是会出问题的，因为提高 IOU阈值会使效果变差，主要有两个原因： 提高IOU会导致正样本数量过少，然而模型比较大，会导致过拟合。 detector肯定是在某一个IOU阈值上训练的，但是inference时，生成anchor box之后，这些anchor box跟ground truth box会有各种各样的IOU，对于这些IOU来说detector是次优化的。 文章解决的也是这个问题。 2 Cascade 结构的提出 首先看一下目标检测中广泛使用的定位损失： 分类损失 2.1 两种可能的解决方法 iterative bounding box regression, denoted as iterative BBox. 如 Figure 3(b). 存在的问题： 三个head部分是一样的，也就是说都是针对某一个IOU阈值，比如0.5，是最优化的，但是当inference时，真正的IOU大于0.5以后他是次优化suboptimal 的。 很明显级联之后，每个输出的box的分布是会变化的如上图 Figure (2)，所以相同的head对于后面的结构来说是次优化的suboptimal 。 第二种方法： 也就是Figure 3(c). 存在的问题：不同IOU阈值的分布是不均衡的，如上图左。所以上面的损失是不均衡的。 2.2 Cascade R-CNN 如图 Figure 3(d): 损失函数： 3 试验结果]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-Focal Loss for Dense Object Detection-论文笔记]]></title>
    <url>%2Farticles%2F6c9f6e2c%2F</url>
    <content type="text"><![CDATA[论文：Focal Loss for Dense Object Detection ICCV2017 Best Student Paper Award代码：https://github.com/facebookresearch/Detectron 1 问题提出 two-stage detector ：精度高，速度慢 the first stage ： 生成一个 稀疏的候选框的集合 （a sparse set of candidate proposals ），过滤掉大部分负样本（ filtering out the majority of negative locations） 这第一阶段可以采用：selective search，edgebox，RPN等算法完成，是一个弱检测器。 可以极大的降低候选框的数量(e.g., 1-2k) the second stage：对候选框进行 前景目标 或 背景 的分类 （classifies the proposals into foreground classes / background） ​ 对类别不平衡问题的解决 ： sampling heuristics：such as a fixed foreground-to-background ratio (1:3), 通过RMS将前景目标和背景（或者负样本和正样本）的比例保持在 1:3 . online hard example mining (OHEM) : 一种训练方法 one-stage detector ：精度低，速度快 生成一个 稠密的候选框的集合 (a dense set of candidate proposals)，直接进行回归。 这个密集的候选框的集合 数量在 100k 左右。然而正样本只有一少部分，大部分都是背景，这个类别不平衡问题比两步检测的方式要严重的多。 解决方法：可以使用上面提到的两种方法，但是由于 易分类样本数量太多 损失函数会被这些样本产生的损失湮没。 这个问题可以使用 bootstrapping 的方法来解决。不过这篇文章提出了一种更有效的方式。 ​ 类别不平衡是导致 one-stage 检测方法精度不如 two-stage 的主要原因。 这是比较容易理解的：比如假设一张图片只包含一个目标，一开始有 100k 个可能包含目标的区域，可以先使用一个弱检测器挑选出 1k 个候选区域，然后再用一个强分类器对这 1k 个候选区域进行筛选，挑出最接近ground truth 的区域。这比直接从100k个初始候选区域中直接挑选1个要更精确。而且类别不平衡问题也没有 one-stage 方法的严重。 但是 two-stage 的代价就是：因为是两次分类所以速度上肯定会慢很多。 这篇文章主要是针对类别不平衡问题，解决 one-stage 检测方法精度不高的现象。 2 解决方法 Focal Loss避免损失函数被 易分类的负样本 产生的损失湮没，注意 是 易分类的负样本 可以从以下两个方面解决 修改 正样本 和 负样本 对损失函数的贡献量，使二者平等 修改 难分类样本 和 易分类样本 对损失函数的贡献量，使二者平等 以下是 Focal Loss 的设计过程 2.1 交叉熵损失 Cross Entropy (CE)先看下经典的 交叉熵损失（cross entropy ）： $y\in\{±1\}$ ，代表正负样本 $p\in[0,1]$ ，是分类的概率，代表正样本的概率（y=1），那么 $1-p$ 代表负样本的概率（y=-1） 用下式改写 交叉熵损失改写为： 2.2 正负类别平衡的交叉熵损失 Balanced Cross Entropy修改 正样本 和 负样本 对损失函数的贡献量 给正样本的损失 乘上 一个权重 $\alpha \in [0,1]$ , 给负样本的损失乘上 $1-\alpha$ 。 这样可以使二者对损失函数的贡献平等。使用同样的方法用 $\alpha _t$ 代表 $\alpha$ . 将交叉熵损失改写为： 2.3 难易类别平衡的损失 Focal Loss修改 难分类样本 和 易分类样本 对损失函数的贡献量 引进一个 调节因子 $(1-p_t)^{\gamma}, \gamma \ge 0$ , 改写损失函数： 当 样本错分类（难分类样本），并且 $p_t$ 很小时，调节因子 趋向于1，而 $FL(p_t)$ 与 $CE(p_t)$ 接近； 如果是 易分类样本， $p_t$ 很大，调节因子趋向于0，损失也为0。 看下图: 最终文章使用的损失函数是这样的： 3 RetinaNet Detector文章构建的一个 one-stage 检测模型，使用了上述提出的损失函数 3.1 实验结果 参考资料Huber Loss function Focal Loss function]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-DSOD-论文笔记]]></title>
    <url>%2Farticles%2Fc0b67e9a%2F</url>
    <content type="text"><![CDATA[论文：DSOD: Learning Deeply Supervised Object Detectors from Scratch代码：https://github.com/szq0214/DSOD 0 简介这篇文章提出了一种不需要预训练的目标检测模型。 现在的目标检测模型为了获得较高的精度，普遍采用的做法是：拿在ImageNet上预训练过的分类网络 fine-tuning。fine-tuning 也可以称为是迁移学习，它的好处有两个：（1）有很多针对于ImageNet公开的深度网络模型，可以很方便将这些公开的网络用到目标检测问题上；（2）fine-tuning 训练要比完全从头训练（train from scratch）更省时间，而且fine-tuning对数据量的要求不像分类问题那么严格，可以使用较少的标注数据训练得到比较好的结果。 但是使用预训练的网络存在以下三个问题： Limited structure design space：网络设计空间受限。预训练的网络往往都是计算量，参数量很大的网络，应用到目标检测上之后，虽然针对 head 部分的设计可以随心所欲，但毕竟前面的backbone是固定的，因此计算量和参数量基本固定，网络调整的灵活性不大。 Learning bias：学习的偏置问题。分类问题和检测问题的loss函数以及类别分布是不同的，作者认为迁移学习时可能会引入 bias ，二者的优化空间是不同的，有可能使检测问题陷入局部最小值。即便fine-tuning可以在一定程度上消除这个bias问题，但是能够消除多少是不确定的，作者认为这并不能从根本上解决问题。 Domain mismatch：应用领域的不匹配。当ImageNet与你要应用的领域有较大的不同时，比如深度图，医疗影像等数据集，由于数据集本身的 huge mismatch 也会对迁移学习后的结果又一定影响。 基于以上问题，作者认为根本的解决方法是构建一个网络，并且 train from scratch 。作者也构建了这么一个网络成为 DSOD 。DSOD的结果确实要好一点。 作者关于迁移学习存在的问题只是理论与经验上的推断，确实有一定道理，但是也仅仅是推断而已，作者并没有用大量的试验来验证。关于迁移学习的问题，刚发现了一篇论文 Do Better ImageNet Models Transfer Better? 使用了大量的试验来验证 关于迁移学习的问题，这个后面会仔细阅读一下。 这篇文章的主要贡献： 全球首例可以 train from scratch 的目标检测网络模型。 在设计过程中提出了应该遵守的原则。 在 PASCAL VOC 2007, 2012 and MS COCO datasets 三个数据集上获得了state-of-the-art 的精度、处理速度和模型紧凑性。 1 DSODDSOD 的设计思想可以从下图中看出。 DSOD是在SSD的基础上发展而来的，DSOD的不同在于，不管是在backbong还是head部分，DSOD都借鉴了DenseNet的设计思想。关于DenseNet的详解，可以看这里。 DSOD的详细网络结构见下表： 点击 这里 查看这个网络结构的可视化。 网络的一些设计原则： Principle 1: Proposal-free. 目前已有的目标检测模型主要有两大类： R-CNN，Fast R-CNN, Faster R-CNN and R-FCN 等 region proposal based 的两步检测模型 YOLO, SSD等 proposal-free的一步检测模型 作者的实验表明 region base的两步检测模型如果train from scratch将不会收敛，作者认为是ROI池化的存在使梯度不能平稳地从region-level 传递到 feature map（RoI pooling generates features for each region proposals, which hinders the gradients being smoothly back-propagated from region-level to convolutional feature maps.），然而对预训练模型 fine-tuning 可以收到比较好的结果是因为 ROI池化之前的层拥有一个比较好的初值，而不需要随机赋初值，这个在train from scratch时是不存在的。 作者使用proposal-free的SSD模型，得到了stateof-the-art 的精度和速度。 Principle 2: Deep Supervision. 这个原则实际上就是如何选择深层神经网络。原文是这么说的 The central idea is to provide integrated objective function as direct supervision to the earlier hidden layers, rather than only at the output layer. 根本思想是提供一个集成的objective function，也就是损失函数不仅要给输出层提供监督信号，还要对网络中前面的非输出层提供监督信号。看到这里我们很显然的想到了 ResNet 和 DenseNet 都是符合这个要求的，二者都存在skip-connection。作者也提到了这两个网络，不过作者最后使用的是DensNet，毕竟DenseNet在ImageNet上的效果略胜一筹。 此外还使用了 Transition without Pooling Layer 来增加dense block数量。 Principle 3: Stem Block. stem block是 inception v3和v4中的结构，就是除了那些卷积block之后，主干网络中与传统 CNN 一样的部分，就是卷积+池化的堆叠结构，不存在skip-connection等特殊结构。 DenseNet的前面几层，也就是Dense block之前的stem部分是下面这样的， DSOD中被修改成了这样的： Principle 4: Dense Prediction Structure. Dense Prediction Structure.中有个特殊的处理，看下图： 每一个scale中的feature map（channel）中只有一半是通过前面层学习得到的，还有一半是直接通过降采样得到的。上面图中的红色箭头所指的黑色箭头就是代表这个意思。 在这个图中用于预测的层也采用了 dense block的结构（head指scale2-scale5预测层，scale1是在backbone中的），但实际上这一部分也可以使用与SSD一样的普通的卷积层（plain），这一点作者也做了相关实验来对比二者之间的差别。 2 训练策略框架：caffe 硬件：NVidia TitanX GPU 训练策略上大部分都沿用了SSD的相关策略，有略微差异，这一部分不再叙述。 3 实验结果3.1 对照实验DSOD本身关于densenet 网络配置参数的对照实验 这里只提一点：当backbone与prediction Layer都采用 densenet的结构时，效果最好。 其他关于压缩系数 growth rate 以及 各种channel数量的影响不再赘述 DSOD提出的设计原则之间的对照实验 注意一点：当Faster R-CNN train from scratch 时，是不收敛的 SSD train from scratch 时 是可以收敛的，但是精度会降低，当结合densenet构成DSOD时，精度一下又上去了。 参数量减少很多，这一部分得益于densenet。 有趣的是，作者提出的DSOD本来是使用train from scratch的方法训练的，但是作者也尝试过使用DSOD先在ImagNet上预训练，然后在VOC上fine-tuning的方式： 作者使用 backbone network DS/64-12-16-1 （这个架构比DenseNet论文中使用的效果好于ResNet的结构在配置上差很多）在 ImageNet上预训练, 获得了 66.8% 的top-1 精度和87.8%的 top-5 精度，其结果稍差于VGG。将其用于在 “07+12” trainval set上预训练, 获得了 70.3% 的mAP（ VOC 2007 test-set）。 同样结构的DSOD直接 train from scratch，获得的mAP是70.7%，可以看到两种策略训练出来的结果很接近，train from scratch还略微好一点，这也说明了 densenet 的结构是比较适用于 train from scratch 的。 还有一个有趣的现象，从table 4中可以看到原版的SSD也就是使用VGG的SSD 从fine tuning 到 train from scratch 其精度是降低的。所以对于VGG结构的网络来说 train from scratch 并没有太大的好处。 参数量与速度方面： DSOD的参数量：1/2 parameters to SSD300 with VGGNet, 1/4 to SSD300 with ResNet-101, 1/4 to R-FCN with ResNet-101 and 1/10 to Faster R-CNN with VGGNet. 具体的速度看table4. 另外还有一个轻量级的版本，表中没列出： A lite-version of DSOD (10.4M parameters, w/o any speed optimization) can run 25.8 fps with only 1% mAP drops 3.2 PASCAL VOC 和 MSCOCOPASCAL VOC 和 MSCOCO 的试验结果不详细赘述。 4 总结这两年来深度网络特别是以 ResNet 为代表的深层神经网络发展很迅速，其促进了计算机视觉领域（还有其他领域比如NLP）的快速发展，很多视觉问题取得了高速的发展，基于这个趋势，人们会认为更深的网络，更多的数据集应该可以取得更好的结果，比如现在谷歌有个 Open Images Dataset ，包含大约900万张图片，这基本上是 ImageNet的 7.5倍大。当然这么多的数据集也许是好的，但是如果能通过合设计网络结构使网络在较小的数据集上能取得相当的结果，那么何乐而不为呢？ DSOD的结果就是很好的例子：train from scratch 仅仅使用了 16551 张图片就可以得到相当或者更好的精度，而fine tuning使用了 1.2 million + 16551 张图片。 DSOD的这个思想我个人觉得是很有意义的，因为尽管对于学术研究来说，随着硬件（GPU，TPU等）的发展和大公司公开新的更大的数据集，研究更深的网络以求获得更好的精度是可行的，但是对于深度学习的产品落地来说，大的网络结构是不现实的，毕竟你无法将100多层的以ResNet为基础的网络或者其他更深的网络跑在一个终端设备或者移动设备上，然而从产品落地的层面来说，终端设备或者移动设备的占比要更大，而大型的网络则只能做成云端或者服务器端的产品。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础DL模型-DenseNet-Densely Connected Convolutional Networks-论文笔记]]></title>
    <url>%2Farticles%2F216a4e98%2F</url>
    <content type="text"><![CDATA[论文：Densely Connected Convolutional Networks CVPR 2017 (Best Paper Award)代码：https://github.com/liuzhuang13/DenseNet 竟然是用lua写的 0 简介这篇文章提出了一种不同于ResNet的深层卷积神经网络，称为DenseNet。 为了在大数据集上获得比较好的效果，主要的做法有两种 设计更宽的网络：代表：GoogLeNet，FractalNets 设计更深的网络：代表：HighwayNet，ResNet 更宽的网络此处不说了，对于更深的网络，因为在训练时存在的梯度消失现象很严重，所以使用直接堆积的方式是不行的。深层卷积神经网络 ResNet 为了解决网络在训练时的梯度消失问题，提出了一种残差模块： 正是这种残差模块中的 shortcut connect 使得深层网络的训练成为可能，解决了深层网络训练过程中的梯度消失问题，ResNet针对于ImageNet的网络最深可以做到152层，而在CIFAR上甚至做到了上千层。 后面很多关于新型模型设计的文章基本都保留或者借鉴了这种类似的 shortcut connect 或者 identity mapping。 而DesNet从另一个角度出发设计出了更深的网络，效果比ResNet好，计算量也更少，这个设计的思想就是 feature reuse 。 原文是这样说的： Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse, yielding condensed models that are easy to train and highly parameterefficient. 一句话概括： 这篇文章提出 feature reuse 的思想，即在前向过程中将每一层 $L_i$ 都添加到 $L_i$ 之后的所有层的输入中，形成一种密集的跳层连接，由这种方式构成的网络称为 DenseNet，是一种不同于 ResNet 而又稍优于 ResNet 的新型的基础深度卷积神经网络。 1 DenseNet设计思想DenseNet的设计思想是 feature reuse ，也就是特征再利用，怎么个再利用法呢，看下图，一目了然： 这个图很直观明了的展示了DenseNet feature reuse的设计思想。这是一个dense block，是DenseNet中的一个子模块，每一层的输入都包含了前面所有的层（通过concatenate的方式），对于一个L层的dense block，其连接的数量有 $L(L+1) \over 2$ ，正是因为它的密集连接，所以这个网络才会叫做DenseNet。公式2中的 $H_l$ 代表的是一系列操作：BN+ReLU+3 × 3Conv。 很明显由于跳层连接的存在，要求一个dense block中的feature map的尺寸应该相等，因此DenseNet同样也是由不同的block构成的，这些 dense skip connection 也只存在于block内部。网络的整体结构如下： ResNet的结构 ResNet的结构是下图这样的： 假设 前面一层为 $x_{l-1}$ 相邻的后面一层为 $x_l$ ，$H_l$ 代表 BN+ReLU+Conv等一系列操作，那么残差模块： 这个式子中的identity mapping可以使梯度直接传递到前一层，而不用经过block中的两个中间层，相当于减少了网络的层数。这对于避免梯度消失是很有效的。 不过作者认为residual block中的 element wise相加的操作会阻碍信息流，所以作者在后面使用的是 concatenation 操作。 Dense connectivity. DenseNet 网络中的密集连接可以保证后面层的输入包含前面的所有层 与公式1相比，这里的梯度可以直接从 Transition layers 或 损失函数传递到dense block中的任一层。$[x_0,x1,…x_{l-1}]$ 是一个concatenation 操作。 Transition layers block之间的层称为 transition layers ：包含 a batch normalization layer and an 1×1 convolutional layer followed by a 2×2 average pooling layer. Growth rate 假设每一个 $H_l$ 都产生 $k$个feature map，也就是同一个block之间的卷积层，尽管其输入是依次增加的，但是输出是固定不变的，也就是 $k$ 个feature map。假设block的输入为 $k_0$ ，那么第 $l$ 层的输入 feature map 数量为 $k_0 + k × (l - 1) $。这个 $k$ 就称为 growth rate，很形象，就是代表后面每一层的输入会比前面相邻一层增加 $k$ 个feature map。这是一个可以调节的超参数。而且这个 k 可以使用一个比较小的数值，比如 $k=12$，不过文章中使用的是32。 这个 $k$ 实际上就是卷积的输出通道数channel，但是跟channel相比又多了一层意义，就是增长率growth rate。一般的CNN比如 ResNet 其输出通道数都是很多的 比如：64，128，256，512等数值，这是为了增加特征的多样性，那这里的通道数为什么可以做到这么少？（通道数减少可以减少计算量，不过这里输入的计算量 貌似 是增加的（因为输入channel增加了），那么总的计算量怎么变化？关于计算量的问题，看实验部分） 我的直观理解是：因为每一层输入的 channel 都在递增，而且输入包含了前面所有层的输出，也就是输入的多样性在递增，所以从这个角度来看，其多样性只是换了个地方而已，因为输入的多样性必然会导致输出的多样性，因此即便每一层输出都只有32个通道，但是每一层的输入其多样性在逐步增加，而且输入是由到这一层为止网络前面的所有信息（global state）构成的，所以输出的 32 个feature map虽然比Resnet少，但是包含的信息并不少，可以说浓缩的都是精华。除此之外，相比于ResNet很大的输出通道来说，这里 $k=32$ 反而更精炼，而且这点对于减少计算量也有一定的帮助。 关于较小的k值为什么能work的原因，作者是这么说的： One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network’s “collective knowledge”. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer. 每一层的输出（比如第k层）都是网络的一种 “collective knowledge” ，也就是知识收集的过程（这是显然的），这些feature map包含了前面所有层的信息，所以它是一种全局的状态信息 global state （到第k层为止，网络前面部分的全局信息），每一层（第k层）又将自己的 k 个输出添加到下一层的输入，相当于对全局状态信息做了贡献，k 代表有多少新的信息添加到了全局状态信息中。 最后这句话 The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer. 这个地方直接理解有点问题：全局信息一旦被某一层改写，那么网络中的所有层都可以访问这个改变并受益。而且不需要像传统的网络架构一样一层一层的往下传输。 因为全局状态信息是不停变化的，每一层都有新的贡献，那么第k层网络改变了全局状态信息后，前面的层（k-1层以及k-1前面的层）还能访问？前面的层肯定不能访问，所以这里应该指的是，第k层后面的所有层（比如k+3层）都可以访问，而且是直接访问（k+3层可以直接访问第k层，由于密集跳层连接的存在），不用像传统网络那样必须一层一层传播到那一层才能访问到（必须经过 $k \to k+1 \to k+2 \to k+3$的过程才可以）。 所以 $k$ 值比较小是可以的，原因就在于 $k$ 的另一个身份，就是增长率 growth rate ，这里网络的多样性是从输入的角度来看的，每一层的输入都以 每次 $k$ 个feature map的速度增加，而且生成新的全局信息，后面的层都可以直接访问。 这个地方，k的作用感觉真的很奇妙，不过他奇妙的作用是由网络的结构引入的。 Bottleneck layers 前面提到 k 是一个比较小的数，比如文章使用的32，而且较小的k可以减小计算量，但是由于输入实在是太多了（越往后越多，比如当一个block包含12层时，最后一层的输入channel 可以 达到 12×32=384），所以对于计算量来说相当于拆东墙补西墙，并不会有明显减少。因此需要借助 Bottleneck layers 来减少一部分计算量。也就是在 3×3 卷积之前 添加 1×1 卷积 降低 channel 数量，1×1 卷积的输出channel为 $4k$ 个，如果不降channel的话3×3 卷积的输入就是 $k_0 + k × (l - 1) $ 个。这样的话 $H_l$ 函数就变成了：BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3) ，这个版本的DenseNet称为：DenseNet-B。 Compression 为了进一步减少计算量，提升模型的紧凑性，作者还提出了一种措施，就是减少transition layers 的channel，假设 dense block的输出 channel 是 m，那么可以让 transition layers 的channel 为 $\lfloor \theta m\rfloor$ $0 &lt;θ ≤1$ ，称之为 Compression ，$\theta &lt;1$ 时（文章使用0.5） 使用这种版本的 DenseNet称为：DenseNet-C， 连同前面的 Bottleneck layers 一块使用时 称为 DenseNet-BC 。 文章针对好几个数据集做了实验，所用的结构也不太一样。这里只给出 DenseNet 在ImageNet 上的模型结构，其他看论文： 2 与ResNet的对比 ResNet： Residual block DenseNet：Dense block ResNet 反向传播时梯度可以直接通过 identity mapping 传递到前一层，而不经过中间 Residual block 的那两层，从而避免梯度消失 DenseNet 可以通过 skip-connection 将梯度传递到前面的任一层，信息流更大，范围更广泛，同样可以避免梯度消失 Dense block 跟 Residual block比较像，但Dense block是密集连接的，其中的一个 skip-connection 相当于 Residual block中的 identity mapping ，差异在于 Dense block 中连接前面的层传递过来的信息时是通过 concatenation 操作，而 Residual block 则是 element-sise summation。 3 实验结果小数据集上的结果：CIFAR-10 CIFAR-100，SVHN 加了 Bottleneck layers 和 Compression 之后，参数量明显减少很多，所以DenseNet之所以参数少，那还是 Bottleneck layers 的和 Compression 的功劳，但不管怎么说，在参数减少的同时，还能降低错误率，那就是很有效。 CIFAR-10上的各种对比： 相同的精度下，DenseNet的参数量只有ResNet的1/3，这里的ResNet是指 (pre-activation) ResNets （来自何凯明大神的论文：Identity mappings in deep residual networks） DenseNet参数量更少，从测试误差方面看，二者不相上下。 大数据集上的结果：ImageNet 下面 table 3 展示了不同的 DenseNet-BC 配置的结果，figure 3 展示了 DenseNet-BC 与ResNet在错误率与参数量和计算量之间的对比 相同参数情况下DenseNet的错误率更低，相同错误率情况下DenseNet参数更少 相同计算量情况下DenseNet的错误率更低，相同错误率情况下DenseNet的计算量更少 DenseNet-BC 应该是全面压制 ResNet 实验部分，此处说的比较简单，详细请看论文 4 总结Model compactness. 模型紧凑性结果，还是看前面提到过的 Figure 4 Implicit Deep Supervision 隐式的深层监督信号主要是通过跳层连接实现的，每一层都可以直接从loss function 获得额外的监督信号，这也是为什么DenseNet效果好的一种解释。loss function可以被网络中的所有层共享。 关于 deep supervision 的好处已经在 deeply-supervised nets (DSN) 这篇论文中解释。 Stochastic vs. deterministic connection. DenseNet的设计思想与 Deep networks with stochastic depth 这篇论文中提出的结构有一定的联系。stochastic depth 会随机的丢弃 residual block中的网络，而只保留 identity mapping。这篇论文这里不再叙述，有兴趣的可以看看这篇论文。 Feature Reuse. 关于特征的再利用，作者做了一个可视化的试验： 坐标 $(l,s)​$ 处的颜色代表第 $l​$ 层与前面的第 $s​$ 层之间的权重的 $L1​$范数的均值。可知： 后面的层确实从前面的层中提取了信息（蓝色区域代表权值基本上在0附近可以认为两层之间没有连接关系） 这个图详细的解释看原文吧，感觉除了上面那一点之外，其他的理解的也不是很清楚。 这篇文章从另一角度提出一种密集的跳层连接，实现了特征再利用，解决了梯度消失的问题，并且通过 1×1 卷积和 压缩卷积channel来减少计算量。与ResNet相比，在同样的参数和计算量情况下，准确率更高；在同样的准确率情况下，参数量和计算量更少。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>basic dl models</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-SqueezeDet-论文笔记]]></title>
    <url>%2Farticles%2F7f3052ae%2F</url>
    <content type="text"><![CDATA[论文：SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural …代码：https://github.com/BichenWuUCB/squeezeDet 1 简要介绍这篇文章提出的SqueezeDet 网络主要是适用于自动驾驶领域的。 SqueezeDet : 基于SqueezeNet 的Detection模型，故称SqueezeDet 。 对于自动驾驶领域来说，几个关键因素如下： 精度。高精度，和尽可能高的 recall，最好使100%的recall； 速度。应该具备实时检测的速度（30FPS）； 较小的模型。小的模型好处有：训练时更加高效，车载客户端进行模型更新时需要较少的无线网络带宽和通信消耗，消耗更少的能量，更适用于嵌入式设备部署。 能量效率。一些桌面和框架级别的神经网络往往都需要很大的能量耗散（250W），但是对于自动驾驶领域来说，嵌入式处理器才是应该考虑的主要对象。毫无疑问，如何让神经网络模型适应小的能量消耗是个关键问题。 目前很多公司都在研发适用于自动驾驶的处理器，英伟达 的 Xavier 号称实现每秒 30 万亿次计算(TOPS)，整个系统功耗（包含CPU,GPU在内）为 30W；英特尔的 EyeQ5 ，号称实现15万亿次计算功耗为 5-6W。 SqueezeDet 主要就是针对以上几个方面提出了自己的架构。 SqueezeDet 主要参考了 SqueezeNet，YOLO和SSD，还有FCN全卷积网络。 最终的效果是，在KITTI 数据集上，输入分辨率为1242×375的情况下，其速度达到了57.2FPS，模型体积仅为不到8MB；在TITAN XGPU上 检测一张图片的能耗为 $1.4J$，这是Faster R-CNN 的1/84. 而精度也达到了目前 state-of-the-art水平。在某些方面甚至超过了其他的模型。 2 结构设计先回顾一下跟这篇文章有关的，其他的网络结构。 2.1 YOLO与Faster R-CNN相比，YOLO是属于一步到位的模型，不需要预先生成候选框，然后再去提取特征进行分类和回归。 YOLO的模型结构，如下： 但实际上YOLO也是有生成候选框的步骤的，只是 这个步骤跟检测合并到一起了。那这个生成候选区域的步骤在哪呢。 先看下面这两张图 YOLO的方法是这样的，将原图分成一个 7×7的网格： 对于每一个网格都预测两个 形状不同的bounding box（box的尺寸可以大于网格的大小），那么图片上目标的bounding box 肯定跟某几个不同的网格中的bounding box 距离最近。 那也就是说每一个网格所关联的两个 box （类似Faster R-CNN中的anchor）里面的区域有可能是目标，也有可能不是目标，或者只包含目标的一部分，或者一部分是目标，一部分不是目标。 那么这样一来每个 bounding box 都有一个概率，表示他是目标的概率。这个概率 是$$P_r(Object)*IOU_{truth}^{pred}$$其中 $P_r(Object)$ 表示这个box中是否存在目标，存在就为1，不存在就为0. 比 如上面两张图中，不同的两个格点（红色）对应着不同的两组 boxs，每个格点对应两个box，由于这四个box都与目标的区域有重叠（IOU不为0），所以这个四个box的 $P_r(Object)$ 都为1. 其实这个 $P_r(Object)$ 就是用 IOU （与ground truth之间的IOU）衡量的，首先计算IOU，只要IOU 不为0， $P_r(Object)$ 就是1. 一个box中有目标的概率其实就是IOU的值。 除此之外，每个box还都有位置信息，用坐标来表示，中心点坐标(x, y),以及box宽和高，w，h。 到此为止，每个格点关联的每个box有5个属性，4个位置信息，1个confidence信息。那一个格点关于box 的定位信息有10个。 但是检测不仅要检测出目标，还要识别出目标是什么物体，因此还需要额外的信息。假设要识别的类别总数为C，YOLO中，对于每个格点（不是关联的两个bounding box）都做一个物体类别预测。 那么一个格点还会产生C个物体类别预测信息。对于PASCAL 数据集来说 C=20。那此处 那也就是说一个格点经过网络之后，要得到 共30个信息，这里面包含定位（10）和识别信息（20维）。 所以我们观察一下YOLO的最后全连接层那一部分，可以看到YOLO用了两个全连接层，一层是4096×1的，还有一个是 1470×1的。前面的4096可以认为是根据经验选取的（alexnet，VGG第一个全连接层都是4096维的），但是后面的1470×1就不是经验值了，其实是 1470=7×7×30。 那这个是什么意思已经很明白了。此处的 7×7 就是要把图片分成多少 格点，后面的30就是每个格点要预测的信息维数。结合下图更明了。 那其实到这里就可以看出来了，YOLO解决的方式其实属于是回归问题。前面24个卷积层提取特征，后面两个全连接层，用于回归。回归的是什么？回归拟合的是每个格点两个box的坐标值（5×2），和每个格点的分类向量（20）。 2.2 Faster R-CNNFaster R-CNN的结构见下图，很显然是两步检测的，先生成一系列可能是目标的区域（stage 1），然后对这些区域进行检测（stage 2）。stage 2的箭头起点不太对，应该是从 proposals开始，前面的卷积部分是共享的。 实际上，Faster R-CNN的stage 1就已经产生目标检测的区域了，可以认为这已经是一个 弱的检测器了（这个称为RPN），这个stage 1已将包含了定位的信息，只是它只确认区域中是否包含目标，而不关心包含的是什么类别的目标。stage 2是一个强检测器，除了进一步精修定位信息，最主要的是把目标类别识别来。 假设我们在stage 1中把候选区域中的目标类别也预测出来，那其实stage 2可以不用了。 YOLO 其实就相当于 Faster R-CNN的stage 1步骤，当然细节上会有不同。比如 YOLO后面使用的是全连接层，而Faster R-CNN stage 1步骤（RPN）后面使用的是全卷积层。 YOLO每个格点的是预测目标类别的（多分类），RPN只预测是否是目标（二分类）。 再看看RPN的主要结构，如下图： 主要就是 anchor这个机制，假设feature map上每个像素关联k个anchor。 假设前面用的ZFNet，那么最后一个卷积层输出的feature map是13×13×256的。 先看其中一个window，如果后面使用256个3×3×256的卷积核，那么 256-d那个地方输出的就是 1×1×256维的，那么后面的cls层（二分类）就是2k个1×1×256的卷积，输出为1×1×2k；reg层（定位）就是4k个1×1×256的卷积，输出为1×1×4k。 与YOLO不同的是，每一个anchor（对应YOLO中每个格点内的两个box）都输出类别概率。 图上那个sliding window的做法，在实际操作的时候，其实就是用了卷积的方式，步长为1，pad为1. 所以实际上 256-d那个地方其实是 13×13×256。对应的cls层和reg层的输出分别是13×13×2k，13×13×4k，如下图。13×13维度上每一个 1×1×2k或4k的长条，都代表图片上不同位置的anchor box的类别信息或者位置信息。 2.3 SSD先看下SSD的宏观结构，如下图，FC6和FC7以及之前的部分是VGG的内容，后面是SSD里面加的。SSD添加的层 这是一个多尺度预测（Multi-scale）的网络，如下图从FPN的论文里找来的图。就是从不同的feature map上预测，然后将各个预测结果融合到一起。 然后将SSD与YOLO相比较一下，如下图。可以看出YOLO是单一尺度的。二者最大的区别也在此。当然在框的生成方面SSD也是一步到位的，一边生成框，一边检测。 更详细的看一下SSD的检测流程如下： 总共6个尺度，每一个尺度上都进行框的生成和预测。最后使用NMS（非最大值抑制）进行过滤，得到最后的结果。 与YOLO的不同主要在于： 多尺度 主干部分，全卷积 预测部分也是卷积（YOLO的预测部分是全连接层） 接下来看看SSD是如何检测的。如上图红色框，这一部分其实是使用 3×3的卷积实现的。具体细节看下图： 再看看是怎么生成default box的，如下图。最右边那个就是对应的feature map的尺寸，然后将feature map上的每一个像素点都映射到原图上（最左边），然后映射到原图上的每一个像素都关联三个不同的default box。 有没有发现这跟 YOLO中的 网格划分很像，只不过YOLO是划分成网格，这个网格划分对应于最后一个全连接层做了reshape 的feature map，每一个网格也只包含一个像素，每一个像素再映射到原图上。 但其实更像的是Faster R-CNN中的 anchor box。 与YOLO不同的是，每个 default box（对应YOLO中每个格点内的两个box），都输出4个位置信息和 (C+1) 个类别信息（包含1个背景）。 与Faster R-CNN 不同的是，每个 default box（对应Faster R-CNN 中的两个anchor box），都输出(C+1) 个类别信息，而不是一个二分类标记box是不是目标。 看下图这个就是分类和检测的实现方法，跟Faster R-CNN中的RPN如出一辙。 2.4 SqueezeDet2.4.1 结构分析网络结构如下： 前面是特征提取网络，这里用的SqueezeNet，中间是本文提出的ConvDet层，后面是Filtering层。Filtering层就是NMS（非最大值抑制）。 ConvDet层意即，使用卷积层来检测（Detection），因为经典的用法是使用全连接层来进行检测（比如YOLO），不过Faster R-CNN中的RPN相当于一个全卷积的弱检测器。这个方面来说SqueezeDet,跟RPN也挺像。 但是SqueezeDet更像SSD. 就相当于SSD的一个尺度分支，只不过前面卷积层提换成了squeezenet。 再看一下 SqueezeDet 关于anchor box的图，是不是跟SSD和RPN很像，简直如出一辙。 2.4.2 计算量分析以下图中 蓝色的为某一层的输出特征，而黄色的是神经元的参数分布（权值和偏置） 以下3D卷积写法均略去前一层的输入通道数 首先看下RPN的参数量： 输入是 $W_f× H_f×Ch_f$ 的feature map， 中间黄色的部分是 $1×1$ 的卷积核，如果把分类和回归放在一起，那么卷积核的个数就是 $K×(4+1)$ 个，4是四个坐标，1是一个概率，K是K个anchor。 把分类和回归的输出也放在一起，那么输出feature map就是$W_f× H_f×(K×(4+1))$ 大小的。 参数量为 ： $1×1×Ch_f ×K×5$ 再看下 YOLO中的参数量： 这个检测模块称之为 FcDet 同样输入是 $W_f× H_f×Ch_f$ 的feature map， 假设将图片分成 $W_o ×H_o$ 个网格（比如7×7）；每个网格关联K个box（比如K=2）；每个box输出（4+1）个信息（4个位置信息，1个类别信息）；每个格点输出C个类别概率（比如20）。 首先看第一个卷积层，FC1标号的是神经元的参数分布，可以看到从卷积层到全连接层的参数分布其实是一个 $W_f× H_f$ 的卷积核，这个卷积核与一整张feature map卷积，只输出一个激活值；这样的卷积核共有 $F_{fc1}$ 个，因此FC1输出为 $1×F_{fc1}$ 。 这一层参数量为：$W_f× H_f×Ch_f×F_{fc1}$ 接着第二个全连接层，FC2标号的也是神经元的参数分布，这里用的是 $1×1$ 的卷积层实现的，由于上述假设，FC2的输出应该是 $1×\{W_o ×H_o×(K(4+1)+C)\}$ ，是一个二维的tensor；但是为了与划分的网格匹配，需要将这个输出reshape成$W_o ×H_o×(K(4+1)+C)$ 这样的三维tensor。 这一层参数量为： $F_{fc1}×W_o ×H_o×(5K+C)$ 。 FcDet整个结构的参数量为：$W_f× H_f×Ch_f×F_{fc1} + F_{fc1}×W_o ×H_o×(5K+C)$ 看下这篇文章提出的ConvDet结构的计算量： 这个结构说白了就是SSD的一部分，再来对比一下，看看下图SSD中相对应的部分，这个图中把分类和回归定位分开画了，不过SSD的代码中实际上也是分开做的，其实这部分可以合并到一块的，因为卷积都是用的3×3，输出也都是 5×5，所以合并到一块也是可以的，只是不同的通道对应着不同的分类或者定位结果。 基本上二者相同，只不过这里起了个名字，叫做ConvDet。 同样输入是 $W_f× H_f×Ch_f$ 的feature map， 只不过这里假设 卷积是 $F_w×F_h$ 的（SSD对应结构中用的是3×3），通道数为 $K×(4+1+C)$ , K为 anchor（或 default box 或 格点内关联的box）的个数，4为每个anchor的位置信息，1为是目标的概率，C为C个分类。这样输出为 $W_f× H_f×\{K×(4+1+C)\}$ 。 ConvDet的参数量为： $F_w×F_h×Ch_f×\{K×(5+C)\}$ 与YOLO 相比，没有使用全连接层，而是使用了全卷积的（FCN）方式。其实就是SSD中的一部分。 三者参数量对比： 举个例子计算下： 假设输入feature map 为 $7×7×1024， F_{fc1} = 4096, K = 2, C = 20, W_o = H_o = 7,$ ConvDet 使用3x3 的卷积核 YOLO: $4096×（7×7×1024+7×7×（5×2+20））=212×10^6$ ConvNet: $3×3×1024×2×（5+20） =0.46 ×10^6$ 2.6 训练 先看一下，网络预测的是什么 如下图： 绿色框为ground truth 红色为anchor 蓝色为预测的box 首先，Ground Truth 与 Anchor 有个偏移值，如下：$$Offset^{GT}=\{\delta ^G(x_{ijk}),\delta ^G(y_{ijk})，\delta ^G(w_{ijk})，\delta ^G(h_{ijk})\}$$其次，Prediction 与 Anchor 有个偏移值, $Offset^{P}$，如下：$$Offset^{P}=\{\delta (x_{ijk}),\delta (y_{ijk})，\delta (w_{ijk})，\delta (h_{ijk})\}$$对于定位来说：可以认为，这个网络预测输出的是 $Offset^{P}$ ，标签是 $Offset{^GT}$ ，转化一下，实际上网络输出的是 $ \delta (x_{ijk}), \delta (y_{ijk}), \delta (w_{ijk}), \delta (h_{ijk})$ ，因为这四个参数决定了 $Offset^{P}$ 。 分类就是输出类别，标签为真实类别。 以上偏移是由网络学习得到的。 偏移与最终的输出之间还有一个映射关系，这个关系是人为指定的。这个关系最先在R-CNN系列的目标检测模型中提出。下述。 最终的预测框的中心坐标 $(x_i^p,y_i^p)$ 和宽高 $(w_k^p,h_k^p)$ 的计算方式如下： $\hat x_i,\hat y_i$ 是与box关联的feature map中的像素（或者说类似于YOLO中的格点）的坐标，如下图中的 $C_x，C_y$ 。 $\hat w_k,\hat h_k$ 是 每个像素（或格点）关联的第k个anchor的 宽和高，如下图中的 $b_w,b_h$ 偏移与真实位置之间的映射有两种，线性映射（中心点坐标）和指数映射（宽，高）。 这个地方可以结合 YOLO9000论文中的一张图理解。不过对于预测和anchor的关系，二者刚好反过来。 再来看一下损失函数，如下： 损失函数与YOLO的损失函数差不多，放一张YOLO的对比一下： 解释一下SqueezeDet的损失函数: 主要分为三部分： 第一部分是回归损失，或者说定位损失，就是下图这个： $I_{ijk}$ 表示：位于 输出feature map上 $(i,j)$ 坐标处的第 $k$ 个anchor 是否包含有目标。也就是 这个$P_r(Object)*IOU_{truth}^{pred}$ 是否为0. $\delta x_{ijk},\delta^G x_{ijk}$ 分别表示anchor与预测box和ground truth 的 x 坐标的偏移。后面其他的参数，y，w，h类似。 这是一个均方误差。没有采用YOLO那样的将 宽度w和高度h开根号，再做均方误差。YOLO这么做是考虑了对于大小目标计算损失时的公平问题。 第二部分是二分类损失，这个二分类决定的是这个anchor是否是目标，如下： $\gamma _{ijk}$ ：$P_r(Object)*IOU_{truth}^{pred}$ 2.5 试验结果]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
        <tag>lightweight models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-FSSD-Feature Fusion Single Shot Multibox Detector-论文笔记]]></title>
    <url>%2Farticles%2F1b1c7744%2F</url>
    <content type="text"><![CDATA[论文：FSSD: Feature Fusion Single Shot Multibox Detector代码：https://github.com/lzx1413/CAFFE_SSD/tree/fssd 这篇文章在FPN的基础上进行了一定的思考，提出了一种新的特征融合方式，并将这种特征融合方式与SSD相结合，在损失一定速度的情况下，提高了检测精度。 这篇文章立足于特征融合。 1 简要介绍首先是模型的拓扑结构： (a)：featurized image pyramids ，使用 image pyramid 构建 feature pyramid，金字塔形状的feature是从不同scale的图片上计算出的。典型的使用这种结构的模型：OverFeat，R-CNN。 (b)：对图片来说只有一个scale，但是为了达到 预测不同scale 的bounding box的目的，Faster R-CNN 提出了 anchor 的机制，解决了这一问题。典型的模型有：Faster R-CNN，R-FCN等。 (d)：先说下d吧，这是SSD中提出的结构，充分利用了CNN自身的 feature pyramid 结构。结合了不同的层，分别预测，预测的时候是独立的，最后生成结果时才通过NMS融合。严格来说不能算是特征融合，只能说是multi-scale 预测。 (c)：这是FPN和DSSD提出的结构，这个才是真正的特征融合，在预测之前先通过一系列上采样额外再生成一系列feature pyramid，并通过lateral/skip connections 与CNN主结构上的feature pyramid 特征进行融合，之后再去预测。 (e)：这是本文的拓扑结构。与FPN结构相似，但实际上跟FPN 有很大的不同。FPN是先上采样，再多个层融合；FSSD是先融合多个层的feature map，后面再跟一个CNN模块，这个CNN模块跟SSD是一样的，采用multi-scale预测。 实际上除了上面的一些拓扑结构之外，还有一种拓扑结构，就是下图的 (b)。 (a)：与前面那个图的(a)是一样的，image pyramid。 (b)：这种结构是前面没有提到的，multi-scale filter ，这个跟SSD比较像，SSD是multi-scale prediction，每一个prediction 都对应一种size的filter。 (c)：Faster R-CNN的anchor机制，从(b)中发展出来的，把multi-scale filter 换成了 multi-scale anchor。 2 模型设计2.1 Feature Fusion Module作者认为 FPN的融合是在多个feature map进行的，这种侧向连接和 element-wise add 是很费时间的： 所以作者提出了一种 lightweight and efficient feature fusion module 特征融合只在一个位置发生 后续的结构跟SSD是一样的 作者提出一种数学模型来表示这种结构： $X_i, i ∈ C$ ：用于融合的 source feature map，SSD中选择的是：conv4 3, fc 7 of the VGG16 and new added layer conv6_2, conv7_2, conv8_2, conv9_2 。这篇文章选用的是conv3 3, conv4 3, fc 7 and conv7_2 $T_i$ ：在融合之前 每一个 source feature map的变换函数，类似于激活函数。在FPN中使用的是 1×1 的卷积来降低输入通道。这篇文章使用的 1×1 卷积 + 线性插值的， 1×1 卷积 同样是为了降低输入通道，线性插值是因为这里的融合是不同level的层或者前后层之间的融合，他们的feature map大小是不一样的，因此需要线性插值将所有的source feature map 转换成统一的大小。FPN中是侧向连接，是同一level的层之间的融合，因此不存在这个问题。 $\phi_f$ ：特征融合函数，FPN中使用的 element-wise summation ，而这个操作不仅要求feature map的大小一样，还要求 feature map的通道数也要统一，这个在FPN中在使用 1×1 卷积时就保证了融合时的通道统一。由于操作麻烦，在FSSD中，1×1 卷积只负责降低通道，而不保证所有feature map的通道统一，融合时选用了concatenation 的操作，简单有效，不需要feature map的通道统一。 $\phi_p$ ：生成 后续 pyramid features的函数，与SSD一样，只不过这个后续结构，作者在这个部分尝试了多种结构的堆叠，见下图。最终通过实验选择了一种简单的方式，这与SSD中的结构是一样的。 $\phi_{c,l}$ ：用于检测和分类的函数，与SSD一样。 2.2 Training两种训练策略： 第一种：由于这是在SSD的基础上改进的，所以可以使用预训练的SSD，然后fintuning 第二种：使用预训练的VGG，然后fintuning SSD300：SSD的训练曲线（VGG+fintuning） FSSD300：前面提到的第二种，VGG+fintuning FSSD300+：前面提到的第三种，SSD+fintuning 两种训练方法训练出的模型的精度相差无几，但是第一种训练方法明显收敛更快。 3 实验一些对照试验，决定了前面提到的很多参数的选取，比如用于融合的source feature map，融合的方法，训练方法，batch normalization等。 PASCAL VOC 注意 在对比速度时，测试的GPU是不一样的，1080ti的性能比TITAN X要好很多。 网络的 backbone network也是不一样的。 同样 注意 在对比速度时，测试的GPU是不一样的 这张图对比 FSSD，与SSD的速度是有意义的，因为其他模型都是在TITAN X上测试的。 COCO]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-HyperNet-论文笔记]]></title>
    <url>%2Farticles%2F33c81bef%2F</url>
    <content type="text"><![CDATA[论文： HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection 1 简介这篇文章主要解决了两个问题： 解决 R-CNN 系列 目标检测模型，需要生成几百上千的候选框（Faster R-CNN 300；Fast R-CNN 2000）的问题。HperNet 只需生成 100~200个候选框。而精度比 Faster R-CNN 还要好一点。 将deep ConvNet的高低层特征进行融合，进而利用融合后的特征进行region proposal提取和进一步目标检测。提升了对小目标的检测效果。 主要的贡献： object proposal task 上：使用50个proposals 可以获得95%的recall；使用100个proposals可以获得97%的recall。 注意：这个recall只是用于region proposals的，并不是针对 PASCAL 或 COCO 数据集的最终检测效果的recall。就是说事先生成的 几百个region 与 ground truth 的区域IOU在某一阈值之上的region 占总 的ground truth的比例，这个比例自然是比最终detection的recall要高很多的。因为这个阶段相当于一个弱检测器，约束条件也不高，生成的框里面自然会包含很多 ground truth 的区域。 detection challenges 上：在PASCAL VOC 2007 和 2012 数据集上取得的 平均检测精度（mAP），分别为76.3%和71.4%。比Fast R-CNN分别高了6个和3个百分点。 HperNet 的快速版，可以达到 5FPS的速度。 宏观架构： 整个网络结构也属于两步检测，即先生成 region proposals ，再进行检测。 2 HyperNet 网络结构 图中标注的 写法 比如 5×5×42 指的是，这是一个5×5的卷积核，卷积核的个数为42个，其他的 3×3×4，3×3×63也是一样的写法，写的都是卷积核的参数。 整个网络结构跟 Faster R-CNN 还是比较像的。 主要分为三部分： 第一部分：卷积层提取特征，然后进行特征融合； 第二部分：弱检测器，生成候选区域，卷积层+全连接层, Conv+FC； 第三部分：强检测器，生成最终的检测结构，卷积层 + 全连接层，Conv+FC。 下面分别介绍： 2.1 Hyper Feature Production特征融合，如下图： 将图片resize，使得最短边 为600（比如：resize后的大小为 1000×600）； 首先使用基础卷积神经网络（Alexnet，VGG）提取特征； 在最底层卷积层（conv1）后面加上 max pooling 层，实现降采样； 在最高层卷积层（conv5）后面加上 反卷积 deconv 层，实现上采样； 中间层（conv3）不做处理； 在上一步操作之后每一个 level 后面都再加 一系列卷积层（绿框中的黄色矩形）： 进一步提取语义特征、将不同分辨率的feature map 压缩到相同的分辨率。 卷积后每个 feature map 加局部响应归一化 LRN，之后输出 Hyper Feture maps。这里的Hyper Feture maps的大小不是13×13×126的，图上写的那个 13×13 是经过ROI 的大小（红色框框）。这点作者没说清楚，代码也没有，搞不明白Hyper Feture maps到底是多大。 假设使用VGG做特征提取，输入图片为1000×600。那么Conv1、Conv2、Conv3、Conv4、Conv5层的feature map 输出（不含池化层）分别为 1000×600，500×300，250×150，125×75，62.5×37.5。 最后的feature map会是62×37, 这样会造成信息损失。 因此把最后的Conv5层的feature map通过反卷积上采样到250×150，然后再经过一系列卷积； 把第一层Conv1的feature map做一个max pooling 降采样，然后再经过一系列卷积； 中间层，不做变化，直接经过一系列卷积； 然后把1，3，5层的feature map 进行 LRN之后连在一起。LRN是必要的，因为不同层的feature resolution 不同，如果不做正则，norm大的feature会压制住norm小的feature。 2.2 Region Proposal Generation 如上图，对于Hyper Feture maps 使用 ROI pooling 。ROI pooling 的大小是 13×13的，因此最后输出的特征是 ROI feature 是 13×13×126的。中间那个红色的框框是ROI，但是怎么来的，作者没说清楚。 ROI pooling之后使用了 3×3的4个卷积核。输出的是 13×13×4 的feature map，图上那个空白的矩形就是。之后再经过一个256维的 全连接层；再之后并行经过两个 两个全连接层，分别是分类和回归（用于定位）。 这个阶段每张图片生成 30k 个不同尺寸的 候选框。然后通过NMS，减少到1k 个，然后再取 top-100或top-200个。 不过有个疑问的地方：这里的ROI是怎么来的作者没说清楚。 这也是与Faster R-CNN的不同之处，Faster R-CNN的 ROI Pooling 是在RPN生成 region proposals 之后作用在生成的region上的。如下图Faster R-CNN的结构，对比一下二者在整体结构上的差别。 而在HyperNet中在生成region proposals时就使用了 ROI Pooling ，如下。 Faster R-CNN中的ROI是 RPN生成的 region proposals。 HyperNet中的ROI是怎么来的是个疑问。而且HyperNet中的ROI相当于 Faster R-CNN中的anchor box。 一直不知道HyperNet中的ROI是怎么来的，直到我在知乎上找到了 这个帖子： 对话CVPR2016：目标检测新进展 这个帖子的作者（孔涛,清华大学）好像就是这篇文章的作者 （Tao Kong，Tsinghua University） 里面有一段话： HyperNet：文章的出发点为一个很重要的观察：神经网络的高层信息体现了更强的语义信息，对于识别问题较为有效；而低层的特征由于分辨率较高，对于目标定位有天然的优势，而检测问题恰恰是识别+定位，因此作者的贡献点在于如何将deep ConvNet的高低层特征进行融合，进而利用融合后的特征进行region proposal提取和进一步目标检测。不同于Faster R-CNN，文章的潜在Anchor是用类似于BING[4]的方法通过扫描窗口的方式生成的，但利用的是CNN的特征，因此取得了更好的性能。 通过以上的改进策略，HyperNet可以在产生大约100个region proposal的时候保证较高的recall，同时目标检测的mAP相对于Fast R-CNN也提高了大约6个百分点。 里面有一句： 不同于Faster R-CNN，文章的潜在Anchor是用类似于BING[4]的方法通过扫描窗口的方式生成的，但利用的是CNN的特征，因此取得了更好的性能。 也就是说 HyperNet中的ROI 是使用类似于BING的方法提取的 ，上面提到的 BING[4] 文献是： [4] Cheng M M, Zhang Z, Lin W Y, et al. BING: Binarized normed gradients for objectness estimation at 300fps. In CVPR 2014 这篇文章的项目主页是：http://mmcheng.net/bing/ 大概看了一下论文，是一种利用手工设计的特征进行 object 估计的。这个特征叫做 binarized normed gradients features （BING）. 具体是怎么弄的，没仔细看。 看一下结果好了，图中的SEL是我们熟知的 Selective Search算法。DR是指object detection rate ，不清楚怎么算的。 运行速度如下： 测试条件：Intel i7-3940XM CPU Method [1] OBN [2] CSVM [3] SEL [4] Our BING Time (seconds) 89.2 3.14 1.32 11.2 0.003 可见BING是比 Selective Search效果更好的算法，速度也比 Selective Search 快很多，可达300FPS。 也就是说HyperNet使用了额外的 region 估计算法，使得后面的 region proposals 提取的region更加精确，然后之后再进行object detection。 作者说的是使用的是类似于 BING的方法，具体怎么用的，没有代码，不知道是怎么用的。 2.3 Object Detection 与Faster R-CNN不同之处在于 在两个全连接层之前使用了一个 3×3 卷积层，降低了输入通道（从126到63）。后面是三个全连接层。 还有一个不同是，全连接层之间的 dropout 层的概率使用的是0.25，不是0.5。 2.4 训练策略损失函数： 没啥大变化 训练步骤： 与Faster R-CNN训练步骤一样，都是先训练 region proposals 网络，然后保持卷积层部分权值不动，再训练object detection 网络。然后再保持卷积层特征提取部分权值不动，再训练region proposals 网络。 只不过这里多训练了两次。 2.5 加速 如上图，上边的图中两个空白矩形框，第一个是 ROI Pooling的输出，第二个是 3×3卷积的输出。 下边 的图中，3×3卷积的输出用了一个立方体，后面那个白色矩形框是ROI Pooling的输出。 在生成region proposals 时，将卷积层放在 ROI Pooling层之后，可以实现加速。 先卷积降低了通道数量（由126降到4）； 大量 proposal 的conv操作移至前方，实现计算共享； ROI Pooling 后面分类器的结构变成了全连接层，简化结构，之前是 卷积层＋全连接层。（个人觉得这个作用不大） 使用这个策略获得了 40倍的加速。 3 试验结果分析3.1 region proposals 上图衡量的是 不同的IOU 阈值 对 recall的影响。再说明一下，这个recall不是最终的detection recall。而是region proposals recall。 IOU 阈值越大，条件越严苛，recall势必会下降。对比几条不同方法的结果，HyperNet下降的最慢，比其他方法recall也高。 上图衡量的是 不同的proposals 数量 对 recall的影响。再说明一下，这个recall不是最终的detection recall。而是region proposals recall。 proposals数量越少,条件越严苛,recall势必会下降。对比几条曲线，HyperNet下降的最慢，比其他方法recall也高。 上面这个表，衡量的是IOU=0.7的情况下，region proposals recall达到50%和75%时，需要proposals的数量。很明显 HyperNet需要的最少。 Conv1、Conv2、Conv3、Conv4、Conv5 5个卷积层，为什么选择1,3,5呢？答案在下图中。 3.2 Object Detection结果： VOC 2007 VOC 2012 关于小目标的检测，没有明确标准，只是使用的PASCAL VOC中相对较小的目标（bottle,plant,chair等）做检测。 3.3 Hyper Feature Visualization 3.4 Running Time 4 总结整个网络就是 Faster R-CNN的改进版。最大的改进就是特征融合方法（Hyper Feature）和ROI 的提取方法（BING）。使得region proposals的数量最少降到了100，就可以获得比 Faster F-CNN高的mAP。 另外，虽然作者说对小目标的的预测效果有改善，但只是从定性的方面来表达的，个人认为应该针对 COCO数据集进行定量测试才更有说服力。 5 参考资料 孔涛：对话CVPR2016：目标检测新进展：https://zhuanlan.zhihu.com/p/21533724 BING: http://mmcheng.net/bing/ http://blog.csdn.net/u012361214/article/details/51374012]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础DL模型-Deformable Convolutional Networks-论文笔记]]></title>
    <url>%2Farticles%2Fa6992dce%2F</url>
    <content type="text"><![CDATA[论文 Deformable Convolutional Networks代码 https://github.com/msracver/Deformable-ConvNets 这篇文章是微软亚研院的作品，跟STN的思路基本上是一样的，但是做法还是有很大不同。 1 简介上一篇讲STN（Spatial Transformer Networks）的时候，提到 CNN中池化层的存在带来了空间不变性（旋转不变性，平移不变性等），而 Spatial Transformer Networks 也是为了增加空间不变性。但是二者增加空间不变性的出发点是不同的： CNN是尽力让网络适应物体的形变，而STN是直接通过 Spatial Transformer 将形变的物体给变回到正常的姿态（比如把字摆正）。 也就是说，从模型的几何变换能力（空间变换能力）来说，CNN是比较弱的，因为它主要是通过合适的降采样来减弱目标形变对预测的影响。CNN的几何变换能力不好，是因为它是一个固定的几何结构（固定的卷积核，池化核），所以CNN对几何形变的建模能力是有限的。而STN就是一个比较好的具备对几何形变建模能力的模型。 通常来说为了适应物体的几何形变一般有以下几种做法： 数据扩展 data augmention ：这个在深度学习中很经常使用，比如 multi-view 的方式，选取5个patch等 使用具备平移不变性的特征或算法 transformation-invariant features and algorithms ：特征，比如 SIFT 特征，一般在传统的方法中使用；算法，比如卷积神经网络，目标检测模型中的sliding window，现代计算视觉中使用。 CNN模型中还可以使用池化的方法 缺点： 几何变换能力是固定的，已知的：比如CNN固定的卷积核，固定的池化核，固定的ROI；数据扩展是先验知识的一种利用。对于新的视觉任务（比如换个数据集），之前构建的固定的几何变换模型可能就不能用了，这也是为什么在ImageNet上训练好的分类模型用到检测或分割上时要 fine-tuning。 对于手工设计的 特征比如SIFT，在一些复杂的视觉任务中，表现很差，甚至是不可行的。 CNN在分类，检测，分割领域取得了很大进展，这些模型增强几何变换能力的方法也基本是上面提到的三种方法,但是上面提到的缺点仍然存在。 CNN固定的卷积核，固定的池化核，固定的ROI，导致高层神经元的感受野（receptive field ）是固定的，没有办法有效的编码语义或者位置信息。 如果感受野能够自适应的动态调节，那对精细化的定位问题是很有利的。 这篇文章在STN的基础之上，针对分类和检测提出了两种新的模块 deformable convolution （可变性卷积）和deformable RoI pooling （可变性 ROI池化），来达到动态调节感受野的作用，取得了不错的效果。 2 Deformable Convolutional Networks为方便表示，以下所有都是在2D卷积的基础上示例，对于3D卷积 来说每个通道的变换是一样的。 2.1 Deformable Convolution 先看下可变性卷积的效果，如上图： (a)：标准卷积采样点 (b), (c), (d)：浅绿色是标准卷积采样点，深蓝色是可变形卷积采样点，浅蓝色箭头是偏移（offset） (c), (d)是(b)的特例，其中(c)是 带孔卷积(Atrous convolution )或者叫膨胀卷积(dilated convolution)，这说明可变形卷积是比较具有一般性的卷积形式。 可变形卷积与标准卷积相比，采样点不再是固定的了，而是有个偏移量offset，这个偏移量是2维的，通过网络学习得到，它使得采样点的形状可以根据目标发生变化。 2D卷积： 用 $R$ 表示卷积核中的格点，比如以下是一个 3×3 的卷积核的表示 标准2D卷积的过程： $x$ ：输入 feature map $y$ ： 输出 feature map $p_0$ ：输出feature map $y$ 上的坐标 $p_n$ ： $R$ 中的坐标 采样点坐标： $p_0+p_n$ ，整数 $w$ ：采样参数，CNN中就是权值 $w(p_n)$ ：代表卷积核中的权值 $x(p_0+p_n)$ : 采样点处的像素值 可变形卷积： 要给 $R$ 加上偏移 offset： $\{\Delta p_n | n=1,…,N\},N=|R|$ 现在的采样坐标点变成了 $p_0+p_n+\Delta p_n$ ，$\Delta p_n$ 是小数，所以采样坐标也变成了小数坐标 $x(p_0+p_n+\Delta p_n)$ ：对应到feature map上没有像素值，因此又需要插值了 插值函数： $p=p_0+p_n+\Delta p_n$ : 输入feature map $x$ 上的小数坐标 $q$ : 输入feature map $x$ 上的所有整数坐标 $G(q,p)$ ：插值核函数，可以代表更一般性的插值核函数，比如线性插值，最近邻插值等，不过这里用的是双线性插值 因此插值核函数： $g(a, b) = max(0, 1 - |a - b|)$ 大概说一下线性插值核函数，如下：$$W_{bil}(x,y)=w_{lin}(x) \cdot w_{lin}(y)=\begin{cases}1-x-y-x\cdot y &amp; for \, \, \,0 \le |x|,|y| &lt;1\[2ex]0 &amp; otherwise\end{cases}$$详细的参考：双线性插值 ， 图像处理之插值运算 . 虽然插值需要对输入feature map $x$ 上的所有整数坐标进行遍历和计算，但是由于 $G(q,p)$ 实际上在很多位置都是0，所以插值计算是很快的。 可变形卷积的结构示意图 这个模块有两条 流动路线， 下面一路是一个标准的 3×3 卷积， 上面也是一个 3×3 卷积，步长一样，输出 offset field 与 input feature map 大小一样 通道为 2N 个，代表N个2维的偏移场，一个通道是一维（x 或 y） 剪裁（crop）出于卷积核相对的那一块区域，得到offsets 与下面一路标准卷积核进行相加，然后线性插值，确定采样点坐标 最后进行卷积操作即可 后向传播： 对输入feature map的后向传播与标准卷积一样 对 offset $\Delta p_n$ 的导数如下： 其中可以通过公式4求出。 $\partial \Delta p_n$ 代表 $\partial \Delta p_n^x$ 或 $\partial \Delta p_n^y$ 2.2 Deformable RoI Pooling首先是 ROI Pooling： 回顾一下 ROI 池化： 输入feature map ：$x$ RoI 的大小 ：$w×h$ 左上角点坐标： $p_0$ RoI pooling 将 RoI 分成 $k × k$ ($k$ 是自由参数，可以人为设定) 个网格(bins )，每个网格中像素有多个，大小： $\dfrac{w}{k} × \dfrac{h}{k}$ 输出 $k × k$ 大小的 feature map： y 坐标 $(i, j)$ 处的网格 $(0 ≤ i, j &lt; k)$ 的输出： $p\in bin(i,j)$ 坐标 $(i, j)$ 处的网格的像素跨度：$\lfloor i {w \over k}\rfloor \le x \le \lceil (i+1){w \over k}\rceil$ 和 $\lfloor j {h \over k}\rfloor \le y \le \lceil (j+1){h \over k}\rceil$ ，$\lfloor \cdot \rfloor$ 和 $\lceil \cdot \rceil $ 分别代表下界和上界 $n_{ij}$ : 坐标 $(i, j)$ 处的网格中的像素总数，这是个平均值池化 $p_0+p$ $(i, j)$ ：处的网格中的坐标，即采样点坐标 同样的要给 ROI 加上偏移 offset： $\{\Delta p_{ij} | (0 ≤ i, j &lt; k) \}$ ，如下： 同样的 $\Delta p_{ij} $ 一般都是小数，同样的还是用双线性插值来完成坐标映射，公式3和公式4. 可变形 ROI 池化 结构图 首先从ROI中生成 池化后的 feature map 其后全连接层生成 归一化的（normalized） offsets： $\Delta \hat p_{ij}$ ，这个归一化的向量是必须的，因为ROI的尺寸不一，必须要统一标准，比如归一化到 [-1,1] 归一化的向量 $\Delta \hat p_{ij}$ 再生成 offsets $\Delta p_{ij}$ , 直接与ROI做 点积： $\gamma$ 是一个预定义的系数，用于衡量 偏差offset 的 重要程度，按经验选取，文章使用的 $\gamma =0.1$ . 后向传播： $\dfrac{\partial y(i,g)}{\partial \Delta \hat p_{ij}}$ 可以很轻松的通过 求出。 再来看看Position-Sensitive (PS) RoI Pooling 基本与ROI Pooling相同，因为池化后的feature map 中的每一个像素都来自不同组的score map 的对应位置（颜色对应），所以公式5和6 中的 输入feature map 由 $x$ 改为 $x_{ij}$ 不同的是 offset的生成方式， offset 的生成方式与 分类问题中的 PS Pooling是一样的 从输入feature map另起一路，先生成 $2k^2(C+1)$ 通道的 score map（C个类别，偏移量是2维的），然后PS Pooling 同样也是先生成 归一化的 偏移量，然后再转换成 与ROI 尺寸相关的 offset 与 池化坐标相加，得到采样点坐标。 上面图中的加了偏移量的网格（bins）已经与原来的网格产生了偏移 3 试验3.1 试验准备现有的CNN模型一般都包含两部分，一部分特征提取，一部分分类或检测或分割等。 Deformable Convolution for Feature Extraction ResNet-101 Aligned-Inception-ResNet ，这是 Inception-ResNet 的一个改进版本，因为对于密集预测任务（比如检测和分割） Inception-ResNet 的特征和原图上的目标不能对齐。 这两种网络的降采样比例由原来的32 调整为 16，并使用了 带孔卷积（Atrous convolution ） 或者叫做 膨胀卷积（dilated convolution ） 经过试验验证 deformable convolution 放置于网络最后3层。 Segmentation and Detection Networks DeepLab ：用于分割 Category-Aware RPN ：检测，RPN的分类由二分类换成多分类 Faster R-CNN 使用 FPN（Feature pyramid networks for object detection. ）的配置，ROI Poling 换为 deformable RoI pooling. R-FCN ：PS Pooling 换为 deformable position-sensitive RoI pooling. 3.2 Understanding Deformable ConvNets (a)：标准卷积中固定的感受野和卷积核采样点 (b)：可变形卷积中自适应的感受野和卷积核采样点， 意味着可变形卷积中高层神经元看到原图的区域更接近目标，所以更容易分类 这图表示的是高层的神经元在原图上的感受野和采样点 每个图像三元组显示了三层3x3可变形卷积核的采样点位置（共729个点），对应于三个不同的图像区域（从左至右，背景，小物体，大物体） 绿色点对应高层的一个像素点，红色点对应高层神经元在底层的感受野内的采样点，与figure 5对应 R-FCN模型中 PS Pooling换成可变形池化后，ROI中的网格分布 比较整齐的黄色框是 PS ROI Pooling 红色框是deformable PS ROI Pooling 很明显红色框更接近目标的形状 3.3 Ablation StudyDeformable Convolution 对检测或分割的效果 控制变量：deformable convolution 的数量 和 不同的网络模型 整体上来说 deformable convolution 越多，效果越好；但是从3-6的增益不大，所以文章其他部分都是用的3个deformable convolution 为了说明模型中高层神经元的感受野是动态自适应的，文章做了一个粗略的测量： 对可变形卷积核，加了offset 前后的采样点对，计算他们之间的平均距离，称为 有效膨胀：effective dilation 。这可以粗略的衡量感受野的大小，effective dilation越大，感受野越大 看看效果： 使用包含3个 deformable layers 的R-FCN 在 PASCLA VOC2007数据集上检测，目标按照COCO 的标准分为大，中，小。这三个可变形卷积层的有效膨胀 统计如下： 越大的目标有效膨胀越大，感受野也越大，确实在感受野的区域面积上实现了动态自适应调整 背景的感受野介于 中等目标和大目标之间 当然这个只是对感受野的区域大小作评估，而感受野的区域是如何跟随目标的形状动态变化的，就只能从figure5，figure6，figure7中观察了 ​ 疑问：这怎么跟figure 6上的可视化效果不太一样？figure 6上可是背景的感受野最大, 感觉有效膨胀确实不太准确, 个人认为有效膨胀衡量的是膨胀前后两个相对应的坐标对之间的距离，跟坐标点分布的大小区域的关系不太大，不过文章也说了是粗略测量。 atrous convolution 下图（b）是带孔卷积，可以跳着选，隔一个加一个，即 dilation=1，隔两个的话就是 dilation=2 再来看看带孔卷积的效果，膨胀卷积实际上是本文提出的可变形卷积的特例： dilation 从 2到4，到6，到8，整体上来看，效果在增加，说明默认 default network 的感受野比较小 最优的 dilation value 对于不同的任务和模型也是不同的，6 for DeepLab but 4 for Faster R-CNN deformable convolution 有最好的效果，说明自适应的学习可变形的卷积核或池化核是有效的 对比 Faster RCNN 和 R-FCN，当 deformable convolution 和 deformable RoI Pooling 同时使用时, 能够得到显著的效果提升， 模型复杂度对比： 参数会增加，但是增加的很少，对时间影响不大 说明可变形卷积网络获得的收益不是由于参数增加而获得的 COCO上的效果对比： 无疑 可变形卷积神经网络取得了不错的成绩 将ResNet101换为Aligned-Inception-ResNet ，即便不使用可变形卷积也会提高精度，说明 Aligned-Inception-ResNet 效果比 ResNet101好，Aligned-Inception-ResNet到底是个啥，等会说。 4 与STN的比较整个结构跟STN(Spatial Transformer Networks )还是比较像的。 deformable convolution 中学习到的偏移量 offset 可以认为是一种 轻量化的 spatial transformer 。 spatial transformer 是一种全局几何变换，一次把 目标所在区域全部给变换过去； 而deformable convolution 则是以局部、密集采样的方式完成几何变换的，本质上还是属于卷积操作。 spatial transformer crop出的区域是一个平行四边形，相对来说还是比较规整的； deformable convolution 中的感受野是可以根据目标形状随意形变的。 STN比deformable convolution的参数要多，而且学习变换矩阵比学习偏移量更难。 STN虽然在小的数据集上取得了比较好的效果，不过在大的数据集上的效果好像不怎么好，这篇论文好像有探讨：Inverse compositional spatial transformer networks. 5 Aligned-Inception-ResNetAligned-Inception-ResNet是Inception-ResNet的改进版本，由于 Inception-ResNet网络太深，使用了很多卷积层和池化层，深层神经元（靠近输入层）在原图上的映射，与感受野的中心是不能对齐的，比如下图这样的一层卷积，后一层的每个像素都与感受野的中心是对齐的，但是这样的层堆起来之后，然后再加一些池化层的话，就很有可能对不齐了。 实际上这种情况在深层卷积神经网络中是比较普遍的，一般都是通过控制池化层数量（比如ResNet，GoogLeNet模型中就没有池化层）和总降采样比例来减缓的，而且有些视觉任务是在能够对齐的假设下构建的，比如FCN，最后的feature map中的一个 cell 的预测对应了原图的一个位置。 为了弥补这个缺陷，何凯明还有微软亚研院一干人等 提出了 Aligned-Inception-ResNet ，这是一篇没出版的论文，arxiv网站上也没找到，这里是在 deformable convolutional networks 这篇论文的 附录部分看到的。 看下面这篇论文中的引用：unpublished work 论文中也声明了： 不过网络结构倒是挺清晰的，个别细节不太清楚。 网络结构看table6 和 figure 8： 与原始的Inception-ResNet，相比， Aligned-Inception-ResNet 使用了很多重复的结构，称为 IRB ，如figure 8，整个网络结构比 Inception-ResNet简单了好多； 关于对齐的方式，附录只是说在卷积层和池化层使用了合适的 padding（proper padding in convolutional and pooling layers. ）。具体是怎么做的也没说清楚. ImageNet上的效果如下： 可见效果并没有超越其他的结构，号称是Inception-ResNet的改良，竟然还不如 Inception-ResNet，而且参数也多，IRB那个结构感觉也是将Inception-ResNet 中的模块调整了一下，没有啥太大本质的变化。 唯一有效的恐怕就是在分割或者目标检测中用作基础网络时效果会好点吧，见table5. 这个结构目前只在两篇文章中见到过：一篇 是上面讲的 deformable convolutional networks ，还有一篇是：Flow-Guided Feature Aggregation for Video Object Detection ，都是微软亚研院的工作。 参考资料 双线性差值 图像处理之插值运算 https://mlnotebook.github.io/post/CNN1/]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>basic dl models</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础DL模型-STN-Spatial Transformer Networks-论文笔记]]></title>
    <url>%2Farticles%2F7c7952f0%2F</url>
    <content type="text"><![CDATA[论文：Spatial Transformer Networks，是Google旗下 DeepMind 公司的研究成果。 这篇论文的试验做的特别好。 1 简介1.2 问题提出CNN在图像分类中取得了显著的成效，主要是得益于 CNN 的深层结构具有 空间不变性（spatially invariance）（平移不变性，旋转不变性），所以图像上的目标物体就算是做了平移或者旋转，CNN仍然能够准确的识别出来，这对于CNN的泛化能力是有益的。 空间不变性主要是由于 Pooling 层 和 步长不为1的卷积层 的存在带来的。实际上主要是池化层的作用，因为大部分的卷积层的步长都是大于1而又小于卷积核大小的，也就是滑动时是有重叠的，而池化层一般不是重叠的。也就是说这些层越多，越深，池化核或卷积核越大，空间不变性也越强；但是随之而来的问题是局部信息丢失，所以这些层越多准确率肯定是下降的，所以主流的CNN分类网络一般都很深，但是池化核都比较小，比如2×2。 比如ResNet，GoogLeNet，VGG，FCN，这些网络的总降采样比例一般是 16或32，基本没有见过 64倍，128倍或者更高倍数的降采样（会损失局部信息降低准确率），也很少见到 2倍或者4倍的降采样比例（空间不变性太弱，泛化能力不好）。不过这个是跟数据集中的图像大小有关的，上述主流图像分类网络基本都是针对于 ImageNet数据集做分类的，ImageNet中的图片都比较大，一般在 256×256 左右。如果数据集中的图像本来就很小，那么降采样比例就也会小，比如 MNIST数据集，图像只有28×28，所以LeNet中的降采样比例是4。总之，降采样比例要根据数据集调整，找到合适的降采样比例，才能保证准确率的情况下，有较强的空间不变性。 那么如何在保证准确率的情况下，即不损失局部信息的前提下，增强网络的空间不变性呢？这篇文章就是为了解决这个问题。 1.2 解决方法对于CNN 来说，即便通过选择合适的降采样比例来保证准确率和空间不变性，但是 池化层 带来的空间不变性是不够的，它受限于预先选定的固定尺寸的池化核（感受野是固定的，局部的）。因为物体的变形包括旋转，平移，扭曲，缩放，混淆噪声等，所以后面feature map中像素点的感受野不一定刚好包含物体或者反映物体的形变。 文章提出了一种 Spatial Transformer Networks，简称 STN，引进了一种可学习的采样模块 Spatial Transformer ，姑且称为空间变换器，Spatial Transformer的学习不需要引入额外的数据标签，它可以在网络中对数据（feature map）进行空间变换操作。这个模块是可微的（后向传播必须），并且可以插入到现有的CNN模型中，使得 feature map具有空间变换能力，也就是说 感受野是动态变化的，feature map的空间变换方向 与 原图片上的目标的空间变换方向（一般认为是数据噪声）是相反的，所以使得整个网络的空间不变性增强。试验结果展示这种方法确实增强了空间不变性，在一些标志性的数据集（benchmark）上取得了先进的水平。 图1 在输入层使用 Spatial Transformer 空说无凭，先看一个简单效果，如图1： (a) ：输入图片 (b) ：框起来的是用于后面网络进行进一步识别分类的部分，这一部分是就是Spatial Transformer的结果 (c) ：输出层的可视化 (d) ：预测结果 整体上来看是一种视觉 attention 机制，也更像一种弱的目标检测机制，就是把图片中物体所在区域送到网络后面的层中，使得后面的分类任务更简单。 CNN是尽力让网络适应物体的形变，而STN是直接通过 Spatial Transformer 将形变的物体给变回到正常的姿态（比如把字摆正），然后再给网络识别。 文章给的 Spatial Transformer 的使用场景： image classification ：如果数据集中的图像上的目标形变很大，噪声很大，位于图片中心较远，那么 Spatial Transformer 可以将物体部分 “剪裁” 出来，并做一定的旋转，缩放变换，使之成为大小统一的图片，便于后续网络识别，并且获得比CNN更好的结果。 co-localisation ：给定输入图片，不确定是否有物体，如果有，可以使用Spatial Transformer做出定位。 spatial attention ：对于使用attention机制的视觉任务，可以很轻松的使用 Spatial Transformer 完成。 看完这篇论文之后，个人觉得目标检测（object detection）也是可以用的，果不其然，真有人将类似的方法用在了 目标检测上，这篇论文就是 Deformable Convolutional Networks ，后面再讲。 2 Spatial Transformer结构文章最重要的一个结构就是 Spatial Transformers ，这个结构的示意图如下： 图2 Spatial Transformers 结构图 这样一个结构相当于 CNN中的一个 卷积层或者池化层： 这个结构又被分为三部分：localisation network ，grid generator和sampler 一些符号意义： $U \in R^{H \times W \times C}$ 为输入 feature map $V \in R^{H’ \times W’ \times C’}$ 为输出 feature map $\theta=f_{loc}(U)$ 是一个回归子网络 $T_{\theta}$ 表示以参数 $\theta$ 为变换矩阵的某种变换，可以是2维仿射变换(2D affine transformation )，平面投影变换(plane projective transformation )，薄板样条变换(thin plate spline ) $ G_i = (x^t_i, y_i^t)$ 代表V中的像素点 $G = \{G_i\} $ 是V中像素点的整体。 $T_{\theta}(G)$ 代表下面图3中，输入U上的绿色区域的坐标。 这个图与图1做个对应，U 相当于 图1 中的 (a) , V相当于 图1 中的(c)，中间那一部分相当于图1 中的(b), 作用就是为了找到那个物体所在的框，或者叫做弱目标检测。 2.1 Localisation network这一部分很简单，可以使用全连接层或者全卷积层，只要保证最后一层是一个回归层即可，最后输出的一个向量是 $\theta$ 。 $\theta$ 的维度下面再说。 2.2 Grid generator前面提到中间那一部分是为了找到那个物体所在的框，并把它给 变换回 “直立的状态”。很自然就能想到使用仿射变换就可以完成，如下图： 图3 (a)恒等变换与采样； (b)仿射变换与采样 我们期望的是输出 V 是 将U中某一部分（比如绿色点覆盖的部分）做了旋转，放缩，平移之后的feature map。 看一下Grid generator是如何进行仿射变换的。 先简单的看一下仿射变换： 仿射变换用于表示旋转，缩放和平移，表示的是两副图之间的关系， 以下 A 为旋转矩阵，B 为平移矩阵，M称为仿射变换矩阵。 假设要对二维向量 进行仿射变换，仿射变换可以写成如下两式，两种写法等价： 输出的结果是： 对于仿射变换来说，一般的用法有两种： 已知 M 和 X，求T; 这个很简单，直接矩阵相乘。 已知 X 和 T , 求M; 可以选取三对点，带入上面的式子中，列方程，6个方程6个未知数； 这里使用的是第一种用法。其中 图3 (b) U 中的被绿点覆盖的那一部分相当于这里的 T，V相当于这里的 X，那不是应该 M也是已知的吗？M哪去了？还记得上面提到的 $\theta$ ？ $\theta$ 就相当于这里的M。因为 M的大小是 2×3 ，所以 $\theta$ 的维度为6。如果使用了别的变换方法，那就根据变换矩阵的大小相应调整。也就是说这里的变换矩阵是学习出来的。 对应于图3的变换公式如下： $(x^t_i, y_i^t)$ are the target coordinates of the regular grid in the output feature map ，代表的是图3输出V中的像素点，即目标像素坐标； $(x^s_i, y_i^s)$ are the source coordinates in the input feature map that define the sample points ,代表的是图3输入U中被绿色点覆盖的像素点，即源像素坐标； $A_{\theta}$ is the affine transformation matrix ，代表的是仿射变换矩阵。其中的成员 $\theta_{ij}$ 由 localisation network 回归生成。图3或图2中的 $T_{\theta}$ 这时指的仿射变换 $A_{\theta}$。 注意他这个仿射变换是 从后向前变换的，就是说这个模块的输出是仿射变换的输入，这个模块的输入的其中一部分（图3(b) 绿点覆盖部分）是仿射变换的输出。 按照一般的做法，应该是从前往后变换，即从 source coordinates 得到 target coordinates 。但是这样做的问题是，如何确定变换的输入？如果是从前往后做变换，U 中绿色部分相当于 X，那怎么确定这一部分是多大，什么形状，位置在哪？ 实际上从后往前变换也就是为了解决这个问题，就是要根据输出V的坐标得到输入U中目标所在的区域的坐标（绿色的区域）。 仿射变换变换的是坐标，既是坐标，那么变换的输入和输出的坐标的参考系应该是一样的，就是说 V 中像素的坐标 和 U 中像素的坐标应该是同一个参考系。这里使用的是针对 宽和高 进行的归一化坐标(height and width normalised coordinates)，把在U和V中的像素坐标归一化到 [-1,1] 之间。U的 尺寸是上一层决定的，V的尺寸是人为固定的，输出 $H’,W’$ 可以分别比 输入$H,W$ 大或者小，或者相等。 可以给仿射变换的变换矩阵添加更多的约束： 这时候，绿色区域已经确定了，相当于V中对应坐标$(x^t_i, y_i^t)$ 的像素都将从U中这块绿色区域中获取。 $H’,W’$ 与$H,W$ 不一定相等；即便是相等，由于变换后的源坐标 $(x^s_i, y_i^s)$ 很有可能不是整数 ，对应U中不是整数像素点，所以没有像素值，没办法直接拷贝。所以V中 $(x^t_i, y_i^t)$ 坐标的像素值如何确定就成了问题。这时就涉及到采样和插值。 2.3 Sampler实际上 CNN中的卷积核 或者 池化核起到的就是采样的作用。 $(x^s_i, y_i^s)$ 是U中绿色区域的坐标，来看看更加具有一般性的采样问题如何描述： $U_{nm}^c$ 是输入feature map上第 $c$ 个通道上坐标为 $(n, m)$ 的像素值； $V_i^c$ 是输出 feature map上第 $c$ 个通道上坐标为 $(x^t_i, y_i^t)$ 的像素值； $k()$ 表示插值核函数； $Φx , Φy$ 代表 x 和 y 方向的插值核函数的参数； $H,W$ 输入U的尺寸; $H’,W’$ 输出 V 的尺寸; 注意上式只是针对一个通道的像素进行采样，实际上每个通道的采样都是一样的，这样可以保留 空间一致性。 卷积的操作也是符合上式的，比如一维卷积： $f(\tau)$ 相当于 $U_{nm}^c$ ； $g(n-\tau)$ 相当于 $k(x_i^s-m; \Phi_x)$ 或 $(y_i^s-m; \Phi_y)$ 因为这里的卷积是 一维的。 理论来说 任意 对 $x^s_i, y_i^s$ 可导或局部可导的采样核函数都是可以使用的. 比如最近邻插值核函数: 其中 $\lfloor x + 0.5\rfloor$ 向下取整 这个插值核函数做的就是把U中 离 当前源坐标 $(x^s_i, y_i^s)$ （小数坐标） 最近的 整数坐标 $(n,m)$ 处的像素值拷贝到V中的 $(x^t_i, y_i^t)$ 坐标处； 不过这篇文章使用的是双线性插值，双线性插值 参考 维基百科 和 图像处理之插值运算，这里放一张示意图吧： 图4 双线性插值（来源于[参考资料 6]） 这里的公式如下： 这个插值核函数做的是利用 U中 离 当前源坐标 $(x^s_i, y_i^s)$ （小数坐标） 最近的 4个整数坐标 $(n,m)$ 处的像素值做双线性插值然后拷贝到V中的 $(x^t_i, y_i^t)$ 坐标处； 我在想他那个通过仿射变换确定绿色区域之后，绿色区域相当于ROI，那采样能不能使用ROI 池化的方式? 2.4 前向传播结合前面的分析，总结一下前向传播的过程，如下图： 实际上首先进行的是 localisation network 的回归，产生 变换矩阵的参数 $\theta$ ，进而resize为 变换矩阵 $T_{\theta}$ ; 根据 V中的 目标坐标 $(x^t_i, y_i^t)$ 做逆向仿射变换变换到源坐标 $(x^s_i, y_i^s)$ ： $Source= T_{\theta} \cdot Target$ ， 源坐标 $(x^s_i, y_i^s)$ 位于U上；对应图中 1,2步； 在U中找到 源坐标 $(x^s_i, y_i^s)$ （小数坐标）附近的四个整数坐标，做双线性插值，插值后的值作为 目标坐标 $(x^t_i, y_i^t)$ 处的像素值；对应图中 3,4步； 图5 前向传播流程（来源于[参考资料 6]） 2.5 梯度流动与反向传播这个函数虽不是 完全可导 但也是局部可导的，求导如下，对 $y_i^s$ 的导数也是类似的： 根据公式(1)很容易求得： $\dfrac{\partial x_i^s}{\partial \theta} $ 和 $\dfrac{\partial y_i^s}{\partial \theta} $ 。 所以反向传播过程，误差可以传播到输入 feature map（公式6），可以传播到 采样格点坐标(sampling grid coordinates )（公式7），还可以传播到变换参数 $\theta$ . 下图是梯度流动的示意图： 图6 反向传播流程（来源于[参考资料 6]） 其中localisation network中的 $\dfrac{\partial x_i^s}{\partial \theta} $ 和 $\dfrac{\partial y_i^s}{\partial \theta} $ 也就是这一股误差流 $\left\{\begin{matrix}\frac{\partial V_{i}^{c}}{\partial x_{i}^{S}}\rightarrow \frac{\partial x_{i}^{S}}{\partial \theta}\ \frac{\partial V_{i}^{c}}{\partial y_{i}^{S}}\rightarrow \frac{\partial y_{i}^{S}}{\partial \theta}\end{matrix}\right.$ ，在定位网络处就断了。 定位网络是一个回归模型，相当于一个子网络，一旦更新完参数，流就断了，独立于主网络。 3 试验3.1 Distorted MNIST这个试验的数据集 是 MNIST，不过与原版的MNIST 不同，这个数据集对图片上的数字做了各种形变操作，比如平移，扭曲，放缩，旋转等。 如下，不同形变操作的简写表示： 旋转：rotation (R), 旋转+缩放+平移：rotation, scale and translation (RTS), 投影变换：projective transformation (P), 弹性变形：elastic warping (E) – note that elastic warping is destructive and can not be inverted in some cases. 文章将 Spatial Transformer 模块嵌入到 两种主流的分类网络，FCN和CNN中（ST-FCN 和 ST-CNN ）。Spatial Transformer 模块嵌入位置在图片输入层与后续分类层之间。 试验也测试了不同的变换函数对结果的影响： 仿射变换：affine transformation (Aff), 投影变换：projective transformation (Proj), 薄板样条变换：16-point thin plate spline transformation (TPS) 其中CNN的模型与 LeNet是一样的，包含两个池化层。为了公平，所有的网络变种都只包含 3 个可学习参数的层，总体网络参数基本一致，训练策略也相同。 试验结果 左侧：不同的形变策略以及不同的 Spatial Transformer网络变种与 baseline的对比； 右侧：一些CNN分错，但是ST-CNN分对的样本 (a)：输入 (b)：Spatial Transformer层 的 源坐标（$T_{\theta}(G)$ ）可视化结果 (c)：Spatial Transformer层输出 很明显：ST-CNN优于CNN, ST-FCN优于FCN，说明Spatial Transformer确实增加了 空间不变性 FCN中由于没有 池化层，所以FCN的空间不变性不如CNN，所以FCN效果不如CNN ST-FCN效果可以达到CNN程度，说明Spatial Transformer确实增加了 空间不变性 ST-CNN效果优于ST-FCN，说明 池化层 确实对 增加 空间不变性很重要 在 Spatial Transformer 中使用 plate spline transformation (TPS) 变换效果是最好的 Spatial Transformer 可以将歪的数字扭正 Spatial Transformer 在输入图片上确定的attention区域很明显利于后续分类层分类，可以更加有效地减少分类损失 作者也做了噪声环境下的试验：将数字 放置在 60×60的图片上，并添加斑点噪声（图1第三行）错误率分别为： FCN ，13.2% error； CNN ， 3.5% error； ST-FCN ，2.0% error； ST-CNN ，1.7% error. 3.2 Street View House NumbersStreet View House Numbers是一个真实的 街景门牌号 数据集，共200k张图片，每张图片包含1-5个数字 ，数字都有形变。 baseline character sequence CNN model ：11层，5个softmax层输出对应位置的预测序列 STCNN Single ：在输入层添加一个Spatial Transformer ST-CNN Multi ：前四层，每一层都添加一个Spatial Transformer 见下面 tabel 2 右侧 localisation networks 子网络：两层32维的全连接层 使用仿射变换和双线性插值 结果： 3.3 Fine-Grained Classification数据集：CUB-200-2011 birds dataset， 6k training images and 5.8k test images, covering 200 species of birds. baseline CNN model ： an Inception architecture with batch normalisation pre-trained on ImageNet and fine-tuned on CUB – which by itself achieves the state-of-the-art accuracy of 82.3% (previous best result is 81.0% [30]). ST-CNN, which contains 2 or 4 parallel spatial transformers, parameterised for attention and acting on the input image. 这里使用了并行的Spatial Transformer ， 效果是可以将图片的不同 部分（part）输入到不同的 Spatial Transformer 层，会产生不同的 part representations 然后经过 inception ，最后再合并起来，经过一个单独的softmax层做分类。 结果： ST-CNN效果最好 右侧上边使用了 2 路 Spatial Transformer并行，可以看到其中一个 spatial transformer(红色) 检测的是鸟的头部, 而另外一个 (绿色) 检测的鸟的身体. 右侧下边使用了 4 路 Spatial Transformer并行，有相似的效果. 此处的Spatial Transformer有点像目标检测的味道 3.4 MNIST Addition这个试验是将任意两张MNIST中的数字独立的进行一系列变形，然后叠加到一块，给网络识别，标签是二者之和。 同样的测试 FCN, CNN, ST-CNN,2×ST-CNN。 2×ST-CNN在输入层使用了两个并行的Spatial Transformer，结构见下面table 4右侧。 由于数据比较复杂，FCN的效果很差，添加了 Spatial Transformer之后，错误率显著下降 CNN有池化层存在所以效果比FCN好 2×ST-CNN效果最好 从右边可视化的图中可以看到虽然每个输入channel都输入到了两个Spatial Transformer中，但是每个Spatial Transformer都是对其中一个channel作用强，而且这两个Spatial Transformer是互补的，所以最后连接起来之后 4个通道的feature map中有两个是完整的数字，所以识别较为有效。 3.5 Co-localisation这个试验将 Spatial Transformer用在了半监督的任务Co-localisation 。 Co-localisation ：给一些图片，假设这些图片包含一些目标（也可能不包含），在不使用目标类别标签和目标位置标签的情况下，定位出常见的目标。 数据集还是 MNIST ，将 28×28大小的 数字图像 随机的放在 84×84 大小的含有噪声的背景上，对每个数字产生100个不同的变形。数据有定位标签，但是在训练时不用，测试时用。 模型还是使用 LeNet CNN模型，在输入层嵌入Spatial Transformer。 文章使用了半监督的方式，监督的学习过程是这样的： 对于一个 包含 N 张图片的 数据集 $I = \{I_n\}$ ，比如table 5 右侧的图。 最下面一行代表在同类别的样本中挑选一张 $I_n$ 做一个随机裁剪，裁剪出的这一块 $I_n^{rand}$ ,认为是目标位置 中间这一行代表将同样的样本 $I_n$输入 Spatial Transformer，输出 $I_n^{T}$ 上面这行代表另选一个样本 $I_m$ 输入Spatial Transformer，输出 $I_m^{T}$ 以上三个输出分别经过一个映射函数 $e()$ ，这个函数由 LeNet CNN模型提供，以便于将上述三个feature map映射成向量，映射成向量后可以计算两两之间的距离 计算 $L1=||e(I_m^T)-e(I_n^T)|_2$ , $L2=||e(I_n^T)-e(I_n^{rand})||_2$ , 训练过程中就是要保证 L1 &lt; L2，L2是一个随机裁剪与经过Spatial Transformer的输出之间的距离，理应大于L1. 经过上面的分析，可以提出如下损失函数: hinge loss (triplet loss) $α$ is a margin ,可以称为 裕度，相当于净赚多少。 半监督是因为这里的标签相当于 L2，而L2是人为构造出来的距离指标。 测试时认为检测出的box与ground truth bounding box的IOU 大于0.5为正确，table5 左侧为测试结果。 在没有噪声时，可以达到100%的准确率，有噪声时在75-93%之间。 下图是优化过程的动态可视化结果，可见随着迭代次数越来越多，模型对目标的定位越来越准确 这个试验使用了一种简单的损失函数，在不使用数据定位标签的情况下，构建了一种距离标签，实现了对目标的检测。这个可以推广到目标检测或追踪问题中去。 作者把前面一些检测的动态效果做成了视频，看起来很清晰明了，看这里：https://goo.gl/qdEhUu 4 总结这篇文章提出的 Spatial Transformer 结构能够很方便的嵌入到现有的CNN模型中去，并且实现端到端（end-to-end）的训练，通过对数据进行反向空间变换来消除图片上目标的变形，从而使得分类网络的识别更加简单高效。现在的CNN的已经非常强大了，但是 Spatial Transformer 仍然能过通过增强空间不变性来提高性能表现。Spatial Transformer实际上是一种attention机制，可以用于目标检测，目标追踪等问题，还可以构建半监督模。 下一篇介绍 Deformable Convolutional Networks ，跟本篇的TSN思路很像，但是又比这个模型简单。 参考资料 opencv中文教程——仿射变换 仿射变换与齐次坐标 知乎——如何通俗易懂的理解高维仿射变换与线性变换 维基百科——双线性插值 图像处理之插值运算 讲STN的一篇博客，不过关于仿射变换那一块写的是错的，但是其中的图还是挺不错的，借用几张图]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>basic dl models</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-从RCNN到Mask RCNN两步检测算法总结]]></title>
    <url>%2Farticles%2F21c44637%2F</url>
    <content type="text"><![CDATA[目标检测中两步检测算法的总结对比，持续更新…… R-CNN, SPPNet, Fast R-CNN, Faster R-CNN, R-FCN, Light-head R-CNN, Mask R-CNN R-CNNRbg提出的R-CNN的方法 一张图像先通过selective search的方法，生成1K~2K个候选区域，这个步骤生成的候选区域大小是不一样的，因此需要 warped region，也就是将不同大小的 region 缩放到同样的尺寸，因为CNN后面的全连接层要求输入尺寸固定。 对每个 warped 后的候选区域，使用CNN提取特征 ，提取的特征需存储到磁盘； 读取特征，送入每一类的 SVM 分类器，判别是否属于该类； 最终还有一个位置回归器用于精细修正。 回归方法： region proposals 与 ground truth之间的逻辑关系 可以推出回归目标（也就是标签） 回归函数： 损失函数： 训练方法： 优点： 结构简单明了，容易理解； CNN自动提取特征，省去手工设计特征的复杂操作，以及对经验和运气的依赖性； 使用 selective search方法来生成候选区域，显著减少候选区域的数量，增加候选区域的质量（包含目标的可能性更大），因为这相当于一个弱检测器，相比于sliding window穷举搜索的方式肯定要好很多； 预测精度提高了30%。 缺点： 提取特征时，CNN需要在每一个候选区域上跑一遍；候选区域之间的交叠使得特征被重复提取, 造成了严重的速度瓶颈, 降低了计算效率; 将候选区域直接缩放到固定大小, 破坏了物体的长宽比, 可能导致物体的局部细节损失; R-CNN还不是端到端的模型，训练步骤繁琐multi-stage（先预训练、fine tuning、存储CNN提取的特征；再训练SVM ；再regression）。从fine tuning 到训练SVM时，不能一步到位，要分成两步； 训练SVM时需要将之前CNN提取到的特征全部存储在磁盘上，磁盘读写耗时，且占用空间大，（Pascal 200G）； 使用额外的selective search 算法生成候选区域的过程也很耗时； 预测时间很慢，一张图片要49s。 SPPNet针对R-CNN的两个缺陷： 先生成候选区域，再对区域进行卷积，候选区域之间的交叠使得特征被重复提取, 造成了严重的速度瓶颈, 降低了计算效率; 将候选区域直接缩放到固定大小, 破坏了物体的长宽比, 可能导致物体的局部细节损失; 何凯明提出的改进方法如下： 改变生成候选区域的顺序 SPP池化 改变生成候选区域的顺序。从先生成候选区域再提取特征，变成先提取特征再生成候选区域，实现了特征提取部分的计算共享，极大的减少计算量。生成候选区域的方式还是 selective search算法，在原图上生成候选区域后，映射到特征图上去。 使用SPP池化（Spatial pyramid model, 空间金字塔池化池化）：传统的池化方式是 已知输入尺寸和 固定池化核大小，确定输出尺寸，那么这时候输出尺寸肯定是随输入尺寸变化的，所以这时候就要求输入图片是固定尺寸。而SPP池化是 已知输入尺寸 和 固定输出尺寸，来确定确定池化核的大小。SPP 层用不同大小的池化窗口作用于卷积得到的特征图，池化窗口的大小和步长根据特征图的尺寸进行动态计算，最终可以组合成一个特定维度的特征输出。这里的输入可以是一个feature map（分类问题），也可以是一个window（检测问题）。 训练过程： 优点： SPP-net 对于一幅图像的所有候选区域, 只需要进行一次卷积过程, 避免了重复计算, 显著提高了计算效率。该方法在速度上比 R-CNN 提高 24 ~102 倍 . SPP池化层使得检测网络可以处理任意尺寸的图像, 因此可以采用多尺度图像来训练网络, 从而使得网络对目标的尺度有很好的鲁棒性. 缺点： SPP-net 的训练过程更复杂了，（先预训练、存储SPP特征、使用SPP特征fine tuning全连接层、存储CNN提取的特征；再训练SVM ；再regression）。 CNN 提取的特征存储需要的空间和时间开销增大; 在微调阶段, SPP-net 只能更新空间金字塔池化层后的全连接层, 而不能更新卷积层(好像是梯度不连续), 这限制了检测性能的提升。 Fast R-CNN 为什么 SPPnet和 R-CNN训练很慢？ 主要原因有两点 使用SVM做分类器时，需要将特征事先存储到磁盘上，磁盘交互耗时； 训练步骤繁琐，不能联合训练。 针对 R-CNN 和 SPPNet 的这两个问题, rbg 提出 能够端到端联合训练的 Fast R-CNN 算法 ，如下： 首先在图像中提取感兴趣区域 (Regions of Interest, RoI)，还是使用selective search算法，生成的候选区域这里称为ROI，将ROI映射到feature map上; 然后采用与 SPP-net 相似的处理方式,对每幅图像只进行一次卷积, 在最后一个卷积层输出的特征图上对每个 RoI 进行映射, 得到相应的RoI 的特征图, 并送入 RoI 池化层 (相当于单层的SPP 层, 通过该层把各尺寸的特征图统一到相同的大小); 最后经过全连接层得到两个输出向量, 一个进行 Softmax 分类, 另一个进行边框回归. SPP池化 ROI池化 改进的方法： 串行结构改成并行结构 ：原来的 R-CNN 是先对候选框区域进行分类，判断有没有物体，如果有则对 Bounding Box 进行精修 回归 。这是一个串联式的任务，那么势必没有并联的快，所以 rbg 就将原有结构改成并行，在分类的同时，对 Bbox 进行回归； ROI池化：在这个模型里，ROI就是感兴趣区域(Regions of Interest, RoI) ，也就是之前模型中的候选区域。SPP池化的改进，相当于只用了一种尺寸的 SPP池化； 不用SVM分类，改用SoftMax分类，可以省去特征存储； 使用multi-task loss 多任务损失函数（分类+回归），端到端（end-to-end）训练。 训练方式： multi-task 损失函数： 新的挑战及解决方法： ROI池化： 一句话概括作用：将不同尺寸输入的feature map或者 ROI，降采样成固定尺寸的输出 feature map，再送入全连接层。 做法： 将image中的ROI 映射到feature map 中对应位置的区域，这些区域大小是不统一的（已知输入）； 将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同，固定输出尺寸）；这个过程中，每个ROI分好的section中的像素数量是不一样的（池化核的大小和步长根据输入和输出尺寸进行动态计算），最终可以组合成一个特定维度的特征输出； 对每个sections进行max pooling操作； 参考：Region of interest pooling explained 优点： 精度有提高 通过使用 multi-task loss ，可以实现 end-to-end训练，single-stage，除了预训练之外，其他的都是可以一气呵成的。 分类和回归任务可以共享卷积特征,相互促进. Fast R-CNN 采用 Softmax 分类与边框回归一起进行训练, 省去了特征存储, 提高了空间和时间利用率。 与 R-CNN 相比, 在训练 VGG 网络时,Fast R-CNN 的训练阶段快 9 倍, 测试阶段快 213倍; 与 SPP-net 相比, Fast R-CNN 的训练阶段快 3倍, 测试阶段快 10 倍。 缺点： Fast R-CNN 仍然存在速度上的瓶颈, 就是候选区域生成步骤耗费了整个检测过程的大量时间. Faster R-CNN为了解决候选区域生成步骤消耗大量计算资源, 导致检测速度过慢的问题, 任少卿，何凯明，rbg联合提出区域生成网络 (Region proposal network, RPN), 并且把RPN 和 Fast R-CNN 融合到一个统一的网络 (称为 Faster R-CNN), 二者共享卷积特征. 如下： RPN 将一整幅图像作为输入, 输出一系列的矩形候选区域. 它是一个全卷积网络模型, 通过在与 Fast R-CNN 共享卷积层的最后一层输出的特征图上滑动一个小型网络（sliding window）, 这个网络与特征图上的小窗口全连接, 每个滑动窗口映射到一个低维的特征向量, 再输入给两个并列的全连接层, 即分类层 (cls layer) 和边框回归层(reg layer), 由于网络是以滑动窗的形式来进行操作, 所以全连接层的参数在所有空间位置是共享的. RPN的结构在实现时实际上是一个全卷积网络。 RPN是一个弱检测器，RPN的输出是一些可能包含目标的候选框（ region proposals 或者称为 region of interest ,ROI）,这些ROI 将会输入Fast R-CNN中，用于最后的检测。 训练方式： Anchor的作用 按照两步检测的惯例，应该要先有初步的ROI ，然后才是最终的分类和回归，对于原版的Fast R-CNN来说，它的ROI是由selective search算法提供的；而Faster R-CNN中的 Fast R-CNN的 ROI 则是由RPN网络产生的。 那么RPN既然是一个弱检测器，那么RPN的ROI或者 region proposals从哪来？ 答案是从 anchor 中来。 结合上面的图，RPN在CNN提取特征之后以sliding window的方式在最后一个feature map上提取特征，每个滑动窗口中心都关联着 k个 box，这些box就称为anchor，或者叫anchor box。这些关联的box 可以通过逆向映射对应到原图上，对应到原图上的区域就是region proposals，不过这些region proposals都是位于同一个中心点。就是说sliding window时的window（大小固定）是由这些原图上的不同大小和比例的 region proposals 生成的（类似于ROI池化的功能）。 实际上sliding window时每个 window 起到了一部分region proposals 的作用，但是由于这里的sliding window的尺寸是固定的，所以不能起到多尺度，多尺寸（multiple scales and sizes ）预测的作用，因此提出关联k个不同大小和长宽比的anchor box，这样二者结合即可起到多尺度，多尺寸预测的作用。参考下图： 使用anchor的好处是，RPN最后sliding window 时可以使用 卷积的方式实现（因为 window的大小是固定的），使网络变得很简单。 而后面的 256维向量的输入 由于用卷积层实现所以也由 （1,1,256），变成了（W,H,256）. 但是我们知道后面的输入是固定尺寸的 window ，那么在分类和回归时是如何来反映不同尺寸和比例的 region proposals呢？答案是通过 标签和损失函数。 以下引用自 [参考资料 1]，在那位大兄弟的博客中，一开始他的理解是对的，但是后面的补充他又给改错了，但是不是什么大错，内容是对的，只是因果关系搞反了，这里只把内容贴出来： 从nxn提出的256d特征是被这k种区域共享的，在clc layer和reg layer计算损失的时候，用这共享的256d特征 加上 anchor推算出k种区域的坐标和前景、背景的标签，便可以对这k种区域同时计算loss。 clc layer和reg layer同时预测k个区域的前景、背景概率（1个区域2个scores，所以是2k个scores），以及bounding box（1个区域4个coordinates，所以是4k个coordinates），具体的说： clc layer输出预测区域的2个参数，即预测为前景的概率pa和pb，损失用softmax loss（cross entropy loss）（本来还以为是sigmoid，这样的话只预测pa就可以了？）。需要的监督信息是Y=0,1，表示这个区域是否ground truth reg layer输出预测区域的4个参数：x,y,w,h，用smooth L1 loss。需要的监督信息是 anchor的区域坐标{xa,ya,wa,ha} 和 ground truth的区域坐标{x,y,w,h} 显然，上面的监督信息：Y，{xa,ya,wa,ha}（k个），{x*,y*,w*,h*}（1个），就是通过anchor机制产生的。这几个参数的指定（比如k个anchor区域的Y是怎么得到的）是根据文章中的样本产生规则，很多博客中也都提到了。 参考资料： faster-rcnn中，对RPN的理解 faster rcnn中rpn的anchor，sliding windows，proposals？ R-FCN如果不考虑生成ROI的部分（比如RPN，Region Proposal Network），两步检测模型可以分为两部分子网络（subnetworks ）： 第一部分是共享计算的全卷积基础子网络 base，或称body，trunk ，这一部分是与ROI独立的，主要用于提取特征 第二部分是不共享计算的子网络 head，生成的每一个ROI都要经过head部分，主要用于分类 Faster R-CNN 实现了很多计算的共享：ROI之间的特征提取共享计算，ROI 提取与base部分共享计算，但是ROI通过head部分是不共享计算。 R-FCN就是基于FCN将 head部分也实现了计算共享。但是由于直接将 Faster R-CNN 的head部分也就是全连接层改为全卷积层，然后再使用 R-FCN是一种新的基于区域的全卷积网络检测方法. 为了给网络引入平移变化, 构建对位置敏感的池化方式 (Position sensitive pooling), 编码感兴趣区域的相对空间位置信息. 该网络解决了 Faster R-CNN 由于重复计算全连接层而导致的耗时问题, 实现了让整个网络中所有的计算都可以共享 。 Position sensitive pooling： ROI Pooling 中的每一个网格都来自前面 position-sensitive score maps中不同组通道的 feature map。这个跟分组卷积的意思有点像，这个可以叫做分组ROI池化。是一种选择性ROI池化，主要是为了增强对位置的敏感程度。 Light-head RCNN不管是 Faster R-CNN还是 R-FCN 在 ROI(Region of Interest) 生成前后都是计算量很大的，比如 Faster R-CNN 的head部分包含两个全连接层用于ROI 分类，而全连接层极大地消耗计算；R-FCN虽然比 Faster R-CNN快许多，但是由于生成的 score maps太多，想要达到实时（30FPS, Frame Per Second）还是有点困难的。也就是说这些模型之所以速度慢的原因在于计算量过于繁重的 head部分 (heavy-head design )，即便是将 base部分削减压缩，计算消耗也不能很大程度的削减。 本文提出了一种新的两步检测模型， Light-Head RCNN ，为了解决现在的两步检测普遍存在的 heavy-head的问题。在本文模型的设计中使用了thin feature map 和 cheap R-CNN subnet (pooling and single fully-connected layer)，是对R-FCN的改进。 改进： 引进inception V3中可分离卷积的思想，作者采用large separable convolution生成channel数更少的feature map(从3969减少到490)。 用FC层代替了R-FCN中的global average pool，避免空间信息的丢失。 参考资料： https://zhuanlan.zhihu.com/p/33158548 Mask R-CNN在fatser rcnn的基础上对ROI添加一个分割的分支，预测ROI当中元素所属分类，使用FCN进行预测； 具体步骤：使用fatser rcnn中的rpn网络产生region proposal（ROI），将ROI分两个分支：（1）fatser rcnn操作，即经过ROI pooling 输入fc进行分类和回归；（2）mask操作，即通过ROIAlign校正经过ROI Pooling之后的相同大小的ROI，然后在用fcn进行预测（分割）。 ROIAlign产生的原因：RoI Pooling就是将原图ROI区域映射到feature map上，最后pooling到固定大小的功能。当把原图上的ROI 映射到 feature map上时，存在归一化或者量化（即取整）的过程。在归一化的过程当中，由于存在多次量化过程（卷积步长，池化），会存在ROI与提取到的特征不对准的现象出现 也就是feature map上的ROI再映射会原图时会跟原来的ROI对不准，由于分类问题对平移问题比较鲁棒，所以影响比较小。但是这在预测像素级精度的掩模时会产生一个非常的大的负面影响。作者就提出了这个概念ROIAlign，使用ROIAlign层对提取的特征和输入之间进行校准。 ROI Align的思路很简单：取消量化操作，使用双线性插值的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作，。值得注意的是，在具体的算法操作上，ROI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如 图 所示： 遍历每一个候选区域，保持浮点数边界不做量化。 将候选区域分割成k x k个单元，每个单元的边界也不做量化。 在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。 这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。 参考资料： 详解 ROI Align 的基本原理和实现细节]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>学习总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-R-FCN-论文笔记]]></title>
    <url>%2Farticles%2F7e6bc4a%2F</url>
    <content type="text"><![CDATA[这篇文章提出了一种高效简洁的目标检测模型 R-FCN(Region-based Fully Convolutional Networks )，作者将FCN(Fully Convolutional Network)应用于 Faster R-CNN，实现了整个网络的计算共享，极大的提高了 检测速度，同时精度也可以与 Faster R-CNN相媲美。文章提出一种 position-sensitive score maps 用来平衡平移不变性(translation-invariance )和平移可变性(translation-variance )之间的矛盾，模型使用了 全卷积的 ResNet 结构，保证高检测精度的同时，将检测速度提高了 2.5~20倍。 1 从 Faster R-CNN 到 R-FCN目前的目标检测模型一般分为两步检测模型（R-CNN系列）和一步检测模型（YOLO系列和SSD），两步检测精度高，一步检测速度快，各有优势。对于两步检测来说，最成功的模型莫过于R-CNN系列了。 R-CNN系列的检测思路一般是 先生成候选区域 region proposals，或者称为感兴趣区域 ROI (Region of Interest)，然后针对这些 ROI （后面都使用ROI一词）进行分类，也就是说本质上他是把目标检测问题（Object Detection）转化成了分类问题（Classification），不过实际上也是包含回归的，主要用于位置精修，提升准确率。 如果不考虑生成ROI的部分（比如RPN，Region Proposal Network），两步检测模型可以分为两部分子网络（subnetworks ）： 第一部分是共享计算的全卷积基础子网络 base，或称body，trunk ，这一部分是与ROI独立的，主要用于提取特征 第二部分是不共享计算的子网络 head，生成的每一个ROI都要经过head部分，主要用于分类 这个设计拓扑结构实际上是跟经典的用于分类的卷积神经网络很像的，比如用于分类的卷积神经网络前面一部分是用于特征提取的基础网络，后面是用于分类的全连接层，从卷积层到全连接层中间通过一个空间池化层连接。而在目标检测模型中，base与head部分之间使用的是 ROI 池化。 1.1 Faster R-CNN首先回顾一下Faster R-CNN： 最初的 Faster R-CNN的base 部分使用的是VGG的卷积层，head是由全连接层构成的，如图1(a). 后来的使用ResNet网络做base部分的 Faster R-CNN, 它的head是由两部分组成，第一部分是 ResNet 中 的 第5个 卷积block，也就是 Conv5，第二部分是全连接层。这个跟最初提出的 以VGG为特征提取层的 Faster R-CNN不太一样。如图1(b)。 这两种结构，生成的每一个ROI都会经过head部分，head部分是不能共享计算的。这也是 Faster R-CNN速度慢的原因之一，其他的比如 生成的ROI数量等也是影响速度的因素，这里不讨论。 (a) Head of Faster R-CNN - VGG (b) Head of Faster R-CNN -ResNet 101 图1 Faster R-CNN的两种结构 实际上 后来的对于图片分类效果比较好的一些网络比如 GoogLeNet，ResNet都是属于全卷积网络的，他们的结构一般都是将一堆按照特定结构排列的卷积层堆在一起，最后一个feature map经过一个全局平均值池化，然后为了使输出与物体类别数（imagenet，C=1000）相等才会加一个 1000层的全连接层，最后加上softmax层。 之前讲过FCN（Fully Convolution Network）, 对于Faster R-CNN来说它的RPN部分已经是全卷积了，那很自然的会想到 以 ResNet 101 实现的 Faster R-CNN 的 head部分 能不能也使用FCN。如果使用FCN的话，最后一个卷积层的输出是一个代表原图的 heatmap，只需要将 ROI 映射到heatmap上，做一个 ROI Pool ， 然后对最后一个卷积层上的ROI区域 做一个全局平均值池化，像下面图2这样，channel=C对应于不同的类别。 这样的话基本上所有的计算量都能够共享，后面每个ROI经过的都是池化层，而池化层与卷积层和全连接层相比计算量大大减少，几乎可以忽略。 图2 一种可行的全卷积方案 但实际上通过实验验证这么做的效果很差，结果如下： 第三行就是 上面的结构，mAP那一栏写着 fail ，不及格，应该是连60 %都没到。这个表在第2节试验部分详细介绍。 这个原因文章中也分析了： 平移不变性 translation invariance 是深层CNN的特点，这对于图像分类问题来说是很好的一个特性，因为图像分类不管你物体在哪个位置都要求较高的准确性。但是对于目标检测来说，不仅要检测出图上有什么，还需要定位出物体在图片上的位置，也就是说目标检测需要 平移可变性 translation variance 。图2的方案中， 不变性与可变性是一对相互矛盾的问题。因为ROI放在Conv5后面的heatmap上，由于网络太深了，原图的变化已经很难反映在heatmap上了。所以它的效果才不好。 这也就是为什么图1 (b)中的 ROI 池化层不是放在 Conv5 输出的feature map后面，而是放在了Conv4和Conv5两组卷积层之间，Conv5和后面的全连接层构成了head部分。一来可以避免位置信息进一步损失，二来可以使用后面的Conv5卷积层学习位置信息。但是缺点就是 每一个ROI都要再经过Conv5这一组卷积，head部分的计算不能实现共享。图1 (b)可能不太清楚，可以看下图： 图？ 基于ResNet 的 Faster R-CNN 而R-FCN就是着重于解决这一问题的，如何做到head部分使用FCN共享计算的同时，准确率不下降甚至是超过原来？ 只要解决这个问题，即便准确率不比原来高，速度也会提升很多。实际上 R-FCN不仅提高了速度，准确率也提高了。下面详细讲解一下。 1.2 R-FCN1.2.1 R-FCN的整体结构 图3 R-FCN 结构图 网络仍然是包含两部分： region proposal region classification 网络使用了ResNet-101，ResNet-101包含100个卷积层，一个全局平均值池化层，和一个1000维的全连接层。本文去掉了最后的全局平均值池化层和全连接层，最后加上了一个1024个 1×1 卷积核的卷积层用以降维（将Conv5输出的2048维降为1024维）。 看起来跟 图2 好像差不多，但其实是很不一样的，不一样的地方在于 R-FCN 改变了 ROI Pooling的使用方法，称为 Position-Sensitive ROI Pooling，位置敏感的ROI池化，简称PS ROI Pooling，这是为了增加平移可变性而引入的结构 。 1.2.2 Position-Sensitive ROI Pooling 图4 Position-Sensitive ROI Pooling 文章设计了一个 position-sensitive score maps ，做法其实很简单就是通过调整Conv5的卷积核数量，使得输出的通道变为： $k^2(C+1)$ 个。也就是说每一个类别都用 $k^2$ 个通道的feature map来表示，图中不同的颜色就代表一组 $C+1$ 通道的 feature map。 $k×k$ 代表ROI池化的网格大小，以 $k=3$ 为例 。图中最后那个由不同颜色组成的 $C+1$ 通道的 k×k 的 feature map 就是使用 Position-Sensitive Pooling得到的。最后一个PS ROI Pooling之后的feature map（9个格点，每个格点颜色都不一样）中的每一个网格都来自前面 position-sensitive score maps中不同组的 feature map，根据颜色一一对应。这个跟分组卷积的意思有点像，这个可以叫做分组ROI池化。原文称为 选择性池化：our position-sensitive RoI layer conducts selective pooling, and each of the k × k bin aggregates responses from only one score map out of the bank of k × k score maps. 最后那个 颜色混合的 feature map 中每一个网格都是对应着不同的位置信息的，比如 {top-left, top-center,top-right, …, bottom-right} 。随着学习过程的深入，对应的position-sensitive score maps中不同组的（不同颜色的）feature map 的激活值就会对目标的不同位置敏感，从而产生对位置敏感的激活。对目标的位置实现编码（见图5，图6）。原文：With end-to-end training, this RoI layer shepherds the last convolutional layer to learn specialized position-sensitive score maps. vote操作其实就是一个全局平均值池化，生成一个 C+1 维的向量，这个向量就可以用于判断当前ROI是什么类别。 也实现了所有的计算量都能够共享，后面每个ROI经过的都是池化层，而池化层与卷积层和全连接层相比计算量大大减少，几乎可以忽略。 实际这个过程就是人为给网络设定一些模式和期望的结果，然后让网络在训练过程中自己完成。就相当于老师丢给你一道很难的竞赛题，只告诉了你答案，但不告诉你怎么做，你要做的就是绞尽脑汁找到解题方法。这里也是一样的，这个对位置敏感，是人为地希望feature map对位置敏感。至于为什么会敏感，是网络在训练过程中学习到的。 文章也给出了一些可视化的图来解释，如图5，图6： 图5 一个正样本的可视化 图6 一个负样本的可视化 可以看到，以 k=3 为例，这9个 position-sensitive score maps的激活值确实是对位置敏感的，而且如果 某一个ROI如果与 ground truth 的IOU 比较大的话，最后的投票结构是yes（图5），否者是no（图6）。 这个与图2 的结构的不同之处在哪呢? 图2 中那个 channe=C+1 的feature map相当于 图4 中 $k^2$ 组feature map中的一组，也就是其中一个颜色。图2中最后ROI池化后的feature map中的每个像素都是来自上一层的同一组feature map，而图4是来自不同组。 就这么一点差别，其他基本上是一样的。但是就这么一点差别，导致 R-FCN可以做到 精度，速度都有提高。 池化时的公式表达： 上式比较抽象，实际上表达的就是 ROI池化，如下图，表示某一组 score map 中的第c个类别的池化： 图7 某一组 score map 中的第c个类别的池化 图8 ROI 池化 将每个ROI都分为 $k×k$ 大小的网格，对于某个 $w×h$ 大小的 ROI，每一个格点的大小约为 $\dfrac{w}{k} × \dfrac{h}{k}$ ，对于最后一个卷积层的 $k^2$ 组score feature map，PS ROI Pooling 之后的feature map上的 $(i, j)$ $(0 ≤ i, j ≤ k - 1)$位置处的格点的像素值也是由上一层的position-sensitive score feature map的对应通道中的 $(i, j)$ 格点中的多个像素值池化得到的（图4中一一对应的颜色块）。 上式： $r_c(i， j)$ 是 $(i, j)$ 格点处第c个类别的响应值或者是池化输出值； $z_{i,j,c}$ 是 score map 中对应组的 $(i, j)$ 格点中的，第c个类别的，多个像素值之一； $(x_0, y_0)$ 是ROI的左上角在图片中的坐标； n 是 score map 中对应组的 $(i, j)$ 格点中的像素总数 $Θ$ 是网络的学习参数； score map 中 $(i, j)$ 位置处的格点中的坐标跨度为： $\lfloor i {w \over k}\rfloor \le x \le \lceil (i+1){w \over k}\rceil$ 和 $\lfloor j {h \over k}\rfloor \le y \le \lceil (j+1){h \over k}\rceil$ ,$\lfloor \cdot \rfloor$ 和 $\lceil \cdot \rceil $ 分别代表下界和上界。 本文中上式使用的是平均值池化，但是最大值池化也是可以的。 PS ROI Pooling之后的feature map再做一个平均值池化，产生一个 C+1 维的向量，然后计算 softmax 回归值： 看上面的公式好像并没有做平均值处理，只是将不同网格的像素值求和。 同样的网络中也使用了回归层用于位置精修，与 $k^2(C+1)$ 个通道的score map并列，另外再使用 $4k^2$ 个通道的score map，同样是 $k^2$ 组，只不过每一组现在变成了4个通道，代表着坐标的四个值。最后PS ROI Pooling之后的feature map是 4×3×3的，平均值池化之后生成一个 4维的向量，代表着 bounding box位置的四个值 $t = (t_x,t_y,t_w,t_h)$ ,分别为中心坐标，和宽高。这里使用的是类别不明确的回归(class-agnostic bounding box regression) ，也就是对于一个ROI只输出 一个 $t$ 向量，然后与分类的结果结合。实际上也可以使用类别明确的回归(class-specific ) ，这种回归方式对一个 ROI 输出 C个 $t$ 向量，也就是说每一类别都输出一个位置向量，这跟分类时每一个类别都输出一个概率是相对的。 1.2.3 训练(Training )损失函数基本与Faster R-CNN是一样的： $c^∗$ 是RoI的 ground-truth label ($c^∗ = 0$ 意味着是背景). $L_{cls}(s_{c^∗}) = -log(s_{c^∗})$ 是用于分类的交叉熵损失； $L_{reg}$ ：是bounding box 回归损失 , $t^∗$ 代表ground truth box. $[c^∗ &gt; 0]$ 是指示函数，括号中表达式为真则为1，否则为0； 设置 $λ = 1$ ； 定义 正样本为 ROI与ground-truth box的IOU (intersection-over-union) 至少为0.5，否则为负样本。 训练方法与参数设置 It is easy for our method to adopt online hard example mining (OHEM) during training. Ournegligible per-RoI computation enables nearly cost-free example mining. We use a weight decay of 0.0005 and a momentum of 0.9. By default we use single-scale training: images are resized such that the scale (shorter side of image) is 600 pixels [6, 18]. Each GPU holds 1 image and selects B = 128 RoIs for backprop. We train the model with 8 GPUs (so the effective mini-batch size is 8×). We fine-tune R-FCN using a learning rate of 0.001 for 20k mini-batches and 0.0001 for 10k mini-batches on VOC. To have R-FCN share features with RPN (Figure 2), we adopt the 4-step alternating training, alternating between training RPN and training R-FCN. 1.2.4 推理(Inference ) an image with a single scale of 600. During inference we evaluate 300 RoIs. The results are post-processed by non-maximum suppression (NMS) using a threshold of 0.3 IoU ， as standard practice. 1.2.5 带孔卷积(Àtrous algorithme )带孔卷积：与FCN一样，本文也将网络的整体降采样步长改为了16。原先ResNet的Conv1-Conv5每一组都是2倍的降采样，总共是32倍降采样。本文中将Conv5的降采样倍数改为了1，提高了分辨率，这样整个结构是16倍的降采样，为了弥补由于降采样倍数改动导致网络后面的卷积层的感受野发生变化，文章使用了 À trous 技巧，也就是带孔卷积，不过带孔卷积只在Conv5中使用，Conv1-Conv4保持原来的不变，因此RPN不受影响。 带孔卷积的详细说明参考 1 2 . 带孔卷积对整个检测结果的影响还是蛮大的，如下是 R-FCN (k × k = 7 × 7, no hard example mining)的结果对比，带孔卷积提高了2.6个百分点。 表？ 带孔卷积对检测结果的影响 2 试验2.1 Pascal VOC试验条件： 类别C=20 训练集：VOC 2007 trainval and VOC 2012 trainval (“07+12”) 测试集：VOC 2007 test set 性能衡量指标：mean Average Precision (mAP) 首先对比一些用于目标检测的 不同全卷积网络设计策略 的结果，以下是一些 不同的全卷积设计策略： Naïve Faster R-CNN. 使用了Conv1-Conv5作为base部分，用于特征提取，ROI直接映射在Conv5最后输出的feature map上，与图2类似，不一样的地方在于：ROI池化之后加了一个 21维的全连接层。使用了 The àtrous 技巧。是一个近似的全卷积网络。 Class-specific RPN. 与标准的Faster R-CNN中使用的RPN类似，RPN训练方法也一样，不一样的在于：RPN部分不是一个二分类，而是一个类别为21的多分类。 为了对比公平，RPN的head使用的是 ResNet-101的 Conv5 层，也使用了àtrous 技巧。注意这个只是Faster R-CNN中的RPN，是一个全卷积网络。 R-FCN without position-sensitivity. 在图4的结构中设置k = 1，就跟图2是一样的，只是ROI池化的尺寸变成了 1×1 ，相当于全局池化。使用了àtrous 技巧。这是一个全卷积网络。 先展示一个 baseline的结果，如表1。这是使用 ResNet101 的标准 Faster R-CNN的测试结果。 我们只关注与本文本节实验条件相同的项，也就是红色方框框起来的那一行。可以看到mAP是 76.4%。标准的 Faster R-CNN 结构我们上面说过，首先这不是一个全卷积网络，其次 ROI pooling位于Conv4 和 Conv5之间， head 部分没有共享计算。 表1 baseline：使用 ResNet101 的标准 Faster R-CNN 以上提到的三种全卷积设计策略的结果对比如表2： 表2 不同全卷积网络设计策略结果对比 Naïve Faster R-CNN的结果最高只有 68.9%，与标准 Faster R-CNN相比，说明 将ROI pooling放置在Conv4 和 Conv5之间，是有效保证位置信息的关键。而深层的全卷积网络损失了位置信息，对位置不敏感。 Class-specific RPN的结果是67.6%，这实际上相当于特殊版本的 Fast R-CNN，相当于在head部分使用了sliding window的稠密检测方法。 R-FCN without position-sensitivity直接fail，不及格，应该是连50%都不到，因为文中提到这种情况下网络不能收敛，也就是说这种情况下，从ROI中提取不到位置信息。这个跟 Naïve Faster R-CNN 的1×1 ROI输出的版本挺像的，不一样的是 Naïve Faster R-CNN中加了全连接层，而且Naïve Faster R-CNN是可以收敛的，只是精度比标准的低很多。 R-FCN的mAP分别为 75.5和76.6，使用 7×7 ROI输出时，超过了标准的 Faster R-CNN。与R-FCN without position-sensitivity相比 ，说明Position-sensitive RoI pooling 起作用了，它可以对位置信息进行编码。 经过以上的试验分析，基本可以确定了 R-FCN with RoI output size 7 ×7 的效果是最好的。 以下的试验中涉及到 R-FCN的都采用这样的设置。 表3 是 Faster F-CNN 与 R-FCN 的 测试结果对比： 表3 Faster R-CNN与R-FCN的对比 depth of per-RoI subnetwork 指的是 head部分的深度，这里Faster F-CNN使用的是 ResNet101 版本的，Conv5有9个卷积层，再加一个全连接层，共10层。而R-FCN的head就是PS ROI Pooling和全局平均值池化，所以深度为0； online hard example mining 是否使用了OHEM策略进行训练，这个策略并不会额外增加训练时间，这个训练方式有待研究。 可以看到 R-FCN不管是训练还是测试都要比 Faster R-CNN快很多，平均精度也部落下风。 很明显的看到，当 ROI 数量是300时，Faster R-CNN训练平均一张图片需要1.5s，R-FCN需要0.45s；而当ROI 数量为300时，Faster R-CNN训练平均一张图片需要2.9s，R-FCN需要0.46s。Faster R-CNN的训练时间受 ROI是数量影响很大，而R-FCN几乎没有影响，只增加了0.01s。这也是使用全卷积，和 PS ROI Pooling 带来的好处。然而增加 ROI 数量并没有良好的精度收益。所以后面的试验中基本上都是使用的是 300个ROI。 更多的测试结果，见表4，表5. 表4 PASCAL VOC 2007测试结果对比 表5 PASCAL VOC 2012测试结果对比 multi-scale training ：resize the image in each training iteration such that the scale is randomly sampled from {400,500,600,700,800} pixels. And still test a single scale of 600 pixels, so add no test-time cost. 使用这个策略后 mAP80.5。 如果先在COCO 数据集（trainval set ）上训练，然后再 PASCAL VOC数据集上 fine-tune，可以获得 83.6%的mAP。但是精度不敌使用了各种技巧的 Faster R-CNN+++。但也不差。Faster R-CNN+++是ResNet论文中提出的。注意R-FCN并没有使用相应的技巧，比如 iterative box regression, context, multi-scale testing. Faster R-CNN+++虽然精度高，但是速度慢，简直被R-FCN 吊打，R-FCN的0.17s比Faster R-CNN+++快20倍。 表5展示了 PASCAL VOC 2012的测试结果，结论基本差不多，R-FCN精度稍逊Faster R-CNN+++，速度吊打Faster R-CNN+++。不过与 Faster R-CNN相比，精度还是高出很多的。 ResNet深度对检测结果的影响见表6： 表6 ResNet深度对检测结果的影响 从50到101，精度是上升的，但是到152就趋于饱和了。 不同的 region proposal 方法对检测精度的影响，见表7： 表7 不同的 region proposal 方法对检测精度的影响 SS: Selective Search , EB: Edge Boxes , RPN最牛。 2.2 MS COCO这一部分是 MS COCO数据集上的测试结果。 试验条件： 总类别数 C=80; 80k train set, 40k val set, and 20k test-dev set； 评价指标：AP@[0.5; 0.95]，指的是阈值在0.5到0.95之间的平均精度（average precise）；AP@0.5，指的是阈值为0.5的平均精度（average precise）。 训练方法，直接把原文的搬过来： We set the learning rate as 0.001 for 90k iterations and 0.0001 for next 30k iterations, with an effective mini-batch size of 8. We extend the alternating training [18] from 4-step to 5-step (i.e., stopping after one more RPN training step), which slightly improves accuracy on this dataset when the features are shared; we also report that 2-step training is sufficient to achieve comparably good accuracy but the features are not shared. 表8 COCO 数据集上的测试结果 multi-scale testing variant following ResNet’s Faster R-CNN, and use testing scales of {200,400,600,800,1000} 结果也是类似的 精度不敌 Faster R-CNN+++，速度吊打Faster R-CNN+++。注意R-FCN并没有使用 iterative box regression, context等技巧。 3 总结文章结合Faster R-CNN和FCN，提出了一个简单高效的网络 R-FCN，可以达到与 Faster R-CNN几乎同等的精度，而速度比Faster R-CNN快2.5-20倍。 从 R-CNN， Fast/er R-CNN 到 R-FCN，改进的路线主要就是为了实现共享计算： R-CNN ：ROI（Region of Interest）直接在原图上提取（使用 seletive search算法），每个ROI都通过 base和head进行计算。每个ROI的特征提取和最终分类都不共享计算。 Fast R-CNN ：ROI直接在原图上提取（使用 seletive search算法），将ROI映射到base部分最后一个卷积层，然后每个ROI就只通过 head 部分，head由几个全连接层构成。ROI之间的特征提取共享计算，ROI生成与base部分不共享计算，ROI通过head部分也不共享计算。 Faster R-CNN ：ROI 通过RPN（Region Proposal Network）提取，RPN与base共享特征提取层，将ROI映射到base最后一个卷积层，然后每个 ROI 只通过 head部分，head由卷积层 和\或 全连接层构成。ROI之间的特征提取共享计算，ROI 提取与base部分共享计算，ROI通过head部分不共享计算。 R-FCN：ROI通过RPN提取，提取之后的ROI仍然只通过一个网络（FCN），实现计算共享，分类层和回归层直接作用于最后一个卷积层。ROI之间的特征提取共享计算，ROI 提取与base部分共享计算，ROI通过head部分共享计算。所有层计算共享。 参考 文章链接：https://arxiv.org/abs/1605.06409 带孔卷积：http://blog.csdn.net/u012759136/article/details/52434826#t9 带孔卷积：https://www.zhihu.com/question/49630217]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-SSD-Single Shot MultiBox Detector-论文笔记]]></title>
    <url>%2Farticles%2F786f1ca3%2F</url>
    <content type="text"><![CDATA[论文地址：https://arxiv.org/abs/1512.02325源码：https://github.com/weiliu89/caffe/tree/ssd 这篇文章提出了一个新的目标检测模型SSD，这是一种 single stage 的检测模型，相比于R-CNN系列模型上要简单许多。 其精度可以与Faster R-CNN相匹敌，而速度达到了惊人的59FPS，速度上完爆 Fster R-CNN。 速度快的根本原因在于移除了 region proposals 步骤以及后续的像素采样或者特征采样步骤。（The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage.） 当然作者还是使用了比较多的trick的。 1 Model 网络模型如上图，前面是一个VGG层用于特征提取，与VGG的区别是把FC6和FC7换成了卷积层，SSD在后面又加了8个卷积层。 最终用于预测的是从这些具有金字塔结构的层中选出的特定层，这些层分别对不同scale（scale的平方是面积，这个参数是假设不考虑纵横比的时候，box的边长）和不同aspect ratios（也就是纵横比）的 bounding box进行预测。 bounding box是 detector/classifier 对 default box 进行回归生成的，而 default box 是由一定规则生成的，这里可以认为 default box 比较像 Faster R-CNN 中的RPN生成的region proposal ，也就是两步检测方案中候选框的作用。 实际上更精确的说 default box 是与 RPN 中的 anchor 机制类似的，而 anchor 实际上在 RPN 中 也就是起到了一种region proposals 的作用。 detector/classifier （图中没有显示）对这些 default box 进行关于 类别 和 位置的 回归，然后得出一个类别得分和位置坐标偏移量。根据坐标偏移量可以计算出bounding box的位置，根据得分确定此bounding box里的物体类别（每个类别都包含 8732个 bounding box，大部分都是背景 或者说共有 8732个bounding box，每个bounding box 都对一个 C 维的得分，C为类别总数）。 最后通过NMS（非最大值抑制）过滤掉背景和得分不是很高的框（这个是为了避免重复预测），得到最终的预测。 上图下半部分也展示了SSD与YOLO两种方案的不同，主要有两点： SSD是在多个feature map上进行的多尺度（multi-scale）预测（每个feature map预测一种 scale）。而YOLO是只在一个feature map上进行 多尺度预测。 两种方案的对比还可以从下面两张图中对比，这图是从FPN的论文里找的。 SSD中用于预测的层也就是 detector/classifier 是全卷积的（上图中没有显示detector，后面有图示），而YOLO中是全连接的。 YOLO SSD 上面的模型结构图可能看着不清楚，看下面这个图[参考资料2]，这个是 inference 时 模型从一张图片中提取目标的过程。 再总结一下，网络模型的主要特征： Multi-scale feature maps for detection VGG中的 conv5_3 以及VGG后面又添加的一些层中的某些层，被用来检测和分类。不同的feature layer 预测的bounding box的scale是不一样的，因此不同feature layer上的卷积模型也是不一样的（体现在参数和回归效果上）。 Convolutional predictors for detection 每一个被选中用于预测的feature layer，是用一个 3×3 的卷积层用于预测的，比如说某个feature layer的是 m×n×p 大小的，那么卷积核就是 3×3×p，这也是某一个 detector/classifier的参数量，它的输出是对应 bounding box中类别的得分或者相对于default box的坐标偏移量。对应于 feature map 上每个位置（cell），都会有 k 个 default box（下面再说怎么生成），那么无疑预测的时候要对每个位置上的每个default box都输出类别得分和坐标偏移。 Default boxes and aspect ratios 每一个被选中预测的feature layer ，其每个位置（cell）都关联k个default box，对每个default box都要输出C个类别得分和4个坐标偏移，因此每个default box有（C+4）个输出，每个位置有 (C+4)k 个输出，对于m×n 大小的feature map输出为 (C+4)kmn 个输出，这个机制跟anchor非常相似，具体可以看后面那个图。 2 Training以上是inference 时模型的结构。接下来介绍模型在训练时需要做的工作。 2.1 Matching strategy匹配策略。分类问题中是不需要匹配策略的，这里之所以要使用匹配策略是由于定位问题引入的。可以简单的认为 检测=分类＋定位，这里的定位使用的是回归方法。那么这里SSD中的回归是怎么弄的？ 我们知道平面上任意两个不重合的框都是可以通过对其中一个框进行一定的变换（比如线性变换+对数变换）使二者重合的。 在SSD中，通俗的说就是先产生一些预选的default box（类似于anchor box），然后标签是 ground truth box，预测的是bounding box，现在有三种框，从default box到ground truth有个变换关系，从default box到prediction bounding box有一个变换关系，如果这两个变换关系是相同的，那么就会导致 prediction bounding box 与 ground truth重合，如下图： 所以回归的就是这两个 变换关系： $l_{*}$ 与 $g_{*}$ ，只要二者接近，就可以使prediction bounding box接近 ground truth box 。上面的 $g_{*}$ 是只在训练的时候才有的，inference 时，就只有 $l_{*}$ 了，但是这时候的 $l_{*}$ 已经相当精确了，所以就可以产生比较准确的定位效果。 现在的问题是生成的 default box （下面讲怎么生成）是有很多的，那么势必会导致只有少部分是包含目标或者是与目标重叠关系比较大的，那当然只有这以少部分才是我们的重点观察对象，我们才能把他用到上述提到的回归过程中去。因为越靠近标签的default box回归的时候越容易，如果二者一个在最上边，一个在最下边，那么回归的时候难度会相当大，而且也会更耗时间。 确定这少部分重点观察对象的过程就是匹配策略。原文是这么所的：ground truth information needs to be assigned to specific outputs in the fixed set of detector outputs ，实际上就是确定正样本的过程，这在YOLO，Faster R-CNN ,Multi-box中也都用到了。 做法是计算default box与任意的ground truth box之间的 杰卡德系数（jaccard overlap ），其实就是IOU，只要二者之间的阈值大于0.5，就认为这是个正样本。 2.2 Training objective损失函数，同样的是multi-task loss，即包含了定位和分类。 N是匹配成功的正样本数量，如果N=0，则令 loss=0. $\alpha$ 是定位损失与分类损失之间的比重，这个值论文中提到是通过交叉验证的方式设置为 1 的。 定位损失，这与Faster R-CNN是一样的，都是用的smooth L1 loss： $l$ 代表预测bounding box与default box之间的变换关系， $g$ 代表的是ground truth box与default box之间的变换关系。 $x_{ij}^p=\{0,1\}$ 代表 第 i 个default box 与类别 p 的 第 j 个ground truth box 是否匹配，匹配为1，否则为0； 分类损失，softmax loss，交叉熵损失： 注意一点：定位损失中是只有正样本的损失的，而分类损失中是包含了正样本和负样本的。因为对于定位问题来说，只要回归出精确的变换关系，在预测的时候是不需要区分正负样本的（或者说是依靠分类来区分的），只需要将这个变换关系应用到所有的default box，最后使用NMS过滤掉得分少的样本框即可。但是分类就不一样了，分类是需要区分出正负样本的。 2.3 Choosing scales and aspect ratios for default boxes首先是 default box 的生成规则。 default box 是 bounding box 的初始参考，也就相当于region proposals，那么为了保证预测时对不同面积大小（等同于scale）和不同纵横比（aspect ratio）的目标进行预测，很明显default box也应该是具有不同scale 和 aspect ratio的。RPN中，每个feature map cell所关联的 9 种 anchor box 也是具有不同scale 和aspect ratio的，他们的作用是类似的。 在其他一些目标检测模型中比如 overfeat 和 SPPNet中，为了达到对不同 scale 和 aspect ratio的目标的预测效果，采用的方法是对原始图片进行不同程度的缩放，以达到数据扩增。这种方式可以称为 Featurized image pyramids ，如下图示，实际上每个尺度都是一个独立的网络和预测结果。 这种方式输入尺寸在变化，然而网络中的全连接层要求固定的输入，所以 overfeat 和 SPPNet 为了解决这个问题，分别使用了 offset pooling和SPP pooling的结构，使得从卷积层输入到全连接层的特征维度固定。 SSD的解决方法比较新颖： 还是看图说话，如上图，每个feature layer预测一种scale，feature layer 上每个位置（cell ）的k个default box 的纵横比（aspect ratio）是不同的，这就实现了 multi-scale 预测的方式。 下面这个图就很好的说明了这种方式，(b)和(c)代表不同的预测feature layer，其中的 default box的 scale 是不一样的，同一个 feature layer 中的同一个cell位置处的 default box的 aspect ratio也是不一样的。 一般对于不同的feature map对原图的 感受野（receptive field sizes ）大小是不一样的，然而不同 feature layer上的 default box 与其对应的感受野是不需要对应的，这个不太理解是什么意思。难道是default box的大小与感受野的大小不需要成固定比例？ （Feature maps from different levels within a network are known to have different (empirical) receptive field sizes [13]. Fortunately, within the SSD framework, the default boxes do not necessary need to correspond to the actual receptive fields of each layer. We design the tiling of default boxes so that specific feature maps learn to be responsive to particular scales of the objects. ） 不过文中还提到： An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work. 所以应该说的是 default box与感受野的对应关系。 假设模型在k个feature layer上进行预测，那么每个feature layer上的default box的scale怎么选呢？如下式： $s_{min}=0.2, s_{max}=0.9$ ，也就是说最底层的scale是0.2，最高层的feature layer的scale是0.9. 中间其它层的scale按照公式依次递增。 每个feature layer都负责对不同面积大小的目标进行预测。 但是我们知道目标不仅有大小之分，长宽比也是有变化的，即便两个目标大小是接近的，他们的长宽比（aspect ratio）也有可能是不同的，也即不可能也不应该把所有的目标都当成正方形的形状。那aspect ratio怎么确定呢？ 这里与Faster R-CNN中的解决方法是一样的：也是人为设定几种 aspect ratio，SSD中设定的几种aspect ratio分别是 $\{1,2,3，{1\over2},{1\over3}\}$ ，然后根据scale和aspect ratio 就可以计算出default box真实的宽和高了，如下两式：$$w^a_k=s_k \sqrt{a_r} \\h^a_k=s_k/\sqrt{a_r}$$这样对于feature layer上的每个cell都有5个不同比例的default box，然后作者在 aspect ratio=1 时，又添加了一种scale， 即 $s^{\prime}_{k}=\sqrt{s_k s_{k+1}}$ ,这样一来每个cell就有6个不同的default box。 那么default box的中心坐标怎么确定呢？如下：$$\left( \frac{i+0.5}{\vert f_k \vert}, \frac{j+0.5}{\vert f_k \vert} \right)$$ $|f_k|$ 是第k个square feature map 的大小 $i，j\in [0,|f_k|]$，这个公式是一个归一化坐标，最终会映射到原图上，以确定原图上对应的区域。之所以加0.5是为了把default box的中心设在与之关联的cell的中心，如下图 这样生成一系列default box之后，就有了预测bounding box的初始参考，最终的bounding box会在default box的基础上产生偏移，也就是说由于不同 scale 和 aspect ratio 的default box的存在 会使网络产生很多的预测bounding box，这些预测包含了不同尺寸和形状的目标物体，如上图，在 4×4 的feature map中只有狗（红色框）是正样本，这是因为 不同的feature map 负责预测的 scale和aspect ratio是不同的，所以在 8×8 的feature map中由于猫的scale不匹配，会被认为是负样本。同理，在 8×8 的feature map中只有猫（蓝色框）是正样本。 关于default box的设计方式，文章提到这是一个开放性的研究内容： An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work. 这应该跟上面提到的关于default box与感受野的对齐是同样的问题。 2.4 Hard negative mining由于存在大量的负样本，所以导致严重的类别不平衡问题，训练时难以收敛。 解决方法是：对负样本根据 confidence loss（也就是分类损失）排序，选取损失最高的一部分保留（也就是最不像目标的那些），使得负样本和正样本的比例保持在 3:1，文章使用这个比例获得了稳定的训练性能加快了优化速度。 2.5 Data augmentation为了使模型对于不同尺寸和纵横比的目标更加鲁棒性，使用了数据扩增。 每张训练图像随机进行如下操作： 使用原图 在原图上采样一个patch，这些patch与目标的杰卡德系数为 0.1, 0.3, 0.5, 0.7, 或者 0.9 随机采样一个patch 采样的patch与原图的比例在[0.1-0.9]之间，aspect ratio在 1/2 到 2之间。经过以上采样之后，采样的patch会被resize到固定大小并以0.5的概率进行水平翻转。 这种扩充数据集的方式增强了模型对目标平移的敏感程度。 2.6 训练流程下图是根据SSD的源码绘制的详细模型结构，caffe自带的 draw_net.py 画图工具生成的图片看起来就跟蜘蛛网一样。建议看图跟看prototxt文件相结合，更容易理解。 [参考资料1] 中还有个简化版的图，这里借鉴一下： 3 Inference下面说一下inference的流程，主要参考 [参考资料2]，有些流程在训练时也是类似的。 整体预测流程如下图，一目了然： 上面图中被红框框起来的 detector和classifier的详细结构如下图： 这是其中一个scale的预测流程 可以看出分类和定位都是通过卷积层预测的，假设某个用于预测的feature map是5×5×256的， default box=3，那么用于定位的卷积层输出是 5×5×12 的（4×3=12，每个通道代表一个位置因素（x,y,w,h）），用于分类的卷积层输出是 5×5×63的（21×3=63，每个通道代表一个类别） 每个用于预测的feature map后面可以认为接了三个并行的层，一个default box generator，一个detector，一个classifier。 上面图中的default box generator如下图： detector 和 classifier对某个 bounding box的回归： 所有的bounding box回归之后，是很乱的，然后再通过NMS过滤掉一大部分： 这只是其中某一个feature map的预测结果 当把所有用于预测的feature map进行预测之后（multi-scale），再通过一个NMS，又过滤掉一部分，保留下来的即是最终的结果 4 Experimental Results这部分不同的数据集上训练策略可能有些不同，具体的训练参数不详细说了。 4.1 PASCAL VOC 4.2 Model analysis对照试验： Data augmentation is crucial ：Fast R-CNN和Faster R-CNN使用的数据扩增方式是原图及其水平翻转。本文使用的数据扩增的方式对预测精度的提升特别明显，使用比不使用提高了8.8%。作者估计本文的数据扩增方式对 Fast R-CNN和Faster R-CNN的预测效果提升不会太多，因为Fast R-CNN和Faster R-CNN中使用的 ROI池化本来就对目标的平移很敏感，这比人为扩充数据集鲁棒性更强。 More default box shapes is better ：更多样性化的default box 显然会产生更好的结果。 Atrous is faster ：论文使用了 atrous algorithm ，这是一种称为 带孔卷积的 卷积方式[参考资料4]。 Multiple output layers at different resolutions is better ：做这个对照实验的时候，为了保证公平，删掉某一层的时候，会把这一层的default box挂载在剩下的层上面并调整scale，以保证总的default box数量不变（8732），堆叠default box时会有很多box是在图像的边缘位置，对这种box的处理方式跟Faster R-CNN的一样，就是直接忽略。 结合高层的粗粒度的feature map，比如 conv11_2(1×1)，进行预测时，效果会下降，这是显然的，网络越深，感受野越大，目标如果没有那么大的话，越到后面目标的信息越少，甚至就缺失了。 4.3 MS COCO 这篇论文对比的还是最初版本的Faster RCNN，即VGG16版的，SSD的效果还是要好于Faster RCNN的，不过后来 Faster RCNN用上了 ResNet后，SSD的精度就比不上Faster RCNN了。可能的原因是Faster R-CNN 第一步关注proposal的效果，第二步关注refine的效果, 提取的ROI区域会有一个zoom in的效果，因而会比SSD混杂在一起单步学习精度更高。而且Faster R-CNN对于小目标的预测更有优势。 4.4 Data Augmentation for Small Object Accurac没有使用特征重采样也就是在feature layer 后面再进行特征提取，这使得SSD对于小目标的预测很困难。前面提到的数据扩增的方式小目标的预测效果有很大的提升作用，特别是对于PASCAL这种小数据集来说。随机剪裁相当于对图片上某一部分进行了zoom in（放大）操作，这使得目标可以变得比较大，也会产生较多的大目标样例。但是不能完全没有小目标，为了产生一种 zoom out（缩小）的效果，文章还将原始图片放置于 16 倍原图大小的画布上，周围空间填充图片均值，然后再进行随机裁剪。使用这个方法之后就有了更多的训练数据。效果如下： 这说明这种数据扩增方式对于提升预测效果以及小目标预测效果都是很重要的技巧。 4.5 Inference time We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz Note that about 80% of the forward time is spent on the base network (VGG16 in our case). Therefore, using a faster base network could even further improve the speed, which can possibly make the SSD512 model real-time as well 。 5 可能的改进方向 An alternative way of improving SSD is to design a better tiling of default boxes sothat its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work. 没看到有针对这个做改进的。 Note that about 80% of the forward time is spent on the base network (VGG16 in our case). Therefore, using a faster base network could even further improve the speed, which can possibly make the SSD512 model real-time as well 。 比如Mobilenet-SSD，还有一些基于其他框架的轻量级检测模型：SqueezeDet，PVANet，Tiny YOLO，Tiny SSD等。 特征融合方式，现在已经很多人在做了，DSSD, HyperNet，RON, FPN等。 参考资料 SSD: Single Shot MultiBox Detector 模型fine-tune和网络架构 一个很棒的讲解SSD的PPT 一个汇集了很多目标检测模型的博客 知乎：atrous convolution是什么？]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测评价标准-AP mAP]]></title>
    <url>%2Farticles%2Fc521a01c%2F</url>
    <content type="text"><![CDATA[1 mAP简介目标检测里面经常用到的评估标准是：mAP（mean average precision），计算mAP需要涉及到precision 和 recall的计算，mAP，precision，recall的定义含义以及计算方式，网上很多博客都有说明，本文不打算重述。 阅读本文之前，请先仔细阅读如下资料： 周志华老师 《机器学习》 模型评估标准一节，主要是precision，recall的计算方式，或者自己网上搜博客 多标签图像分类任务的评价方法-mAP 通过一个简单的二分类阐述 mAP的含义与计算 average precision 几种不同形式 AP 的计算方式与异同 以博客 多标签图像分类任务的评价方法-mAP 中的数据为例，下面是这个二分类问题的P-R曲线（precision-recall curve），P-R曲线下面与x轴围成的面积称为 average precision。 那么问题就在于如何计算AP，这里很显然可以通过积分来计算$$AP=\int_0^1 P(r) dr$$但通常情况下都是使用估算或者插值的方式计算： approximated average precision$$AP=\sum_{k=1}^N P(k) \Delta r(k)$$ 这个计算方式称为 approximated 形式的，插值计算的方式里面这个是最精确的，每个样本点都参与了计算 很显然位于一条竖直线上的点对计算AP没有贡献 这里N为数据总量，k为每个样本点的索引， $\Delta r(k)=r(k)-r(k-1)$ Interpolated average precision 这是一种插值计算方式：$$P_{interp}(k)=max_{\hat k \ge k} P(\hat k)$$ $$\sum_{k=1}^N P_{interp(k)} \Delta r(k)$$ k 为每一个样本点的索引，参与计算的是所有样本点 $P_{interp}(k)$ 取第 k 个样本点之后的样本中的最大值 这种方式不常用，所以不画图了 插值方式进一步演变：$$P_{interp}(k)=max_{\hat k \ge k} P(\hat k)$$ $$\sum_{k=1}^K P_{interp}(k) \Delta r(k)$$ 这是通常意义上的 Interpolated 形式的 AP，这种形式使用的是比较多的，因为这个式子跟上面提到的计算方式在最终的计算结果上来说是一样的，上面那个式子的曲线跟这里图中阴影部分的外部轮廓是一样的 当一组数据中的正样本有K个时，那么recall的阈值也有K个，k代表阈值索引，参与计算的只有K个阈值所对应的样本点 $P_{interp}(k)$ 取第 k 个阈值所对应的样本点之后的样本中的最大值 再进一步演变：$$P_{interp}(k)=max_{r(\hat k) \ge R(k)} P(\hat k) \quad R \in \{0,0.1,0.2,…,1.0\}$$ $$\sum_{k=1}^K P_{interp}(k) \Delta r(k)$$ 这是通常意义上的 11points_Interpolated 形式的 AP，选取固定的 $\{0,0.1,0.2,…,1.0\}$ 11个阈值，这个在PASCAL2007中有使用 这里因为参与计算的只有11个点，所以 K=11，称为11points_Interpolated，k为阈值索引 $P_{interp}(k)$ 取第 k 个阈值所对应的样本点之后的样本中的最大值，只不过这里的阈值被限定在了 $\{0,0.1,0.2,…,1.0\}$ 范围内。 从曲线上看，真实 AP&lt; approximated AP &lt; Interpolated AP 11-points Interpolated AP 可能大也可能小，当数据量很多的时候会接近于 Interpolated AP 前面的公式中计算AP时都是对PR曲线的面积估计，然后我看到PASCAL的论文里给出的公式就更加简单粗暴了，如下：$$AP=\frac{1}{11} \sum_{r \in \{ 0,0.1,0.2,…,1.0 \}} P_{intep} (r)$$ $$P_{interp}(r)=MAX_{\hat r: \hat r\ge r} P(\hat r)$$ 直接计算11个阈值处的precision的平均值。 不过我看 Itroduction to Modern Information（中译本：王斌《信息检索导论》）一书中也是直接计算平均值的。 对于Interpolated 形式的 AP，因为recall的阈值变化是等差的（或者recall轴是等分的），所以计算面积和直接计算平均值结果是一样的， 对于11points_Interpolated 来说，虽然recall的阈值也是等差的，但是11points计算平均值时会把recall=0那一点的precision算进去，但实际上那一点是人为添加的，所以计算面积和直接计算平均值会有略微差异。 实际上这是一个极限问题，如果recall轴等分且不考虑初始点，那么计算面积和均值的结果是一样的；如果不等分，只有当分割recall无限多的时候，二者无限趋近，这是一个数学问题。 第 4 节的代码包含这两种计算方式，可以用来验证。 以上几种不同形式的 AP 在第4节会有简单的代码实现。 2 PASCAL数据集mAP计算方式 一定要先看这个博客 多标签图像分类任务的评价方法-mAP 。 PASCAL VOC最终的检测结构是如下这种格式的： 比如comp3_det_test_car.txt: 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 每一行依次为 ： 1&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt; 每一行都是一个bounding box，后面四个数定义了检测出的bounding box的左上角点和右下角点的坐标。 在计算mAP时，如果按照二分类问题理解，那么每一行都应该对应一个标签，这个标签可以通过ground truth计算出来。 但是如果严格按照 ground truth 的坐标来判断这个bounding box是否正确，那么这个标准就太严格了，因为这是属于像素级别的检测，所以PASCAL中规定当一个bounding box与ground truth的 IOU&gt;0.5 时就认为这个框是正样本，标记为1；否则标记为0。这样一来每个bounding box都有个得分，也有一个标签，这时你可以认为前面的文件是这样的，后面多了一个标签项： 12345000004 0.702732 89 112 516 466 1000006 0.870849 373 168 488 229 0000006 0.852346 407 157 500 213 1000006 0.914587 2 161 55 221 0000008 0.532489 175 184 232 201 1 进而你可以认为是这样的，后面的标签实际上是通过坐标计算出来的 12345000004 0.702732 1000006 0.870849 0000006 0.852346 1000006 0.914587 0000008 0.532489 1 这样一来就可以根据前面博客中的二分类方法计算AP了。但这是某一个类别的，将所有类别的都计算出来，再做平均即可得到mAP. 3 COCO数据集AP计算方式COCO数据集里的评估标准比PASCAL 严格许多 COCO检测出的结果是json文件格式，比如下面的： 123456789101112131415[ &#123; "image_id": 139, "category_id": 1, "bbox": [ 431.23001, 164.85001, 42.580002, 124.79 ], "score": 0.16355941 &#125;, …… ……] 我们还是按照前面的形式来便于理解： 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 前面提到可以使用IOU来计算出一个标签，PASCAL用的是 IOU&gt;0.5即认为是正样本，但是COCO要求IOU阈值在[0.5, 0.95]区间内每隔0.05取一次，这样就可以计算出10个类似于PASCAL的mAP，然后这10个还要再做平均，即为最后的AP，COCO中并不将AP与mAP做区分，许多论文中的写法是 AP@[0.5:0.95]。而COCO中的 AP@0.5 与PASCAL 中的mAP是一样的。 4 代码简单实现 一定要先看这个博客 多标签图像分类任务的评价方法-mAP 。 计算AP的代码上，我觉得可以看看sklearn关于AP计算的源码，必要时可以逐步调试以加深理解。 sklearn的average_precision_score API，average_precision_score 源码 sklearn上的一个计算AP的例子 Precision-Recall and average precision compute 另外PASCAL和COCO都有公开的代码用于评估标准的计算 PASCAL development kit code and documentation COCO github 下面是仿照sklearn上的计算AP的例子写的一个简单的代码，与sklearn略有差异并做了一些扩展，这个代码可以计算 approximated，interpolated，11point_interpolated形式的AP，sklearn的API只能计算approximated形式的AP。这几个形式的AP的差异，参考 average precision 这个博客。PASCAL2007的测量标准用的 11point_interpolated形式，而 PASCAL2010往后使用的是 interpolated 形式的。 从计算的精确度上 approximated &gt; interpolated &gt; 11point_interpolated，当然最精确的是积分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248# coding=utf-8from sklearn import svm, datasetsfrom sklearn.model_selection import train_test_splitimport matplotlib.pyplot as pltimport numpy as npdef preprocess_iris_data(): iris = datasets.load_iris() X = iris.data y = iris.target # Add noisy features random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)] # 鸢尾花数据集有三类，取其中两类 # Limit to the two first classes, and split into training and test X_train, X_test, y_train, y_test = train_test_split(X[y &lt; 2], y[y &lt; 2], test_size=.5, random_state=random_state) # Create a simple classifier classifier = svm.LinearSVC(random_state=random_state) classifier.fit(X_train, y_train) y_score = classifier.decision_function(X_test) return y_test, y_scoredef precision_recall_curve(y_true, y_score, pos_label=None): if pos_label is None: pos_label = 1 # 不同的排序方式，其结果也会有略微差别， # 比如 kind="mergesort" 的结果跟kind="quicksort"的结果是不同的， # 这是因为归并排序是稳定的，快速排序是不稳定的，sklearn中使用的是 kind="mergesort" desc_score_indices = np.argsort(y_score, kind="quicksort")[::-1] y_score = y_score[desc_score_indices] y_true = y_true[desc_score_indices] # 确定阈值下标索引，score中可能会有重复的分数，在sklearn中，重复的元素被去掉了 # 本来以为去除重复分数会影响结果呢，因为如果两个样本有重复的分数，一个标签是1，一个是0， # 感觉去掉哪个都会影响结果啊，但是实验发现竟然不影响结果，有点纳闷，以后有时间再分析吧 # distinct_value_indices = np.where(np.diff(y_score))[0] # threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1] # 这里没有去除重复分数 threshold_idxs = np.arange(y_score.size) # 按照阈值依次降低的顺序，确定当前阈值下的true positives 个数，tps[-1]对应于所有的正例数量 tps = np.cumsum(y_true * 1.)[threshold_idxs] # 计算当前阈值下的 false positives 个数， # 它与 tps的关系为fps=1+threshold_idxs-tps，这个关系是比较明显的 fps = 1 + threshold_idxs - tps # y_score[threshold_idxs]把对应下标的阈值取出 thresholds = y_score[threshold_idxs] precision = tps / (tps + fps) recall = tps / tps[-1] # 这里与sklearn有略微不同，即样本点全部输出，令last_ind = tps.size，即可 last_ind = tps.size sl = slice(0, last_ind) return np.r_[1, precision[sl]], np.r_[0, recall[sl]], thresholds[sl]def average_precision_approximated(y_true, y_predict): """ 计算approximated形式的ap，每个样本点的分数都是recall的一个cutoff :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) average_precision = np.sum(np.diff(recall) * np.array(precision)[1:]) return precision, recall, thresholds, average_precisiondef average_precision_interpolated(y_true, y_predict): """ 计算interpolated形式的ap，每个正样本对应的分数都是recalll的一个cutoff :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) # 获取recall轴上的分割，np.insert(recall, 0 , -1, axis=0)是为了保证获取到重复recall的第一个索引值 # 因为重复的recall中只有对应的第一个precision是最大的，我们只需要获取这个最大的precision # 或者说每遇到一个正样本，需要将其对应的recall值作为横轴的切分 recall_cutoff_index = np.where( np.diff(np.insert(recall, 0, -1, axis=0)))[0] # 从recall的cutoff 索引值开始往后获取precision最大值，相同的precision只取索引值最大的那个 # P(r) = max&#123;P(r')&#125; | r'&gt;=r precision_cutoff_index = [] for index in recall_cutoff_index: precision_cutoff_index.append( max([x for x in np.where(precision == np.max(precision[index:]))[0] if x &gt;= index])) # interpolated_idx=np.unique(interpolated_cutoff) # 从原始的precision和recall中截取对应索引的片段，即可得到 interpolated 方式下的precision，recall以及AP precision_interpolated = precision[precision_cutoff_index] recall_interpolated = recall[recall_cutoff_index] # 以上获得的 recall_cutoff_index 和 precision_cutoff_index 切片包含人为添加的0 和 1（为了画图时与坐标轴封闭） # 而计算thresholds_interpolated时要去掉相应索引值的影响 # 阈值不包括recall=0 thresholds_interpolated = thresholds[ [x - 1 for x in recall_cutoff_index if 0 &lt;= x - 1 &lt; thresholds.size]] # 按说ap计算应该按照面积的方式计算，也就是下面被注释的部分，但论文里面是直接计算均值， # 这里也直接计算均值，因为阈值不包括recall=0，所以这种情况下二者结果是一样的 average_precision = np.mean(precision_interpolated[1:]) # average_precision = np.sum( # np.diff(recall_interpolated) * np.array(precision_interpolated)[1:]) return precision_interpolated, recall_interpolated, thresholds_interpolated, average_precisiondef average_precision_11point_interpolated(y_true, y_predict): """ 计算 11point形式的 ap :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) recall_11point_cutoff = np.array( [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) # 计算离11个cutoff最近的样本点 recall_cutoff_index = [] for cutoff in recall_11point_cutoff: recall_cutoff_index.append(np.where(recall &gt;= cutoff)[0][0]) precision_cutoff_index = [] for index in recall_cutoff_index: precision_cutoff_index.append( max([x for x in np.where(precision == np.max(precision[index:]))[0] if x &gt;= index])) precision_11point = precision[precision_cutoff_index] recall_11point = recall[recall_cutoff_index] # 此处阈值包括recall=0，因为是11points thresholds_11point = thresholds[ [x - 1 for x in recall_cutoff_index if -1 &lt;= x - 1 &lt; thresholds.size]] # 此处阈值包括recall=0，因为是11points，所以这种情况下两种计算AP的方式结果不同，有略微差别 average_precision = np.mean(precision_11point) # average_precision = np.sum( # 0.1 * np.array(precision_11point)[1:]) # 此处直接返回 recall_11point_cutoff，实际上返回 recall_11point 也是可以的， # 差别就是图线的转折点不在[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]这11个刻度处 # return precision_11point, recall_11point, thresholds_11point, average_precision return precision_11point, recall_11point_cutoff, thresholds_11point, average_precisiondef main(data=None): y_test = [] y_score = [] if data is None: # 一个简单的示例，这个示例直接给定了y_test和y_score y_test = np.array([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]) y_score = np.array([0.23, 0.76, 0.01, 0.91, 0.13, 0.45, 0.12, 0.03, 0.38, 0.11, 0.03, 0.09, 0.65, 0.07, 0.12, 0.24, 0.1, 0.23, 0.46, 0.08]) if data == 'iris': # 还可以导入鸢尾花数据集并构建一个简单的SVM分类器，通过一个完整的模型来理解 PR曲线的绘制 # 使用鸢尾花数据集 y_test, y_score = preprocess_iris_data() # 计算AP，并画图 precision_approximated, recall_approximated, _, ap_approximated = \ average_precision_approximated(y_test, y_score) precision_interpolated, recall_interpolated, _, ap_interpolated = \ average_precision_interpolated(y_test, y_score) precision_11point, recall_11point, _, ap_11point = \ average_precision_11point_interpolated(y_test, y_score) print('Approximated average precision-recall score: &#123;0:0.5f&#125;'.format( ap_approximated)) print('Interpolated average precision-recall score: &#123;0:0.5f&#125;'.format( ap_interpolated)) print('Interpolated at fixed 11 points average precision-recall score: &#123;0:0.5f&#125;'.format( ap_11point)) # print the AP plot fig1 = plt.figure('fig1') # plt.subplot(311) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.step(recall_approximated, precision_approximated, color='c', where='pre') plt.fill_between(recall_approximated, precision_approximated, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Approximated): AP=&#123;0:0.5f&#125;'.format( ap_approximated)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', 'Approximated-PR-curve', 'Approximated-AP'), loc='upper right') fig2 = plt.figure('fig2') # plt.subplot(312) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.plot(recall_interpolated, precision_interpolated, color='c', marker='o', mec='g', ms=3, alpha=0.5) plt.fill_between(recall_interpolated, precision_interpolated, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Interpolated): AP=&#123;0:0.5f&#125;'.format( ap_interpolated)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', 'Interpolated-PR-curve', 'Interpolated-AP'), loc='upper right') fig3 = plt.figure('fig3') # plt.subplot(313) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.plot(recall_11point, precision_11point, color='c', marker='o', mec='g', ms=3) plt.fill_between(recall_11point, precision_11point, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Interpolated_11point): AP=&#123;0:0.5f&#125;'.format( ap_11point)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', '11point-PR-curve', '11point-AP'), loc='upper right') # plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5, # wspace=0.35) plt.show()if __name__ == '__main__': # main() # 用这个测试博客 “多标签图像分类任务的评价方法-mAP” 中的简单例程 main('iris') 这个例子使用了 鸢尾花数据集中的两类构建了一个SVM分类器，然后对分类结果计算AP. 最终的结果如下所示： 参考资料 周志华老师 《机器学习》 多标签图像分类任务的评价方法-mAP average precision 王斌译 《信息检索导论》 论文： The PASCAL Visual Object Classes (VOC) Challenge 2007 和 2012 http://cocodataset.org/#detection-eval http://host.robots.ox.ac.uk/pascal/VOC/ http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>评价标准</tag>
        <tag>mAP</tag>
        <tag>wheel making</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测数据集MSCOCO简介]]></title>
    <url>%2Farticles%2Fe5b86f16%2F</url>
    <content type="text"><![CDATA[简介介绍一下目标检测领域另外一个比较有名的数据集 MS COCO (Microsoft COCO: Common Objects in Context) . MSCOCO 数据集是微软构建的一个数据集，其包含 detection, segmentation, keypoints等任务。 MSCOCO主要是为了解决detecting non-iconic views of objects（对应常说的detection）, contextual reasoning between objects and the precise 2D localization of objects（对应常说的分割问题） 这三种场景下的问题。 下面是iconic 图片和 non-iconic 图片之间的对比。 与PASCAL COCO数据集相比，COCO中的图片包含了自然图片以及生活中常见的目标图片，背景比较复杂，目标数量比较多，目标尺寸更小，因此COCO数据集上的任务更难，对于检测任务来说，现在衡量一个模型好坏的标准更加倾向于使用COCO数据集上的检测结果。 数据集的构建过程不说了。主要关注一下统计信息 1 统计信息MSCOCO总共包含91个类别，每个类别的图片数量如下： 图中也标出了PASCAL VOC的统计数据作为对比。 下图展示的是几个不同数据集的总类别数量，以及每个类别的总实例数量，一个实例就是图片上的一个目标，主要关注一下 PASCAL 和 ImageNet。 COCO数据集的类别总数虽然没有 ImageNet 中用于detection的类别总数多，但是每个类别的实例目标总数要比PASCAL和ImageNet都要多。 下图是每张图片上的类别数量或者实例数量的分布，括号中为平均值 PASCAL和ImageNet中，每张图片上的类别或者实例数量普遍都很少。 以PASCAL为例：有多于70%的图片上都只有一个类别，而多于50%的图片上只有一个实例或者目标。PASCAL数据集平均每张图片包含1.4个类别和2.3个实例目标，ImageNet也仅有1.7和3.0个。 COCO数据集平均每张图片包含 3.5个类别和 7.7 个实例目标，仅有不到20%的图片只包含一个类别，仅有10%的图片包含一个实例目标。 COCO数据集不仅数据量大，种类和实例数量也多。从这角度来说 SUN 数据集这两个指标更高一点，但是这个数据集在目标检测里面并不常用。 实例目标的分布 COCO数据集中的小目标数量占比更多 关于数据集的划分，COCO的论文里是这么说的， The 2014 release contains 82,783 training, 40,504 validation, and 40,775 testing images (approximately 1/2 train, 1/4 val, and /4 test). There are nearly 270k segmented people and a total of 886k segmented object instances in the 2014 train+val data alone. The cumulative 2015 release will contain a total of 165,482 train, 81,208 val, and 81,434 test images. 2014年的数据在官网是可以下载的，但是2015年只有test部分，train和val部分的数据没有。另外2017年的数据并没有什么新的图片，只是将数据重新划分，train的数据更多了，如下： 2 评估标准COCO的测试标准比PASCAL VOC更严格： PASCAL 中在测试mAP时，是在IOU=0.5时测的 COCO中的AP 是指在 10个IOU层面 以及 80个类别层面 的平均值 COCO的主要评价指标是AP，指 IOU从0.5到0.95 每变化 0.05 就测试一次 AP，然后求这10次测量结果的平均值作为最终的 AP AP@0.5 跟PASCAL VOC中的mAP是相同的含义 AP@0.75 跟PASCAL VOC中的mAP也相同，只是IOU阈值提高到了0.75，显然这个层面更严格，精度也会更低 IOU越高，AP就越低，所以最终的平均之后的AP要比 AP@0.5 小很多，这也就是为什么COCO的AP 超过 50%的只有寥寥几个而已，因为超过50%太难了。而且由于COCO数据集本身数据的复杂性，所以目前的 AP@0.5 最高也只有 73% 。 COCO数据集还针对 三种不同大小（small，medium，large） 的图片提出了测量标准，COCO中包含大约 41% 的小目标 ($area &lt; 32×32$), 34% 的中等目标 ($32×32 &lt; area &lt; 96×96$), 和 24% 的大目标 ($area &gt; 96×96$). 小目标的AP是很难提升的。 除了AP之外，还提出了 AR 的测量标准 跟AP是类似的。 COCO提供了一些代码，方便对数据集的使用和模型评估 ：https://github.com/cocodataset/cocoapi 3 总结为什么COCO的检测任务那么难？ 图片大多数来源于生活中，背景更复杂 每张图片上的实例目标个数多，平均每张图片7.7个 小目标更多 评估标准更严格 所以现在大家更倾向于使用COCO来评估模型的质量。 参考资料 COCO官网：http://cocodataset.org 论文：Microsoft COCO: Common Objects in Context 论文：What makes for effective detection proposals?]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测数据集PASCAL VOC简介]]></title>
    <url>%2Farticles%2F1dc20586%2F</url>
    <content type="text"><![CDATA[简介PASCAL VOC挑战赛 （The PASCAL Visual Object Classes ）是一个世界级的计算机视觉挑战赛, PASCAL全称：Pattern Analysis, Statical Modeling and Computational Learning，是一个由欧盟资助的网络组织。 很多优秀的计算机视觉模型比如分类，定位，检测，分割，动作识别等模型都是基于PASCAL VOC挑战赛及其数据集上推出的，尤其是一些目标检测模型（比如大名鼎鼎的R CNN系列，以及后面的YOLO，SSD等）。 PASCAL VOC从2005年开始举办挑战赛，每年的内容都有所不同，从最开始的分类，到后面逐渐增加检测，分割，人体布局，动作识别（Object Classification 、Object Detection、Object Segmentation、Human Layout、Action Classification）等内容，数据集的容量以及种类也在不断的增加和改善。该项挑战赛催生出了一大批优秀的计算机视觉模型（尤其是以深度学习技术为主的）。 我们知道在 ImageNet挑战赛上涌现了一大批优秀的分类模型，而PASCAL挑战赛上则是涌现了一大批优秀的目标检测和分割模型，这项挑战赛已于2012年停止举办了，但是研究者仍然可以在其服务器上提交预测结果以评估模型的性能。 虽然近期的目标检测或分割模型更倾向于使用MS COCO数据集，但是这丝毫不影响 PASCAL VOC数据集的重要性，毕竟PASCAL对于目标检测或分割类型来说属于先驱者的地位。对于现在的研究者来说比较重要的两个年份的数据集是 PASCAL VOC 2007 与 PASCAL VOC 2012，这两个数据集频频在现在的一些检测或分割类的论文当中出现。 PASCAL主页 与 排行榜 （榜上已几乎看不到传统的视觉模型了，全是基于深度学习的） PASCAL VOC 2007 挑战赛主页 与 PASCAL VOC 2012 挑战赛主页 与 PASCAL VOC Evaluation Server. 以及在两个重要时间点对 PASCAL VOC挑战赛 成绩进行总结的两篇论文 The PASCAL Visual Object Classes Challenge: A Retrospective Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.International Journal of Computer Vision, 111(1), 98-136, 2015Bibtex source | Abstract | PDF 主要总结PASCAL VOC 2012的数据集情况，以及2011年-2013年之间出现的模型及其性能对比 The PASCAL Visual Object Classes (VOC) ChallengeEveringham, M., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.International Journal of Computer Vision, 88(2), 303-338, 2010Bibtex source | Abstract | PDF 主要总结PASCAL VOC 2007的数据集情况，以及2008年之前出现的模型及其性能对比 不过在以上论文中出现的深度学习模型只有一个 R-CNN吧，大部分都是传统方式的模型，毕竟深度学习模型主要在14年以后才大量涌现。 本文也是以PASCAL VOC 2007 和 2012 为例简要介绍VOC数据集的结构。 1 数据集整体概况1.1 层级结构PASCAL VOC 数据集的20个类别及其层级结构： 从2007年开始，PASCAL VOC每年的数据集都是这个层级结构 总共四个大类：vehicle,household,animal,person 总共20个小类，预测的时候是只输出图中黑色粗体的类别 数据集主要关注分类和检测，也就是分类和检测用到的数据集相对规模较大。关于其他任务比如分割，动作识别等，其数据集一般是分类和检测数据集的子集。 1.2 发展历程与使用方法简要提一下在几个关键时间点数据集的一些关键变化，详细的请查看PASCAL VOC主页 。 2005年：还只有4个类别： bicycles, cars, motorbikes, people. Train/validation/test共有图片1578 张，包含2209 个已标注的目标objects. 2007年 ：在这一年PASCAL VOC初步建立成一个完善的数据集。类别扩充到20类，Train/validation/test共有9963张图片，包含24640 个已标注的目标objects. 07年之前的数据集中test部分都是公布的，但是之后的都没有公布。 2009年：从这一年开始，通过在前一年的数据集基础上增加新数据的方式来扩充数据集。比如09年的数据集是包含了08年的数据集的，也就是说08年的数据集是09年的一个子集，以后每年都是这样的扩充方式，直到2012年；09年之前虽然每年的数据集都在变大（08年比07年略少），但是每年的数据集都是不一样的，也就是说每年的数据集都是互斥的，没有重叠的图片。 2012年：从09年到11年，数据量仍然通过上述方式不断增长，11年到12年，用于分类、检测和person layout 任务的数据量没有改变。主要是针对分割和动作识别，完善相应的数据子集以及标注信息。 对于分类和检测来说，也就是下图所示的发展历程，相同颜色的代表相同的数据集： 分割任务的数据集变化略有不同： VOC 2012用于分类和检测的数据包含 2008-2011年间的所有数据，并与VOC2007互斥。 VOC 2012用于分割的数据中train+val包含 2007-2011年间的所有数据，test包含2008-2011年间的数据，没有包含07年的是因为07年的test数据已经公开了。 2012年是最后一次挑战赛，最终用于分类和检测的数据集规模为：train/val ：11540 张图片，包含 27450 个已被标注的 ROI annotated objects ；用于分割的数据集规模为：trainval：2913张图片，6929个分割，用于其他任务的不再细说，参考这里 。 即便挑战赛结束了，但是研究者们仍然可以上传预测结果进行评估。上传入口： PASCAL VOC Evaluation Server. 目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。 论文中针对 VOC2007和VOC2012 的具体用法有以下几种： 只用VOC2007的trainval 训练，使用VOC2007的test测试 只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用 使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12 ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。 使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 07++12 ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12+COCO 。 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 07++12+COCO，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。 在各自数据集上分别进行建模和评测的用法比较少，基本上在早期论文里出现就是起个对照作用；现在的大部分论文都会为了增加数据量而将二者合起来使用。 2 数据量统计由于现在的研究基本上都是在VOC2007和VOC2012上面进行，因此只介绍这两个年份的。 2.1 VOC 2007一些示例图片展示：Classification/detection example images 数据集总体统计： 以上是数据集总体的统计情况，这个里面是包含了测试集的，可见person 类是最多的。 训练集，验证集，测试集划分情况 PASCAL VOC 2007 数据集分为两部分：训练和验证集trainval，测试集test ，两部分各占数据总量的约 50%。其中trainval 又分为训练集和测试集，二者分别各占trainval的50%。 每张图片中有可能包含不只一个目标object。 这里我就只贴出用于分类和检测的划分情况，关于分割或者其他任务的划分方式 点击这里查看 。 2.2 VOC 2012一些示例图片展示：Classification/detection example images 数据集总体统计 这个统计是没有包含 test部分的，仍然是person类最多 trainval部分的数据统计： test部分没有公布，同样的 除了分类和检测之外的数据统计，参考这里 2.3 VOC 2007 与 2012 的对比VOC 2007 与 2012 数据集及二者的并集 数据量对比 黑色字体所示数字是官方给定的，由于VOC2012数据集中 test 部分没有公布，因此红色字体所示数字为估计数据，按照PASCAL 通常的划分方法，即 trainval 与test 各占总数据量的一半。 3 标注信息数据集的标注还是很谨慎的，有专门的标注团队，并遵从统一的标注标准，参考 guidelines 。 标注信息是用 xml 文件组织的如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;annotation&gt; &lt;folder&gt;VOC2007&lt;/folder&gt; &lt;filename&gt;000001.jpg&lt;/filename&gt; &lt;source&gt; &lt;database&gt;The VOC2007 Database&lt;/database&gt; &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt; &lt;image&gt;flickr&lt;/image&gt; &lt;flickrid&gt;341012865&lt;/flickrid&gt; &lt;/source&gt; &lt;owner&gt; &lt;flickrid&gt;Fried Camels&lt;/flickrid&gt; &lt;name&gt;Jinky the Fruit Bat&lt;/name&gt; &lt;/owner&gt; &lt;size&gt; &lt;width&gt;353&lt;/width&gt; &lt;height&gt;500&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;dog&lt;/name&gt; &lt;pose&gt;Left&lt;/pose&gt; &lt;truncated&gt;1&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;48&lt;/xmin&gt; &lt;ymin&gt;240&lt;/ymin&gt; &lt;xmax&gt;195&lt;/xmax&gt; &lt;ymax&gt;371&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;person&lt;/name&gt; &lt;pose&gt;Left&lt;/pose&gt; &lt;truncated&gt;1&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;8&lt;/xmin&gt; &lt;ymin&gt;12&lt;/ymin&gt; &lt;xmax&gt;352&lt;/xmax&gt; &lt;ymax&gt;498&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; filename ：文件名 source，owner：图片来源，及拥有者 size：图片大小 segmented：是否分割 object：表明这是一个目标，里面的内容是目标的相关信息 name：object名称，20个类别 pose：拍摄角度：front, rear, left, right, unspecified truncated：目标是否被截断（比如在图片之外），或者被遮挡（超过15%） difficult：检测难易程度，这个主要是根据目标的大小，光照变化，图片质量来判断 difficult 标签示例：图中白色虚线，被标记为 difficult。 bndbox：bounding box 的左上角点和右下角点的4个坐标值。 4 提交格式4.1 Classification Task每一类都有一个txt文件，里面每一行都是测试集中的一张图片，前面一列是图片名称，后面一列是预测的分数。 comp1_cls_test_car.txt: 12345000004 0.702732000006 0.870849000008 0.532489000018 0.477167000019 0.112426 4.2 Detection Taskcomp3_det_test_car.txt: 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 每一类都有一个txt文件，里面每一行都是测试集中的一张图片，每行的格式按照如下方式组织 1&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt; confidence 用来计算 mAP. 5 评估标准PASCAL的评估标准是 mAP(mean average precision) 关于mAP不再详细解释，参考以下资料： 性能指标（模型评估）之mAP average precision 周志华老师 《机器学习》 模型评估标准一节 这里简单的提一下： 下面是一个二分类的P-R曲线（precision-recall curve），对于PASCAL来说，每一类都有一个这样的 P-R曲线，P-R曲线下面与x轴围成的面积称为 average precision，每个类别都有一个 AP, 20个类别的AP 取平均值就是 mAP。 PASCAL官方给了评估脚本mAP的脚本和示例代码 development kit code and documentation ，是用MATLAB写的。 6 数据集组织结构数据集的下载: 123456789# Download the data.cd $HOME/datawget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar# Extract the data.tar -xvf VOCtrainval_11-May-2012.tartar -xvf VOCtrainval_06-Nov-2007.tartar -xvf VOCtest_06-Nov-2007.tar 或者支直接点击下面链接下载： Download the training/validation data (450MB tar file) Download the annotated test data (430MB tar file) 上面的解压命令会将VOC2007的trainval和test解压到一块，数据会混在一起，如果不想，可以自己指定解压路径。以VOC 2007 为例，解压后的文件： 123456.├── Annotations 进行detection 任务时的 标签文件，xml文件形式├── ImageSets 存放数据集的分割文件，比如train，val，test├── JPEGImages 存放 .jpg格式的图片文件├── SegmentationClass 存放 按照class 分割的图片└── SegmentationObject 存放 按照 object 分割的图片 Annotations 文件夹： 12345678910.├── 000001.xml├── 000002.xml├── 000003.xml├── 000004.xml………………├── 009962.xml└── 009963.xml 以xml 文件的形式，存放标签文件，文件内容如前述，文件名与图片名是一样的，6位整数 ImageSets文件夹： 存放数据集的分割文件 包含三个子文件夹 Layout，Main，Segmentation，其中Main文件夹存放的是用于分类和检测的数据集分割文件，Layout文件夹用于 person layout任务，Segmentation用于分割任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798.├── Layout│ ├── test.txt│ ├── train.txt│ ├── trainval.txt│ └── val.txt├── Main│ ├── aeroplane_test.txt│ ├── aeroplane_train.txt│ ├── aeroplane_trainval.txt│ ├── aeroplane_val.txt│ ├── bicycle_test.txt│ ├── bicycle_train.txt│ ├── bicycle_trainval.txt│ ├── bicycle_val.txt│ ├── bird_test.txt│ ├── bird_train.txt│ ├── bird_trainval.txt│ ├── bird_val.txt│ ├── boat_test.txt│ ├── boat_train.txt│ ├── boat_trainval.txt│ ├── boat_val.txt│ ├── bottle_test.txt│ ├── bottle_train.txt│ ├── bottle_trainval.txt│ ├── bottle_val.txt│ ├── bus_test.txt│ ├── bus_train.txt│ ├── bus_trainval.txt│ ├── bus_val.txt│ ├── car_test.txt│ ├── car_train.txt│ ├── car_trainval.txt│ ├── car_val.txt│ ├── cat_test.txt│ ├── cat_train.txt│ ├── cat_trainval.txt│ ├── cat_val.txt│ ├── chair_test.txt│ ├── chair_train.txt│ ├── chair_trainval.txt│ ├── chair_val.txt│ ├── cow_test.txt│ ├── cow_train.txt│ ├── cow_trainval.txt│ ├── cow_val.txt│ ├── diningtable_test.txt│ ├── diningtable_train.txt│ ├── diningtable_trainval.txt│ ├── diningtable_val.txt│ ├── dog_test.txt│ ├── dog_train.txt│ ├── dog_trainval.txt│ ├── dog_val.txt│ ├── horse_test.txt│ ├── horse_train.txt│ ├── horse_trainval.txt│ ├── horse_val.txt│ ├── motorbike_test.txt│ ├── motorbike_train.txt│ ├── motorbike_trainval.txt│ ├── motorbike_val.txt│ ├── person_test.txt│ ├── person_train.txt│ ├── person_trainval.txt│ ├── person_val.txt│ ├── pottedplant_test.txt│ ├── pottedplant_train.txt│ ├── pottedplant_trainval.txt│ ├── pottedplant_val.txt│ ├── sheep_test.txt│ ├── sheep_train.txt│ ├── sheep_trainval.txt│ ├── sheep_val.txt│ ├── sofa_test.txt│ ├── sofa_train.txt│ ├── sofa_trainval.txt│ ├── sofa_val.txt│ ├── test.txt│ ├── train_test.txt│ ├── train_train.txt│ ├── train_trainval.txt│ ├── train.txt│ ├── train_val.txt│ ├── trainval.txt│ ├── tvmonitor_test.txt│ ├── tvmonitor_train.txt│ ├── tvmonitor_trainval.txt│ ├── tvmonitor_val.txt│ └── val.txt└── Segmentation ├── test.txt ├── train.txt ├── trainval.txt └── val.txt 3 directories, 92 files 主要介绍一下Main文件夹中的组织结构，先来看以下这几个文件： 12345├── Main│ ├── train.txt 写着用于训练的图片名称 共2501个│ ├── val.txt 写着用于验证的图片名称 共2510个│ ├── trainval.txt train与val的合集 共5011个│ ├── test.txt 写着用于测试的图片名称 共4952个 里面的文件内容是下面这样的：以train.txt文件为例 123456789101112131415000012000017000023000026000032000033000034000035000036000042…………009949009959009961 就是对数据库的分割，这一部分图片用于train，其他的用作val，test等。 Main中剩下的文件很显然就是每一类别在train或val或test中的ground truth，这个ground truth是为了方便classification 任务而提供的；如果是detection的话，使用的是上面的xml标签文件。 1234567├── Main│ ├── aeroplane_test.txt 写着用于训练的图片名称 共2501个，指定正负样本│ ├── aeroplane_train.txt 写着用于验证的图片名称 共2510个，指定正负样本│ ├── aeroplane_trainval.txt train与val的合集 共5011个，指定正负样本│ ├── aeroplane_val.txt 写着用于测试的图片名称 共4952个，指定正负样本………… 里面文件是这样的（以aeroplane_train.txt为例）： 123456789101112131415000012 -1000017 -1000023 -1000026 -1000032 1000033 1000034 -1000035 -1000036 -1000042 -1…………009949 -1009959 -1009961 -1 前面一列是训练集中的图片名称，这一列跟train.txt文件中的内容是一样的，后面一列是标签，即训练集中这张图片是不是aeroplane，是的话为1，否则为-1. 其他所有的 (class)_(imgset).txt 文件都是类似的。 (class)_train 存放的是训练使用的数据，每一个class都有2501个train数据。 (class)_val 存放的是验证使用的数据，每一个class都有2510个val数据。 (class)_trainval 将上面两个进行了合并，每一个class有5011个数据。 (class)_test 存放的是测试使用的数据，每一个class有4952个test数据。 所有文件都 指定了正负样本，每个class的实际数量为正样本的数量，train和val两者没有交集。 VOC2012 的数据集组织结构是类似的，不一样的地方在于VOC2012 中没有 test类的图片和以及相关标签和分割文件，因为这部分数据 VOC2012没有公布。 参考资料 http://host.robots.ox.ac.uk/pascal/VOC/ 性能指标（模型评估）之mAP 多标签图像分类任务的评价方法-mAP average precision]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>dataset</tag>
      </tags>
  </entry>
</search>
