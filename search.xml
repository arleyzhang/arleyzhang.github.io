<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[目标检测-SqueezeDet-论文笔记]]></title>
    <url>%2Farticles%2Ftarget-detection--squeezedet--paper-notes.html</url>
    <content type="text"><![CDATA[论文：SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural …代码：https://github.com/BichenWuUCB/squeezeDet 1 简要介绍这篇文章提出的SqueezeDet 网络主要是适用于自动驾驶领域的。 SqueezeDet : 基于SqueezeNet 的Detection模型，故称SqueezeDet 。 对于自动驾驶领域来说，几个关键因素如下： 精度。高精度，和尽可能高的 recall，最好使100%的recall； 速度。应该具备实时检测的速度（30FPS）； 较小的模型。小的模型好处有：训练时更加高效，车载客户端进行模型更新时需要较少的无线网络带宽和通信消耗，消耗更少的能量，更适用于嵌入式设备部署。 能量效率。一些桌面和框架级别的神经网络往往都需要很大的能量耗散（250W），但是对于自动驾驶领域来说，嵌入式处理器才是应该考虑的主要对象。毫无疑问，如何让神经网络模型适应小的能量消耗是个关键问题。 目前很多公司都在研发适用于自动驾驶的处理器，英伟达 的 Xavier 号称实现每秒 30 万亿次计算(TOPS)，整个系统功耗（包含CPU,GPU在内）为 30W；英特尔的 EyeQ5 ，号称实现15万亿次计算功耗为 5-6W。 SqueezeDet 主要就是针对以上几个方面提出了自己的架构。 SqueezeDet 主要参考了 SqueezeNet，YOLO和SSD，还有FCN全卷积网络。 最终的效果是，在KITTI 数据集上，输入分辨率为1242×375的情况下，其速度达到了57.2FPS，模型体积仅为不到8MB；在TITAN XGPU上 检测一张图片的能耗为 $1.4J$，这是Faster R-CNN 的1/84. 而精度也达到了目前 state-of-the-art水平。在某些方面甚至超过了其他的模型。 2 结构设计先回顾一下跟这篇文章有关的，其他的网络结构。 2.1 YOLO与Faster R-CNN相比，YOLO是属于一步到位的模型，不需要预先生成候选框，然后再去提取特征进行分类和回归。 YOLO的模型结构，如下： 但实际上YOLO也是有生成候选框的步骤的，只是 这个步骤跟检测合并到一起了。那这个生成候选区域的步骤在哪呢。 先看下面这两张图 YOLO的方法是这样的，将原图分成一个 7×7的网格： 对于每一个网格都预测两个 形状不同的bounding box（box的尺寸可以大于网格的大小），那么图片上目标的bounding box 肯定跟某几个不同的网格中的bounding box 距离最近。 那也就是说每一个网格所关联的两个 box （类似Faster R-CNN中的anchor）里面的区域有可能是目标，也有可能不是目标，或者只包含目标的一部分，或者一部分是目标，一部分不是目标。 那么这样一来每个 bounding box 都有一个概率，表示他是目标的概率。这个概率 是$$P_r(Object)*IOU_{truth}^{pred}$$其中 $P_r(Object)$ 表示这个box中是否存在目标，存在就为1，不存在就为0. 比 如上面两张图中，不同的两个格点（红色）对应着不同的两组 boxs，每个格点对应两个box，由于这四个box都与目标的区域有重叠（IOU不为0），所以这个四个box的 $P_r(Object)$ 都为1. 其实这个 $P_r(Object)$ 就是用 IOU （与ground truth之间的IOU）衡量的，首先计算IOU，只要IOU 不为0， $P_r(Object)$ 就是1. 一个box中有目标的概率其实就是IOU的值。 除此之外，每个box还都有位置信息，用坐标来表示，中心点坐标(x, y),以及box宽和高，w，h。 到此为止，每个格点关联的每个box有5个属性，4个位置信息，1个confidence信息。那一个格点关于box 的定位信息有10个。 但是检测不仅要检测出目标，还要识别出目标是什么物体，因此还需要额外的信息。假设要识别的类别总数为C，YOLO中，对于每个格点（不是关联的两个bounding box）都做一个物体类别预测。 那么一个格点还会产生C个物体类别预测信息。对于PASCAL 数据集来说 C=20。那此处 那也就是说一个格点经过网络之后，要得到 共30个信息，这里面包含定位（10）和识别信息（20维）。 所以我们观察一下YOLO的最后全连接层那一部分，可以看到YOLO用了两个全连接层，一层是4096×1的，还有一个是 1470×1的。前面的4096可以认为是根据经验选取的（alexnet，VGG第一个全连接层都是4096维的），但是后面的1470×1就不是经验值了，其实是 1470=7×7×30。 那这个是什么意思已经很明白了。此处的 7×7 就是要把图片分成多少 格点，后面的30就是每个格点要预测的信息维数。结合下图更明了。 那其实到这里就可以看出来了，YOLO解决的方式其实属于是回归问题。前面24个卷积层提取特征，后面两个全连接层，用于回归。回归的是什么？回归拟合的是每个格点两个box的坐标值（5×2），和每个格点的分类向量（20）。 2.2 Faster R-CNNFaster R-CNN的结构见下图，很显然是两步检测的，先生成一系列可能是目标的区域（stage 1），然后对这些区域进行检测（stage 2）。stage 2的箭头起点不太对，应该是从 proposals开始，前面的卷积部分是共享的。 实际上，Faster R-CNN的stage 1就已经产生目标检测的区域了，可以认为这已经是一个 弱的检测器了（这个称为RPN），这个stage 1已将包含了定位的信息，只是它只确认区域中是否包含目标，而不关心包含的是什么类别的目标。stage 2是一个强检测器，除了进一步精修定位信息，最主要的是把目标类别识别来。 假设我们在stage 1中把候选区域中的目标类别也预测出来，那其实stage 2可以不用了。 YOLO 其实就相当于 Faster R-CNN的stage 1步骤，当然细节上会有不同。比如 YOLO后面使用的是全连接层，而Faster R-CNN stage 1步骤（RPN）后面使用的是全卷积层。 YOLO每个格点的是预测目标类别的（多分类），RPN只预测是否是目标（二分类）。 再看看RPN的主要结构，如下图： 主要就是 anchor这个机制，假设feature map上每个像素关联k个anchor。 假设前面用的ZFNet，那么最后一个卷积层输出的feature map是13×13×256的。 先看其中一个window，如果后面使用256个3×3×256的卷积核，那么 256-d那个地方输出的就是 1×1×256维的，那么后面的cls层（二分类）就是2k个1×1×256的卷积，输出为1×1×2k；reg层（定位）就是4k个1×1×256的卷积，输出为1×1×4k。 与YOLO不同的是，每一个anchor（对应YOLO中每个格点内的两个box）都输出类别概率。 图上那个sliding window的做法，在实际操作的时候，其实就是用了卷积的方式，步长为1，pad为1. 所以实际上 256-d那个地方其实是 13×13×256。对应的cls层和reg层的输出分别是13×13×2k，13×13×4k，如下图。13×13维度上每一个 1×1×2k或4k的长条，都代表图片上不同位置的anchor box的类别信息或者位置信息。 2.3 SSD先看下SSD的宏观结构，如下图，FC6和FC7以及之前的部分是VGG的内容，后面是SSD里面加的。SSD添加的层 这是一个多尺度预测（Multi-scale）的网络，如下图从FPN的论文里找来的图。就是从不同的feature map上预测，然后将各个预测结果融合到一起。 然后将SSD与YOLO相比较一下，如下图。可以看出YOLO是单一尺度的。二者最大的区别也在此。当然在框的生成方面SSD也是一步到位的，一边生成框，一边检测。 更详细的看一下SSD的检测流程如下： 总共6个尺度，每一个尺度上都进行框的生成和预测。最后使用NMS（非最大值抑制）进行过滤，得到最后的结果。 与YOLO的不同主要在于： 多尺度 主干部分，全卷积 预测部分也是卷积（YOLO的预测部分是全连接层） 接下来看看SSD是如何检测的。如上图红色框，这一部分其实是使用 3×3的卷积实现的。具体细节看下图： 再看看是怎么生成default box的，如下图。最右边那个就是对应的feature map的尺寸，然后将feature map上的每一个像素点都映射到原图上（最左边），然后映射到原图上的每一个像素都关联三个不同的default box。 有没有发现这跟 YOLO中的 网格划分很像，只不过YOLO是划分成网格，这个网格划分对应于最后一个全连接层做了reshape 的feature map，每一个网格也只包含一个像素，每一个像素再映射到原图上。 但其实更像的是Faster R-CNN中的 anchor box。 与YOLO不同的是，每个 default box（对应YOLO中每个格点内的两个box），都输出4个位置信息和 (C+1) 个类别信息（包含1个背景）。 与Faster R-CNN 不同的是，每个 default box（对应Faster R-CNN 中的两个anchor box），都输出(C+1) 个类别信息，而不是一个二分类标记box是不是目标。 看下图这个就是分类和检测的实现方法，跟Faster R-CNN中的RPN如出一辙。 2.4 SqueezeDet2.4.1 结构分析网络结构如下： 前面是特征提取网络，这里用的SqueezeNet，中间是本文提出的ConvDet层，后面是Filtering层。Filtering层就是NMS（非最大值抑制）。 ConvDet层意即，使用卷积层来检测（Detection），因为经典的用法是使用全连接层来进行检测（比如YOLO），不过Faster R-CNN中的RPN相当于一个全卷积的弱检测器。这个方面来说SqueezeDet,跟RPN也挺像。 但是SqueezeDet更像SSD. 就相当于SSD的一个尺度分支，只不过前面卷积层提换成了squeezenet。 再看一下 SqueezeDet 关于anchor box的图，是不是跟SSD和RPN很像，简直如出一辙。 2.4.2 计算量分析以下图中 蓝色的为某一层的输出特征，而黄色的是神经元的参数分布（权值和偏置） 以下3D卷积写法均略去前一层的输入通道数 首先看下RPN的参数量： 输入是 $W_f× H_f×Ch_f$ 的feature map， 中间黄色的部分是 $1×1$ 的卷积核，如果把分类和回归放在一起，那么卷积核的个数就是 $K×(4+1)$ 个，4是四个坐标，1是一个概率，K是K个anchor。 把分类和回归的输出也放在一起，那么输出feature map就是$W_f× H_f×(K×(4+1))$ 大小的。 参数量为 ： $1×1×Ch_f ×K×5$ 再看下 YOLO中的参数量： 这个检测模块称之为 FcDet 同样输入是 $W_f× H_f×Ch_f$ 的feature map， 假设将图片分成 $W_o ×H_o$ 个网格（比如7×7）；每个网格关联K个box（比如K=2）；每个box输出（4+1）个信息（4个位置信息，1个类别信息）；每个格点输出C个类别概率（比如20）。 首先看第一个卷积层，FC1标号的是神经元的参数分布，可以看到从卷积层到全连接层的参数分布其实是一个 $W_f× H_f$ 的卷积核，这个卷积核与一整张feature map卷积，只输出一个激活值；这样的卷积核共有 $F_{fc1}$ 个，因此FC1输出为 $1×F_{fc1}$ 。 这一层参数量为：$W_f× H_f×Ch_f×F_{fc1}$ 接着第二个全连接层，FC2标号的也是神经元的参数分布，这里用的是 $1×1$ 的卷积层实现的，由于上述假设，FC2的输出应该是 $1×\{W_o ×H_o×(K(4+1)+C)\}$ ，是一个二维的tensor；但是为了与划分的网格匹配，需要将这个输出reshape成$W_o ×H_o×(K(4+1)+C)$ 这样的三维tensor。 这一层参数量为： $F_{fc1}×W_o ×H_o×(5K+C)$ 。 FcDet整个结构的参数量为：$W_f× H_f×Ch_f×F_{fc1} + F_{fc1}×W_o ×H_o×(5K+C)$ 看下这篇文章提出的ConvDet结构的计算量： 这个结构说白了就是SSD的一部分，再来对比一下，看看下图SSD中相对应的部分，这个图中把分类和回归定位分开画了，不过SSD的代码中实际上也是分开做的，其实这部分可以合并到一块的，因为卷积都是用的3×3，输出也都是 5×5，所以合并到一块也是可以的，只是不同的通道对应着不同的分类或者定位结果。 基本上二者相同，只不过这里起了个名字，叫做ConvDet。 同样输入是 $W_f× H_f×Ch_f$ 的feature map， 只不过这里假设 卷积是 $F_w×F_h$ 的（SSD对应结构中用的是3×3），通道数为 $K×(4+1+C)$ , K为 anchor（或 default box 或 格点内关联的box）的个数，4为每个anchor的位置信息，1为是目标的概率，C为C个分类。这样输出为 $W_f× H_f×\{K×(4+1+C)\}$ 。 ConvDet的参数量为： $F_w×F_h×Ch_f×\{K×(5+C)\}$ 与YOLO 相比，没有使用全连接层，而是使用了全卷积的（FCN）方式。其实就是SSD中的一部分。 三者参数量对比： 举个例子计算下： 假设输入feature map 为 $7×7×1024， F_{fc1} = 4096, K = 2, C = 20, W_o = H_o = 7,$ ConvDet 使用3x3 的卷积核 YOLO: $4096×（7×7×1024+7×7×（5×2+20））=212×10^6$ ConvNet: $3×3×1024×2×（5+20） =0.46 ×10^6$ 2.6 训练 先看一下，网络预测的是什么 如下图： 绿色框为ground truth 红色为anchor 蓝色为预测的box 首先，Ground Truth 与 Anchor 有个偏移值，如下：$$Offset^{GT}=\{\delta ^G(x_{ijk}),\delta ^G(y_{ijk})，\delta ^G(w_{ijk})，\delta ^G(h_{ijk})\}$$其次，Prediction 与 Anchor 有个偏移值, $Offset^{P}$，如下：$$Offset^{P}=\{\delta (x_{ijk}),\delta (y_{ijk})，\delta (w_{ijk})，\delta (h_{ijk})\}$$对于定位来说：可以认为，这个网络预测输出的是 $Offset^{P}$ ，标签是 $Offset{^GT}$ ，转化一下，实际上网络输出的是 $ \delta (x_{ijk}), \delta (y_{ijk}), \delta (w_{ijk}), \delta (h_{ijk})$ ，因为这四个参数决定了 $Offset^{P}$ 。 分类就是输出类别，标签为真实类别。 以上偏移是由网络学习得到的。 偏移与最终的输出之间还有一个映射关系，这个关系是人为指定的。这个关系最先在R-CNN系列的目标检测模型中提出。下述。 最终的预测框的中心坐标 $(x_i^p,y_i^p)$ 和宽高 $(w_k^p,h_k^p)$ 的计算方式如下： $\hat x_i,\hat y_i$ 是与box关联的feature map中的像素（或者说类似于YOLO中的格点）的坐标，如下图中的 $C_x，C_y$ 。 $\hat w_k,\hat h_k$ 是 每个像素（或格点）关联的第k个anchor的 宽和高，如下图中的 $b_w,b_h$ 偏移与真实位置之间的映射有两种，线性映射（中心点坐标）和指数映射（宽，高）。 这个地方可以结合 YOLO9000论文中的一张图理解。不过对于预测和anchor的关系，二者刚好反过来。 再来看一下损失函数，如下： 损失函数与YOLO的损失函数差不多，放一张YOLO的对比一下： 解释一下SqueezeDet的损失函数: 主要分为三部分： 第一部分是回归损失，或者说定位损失，就是下图这个： $I_{ijk}$ 表示：位于 输出feature map上 $(i,j)$ 坐标处的第 $k$ 个anchor 是否包含有目标。也就是 这个$P_r(Object)*IOU_{truth}^{pred}$ 是否为0. $\delta x_{ijk},\delta^G x_{ijk}$ 分别表示anchor与预测box和ground truth 的 x 坐标的偏移。后面其他的参数，y，w，h类似。 这是一个均方误差。没有采用YOLO那样的将 宽度w和高度h开根号，再做均方误差。YOLO这么做是考虑了对于大小目标计算损失时的公平问题。 第二部分是二分类损失，这个二分类决定的是这个anchor是否是目标，如下： $\gamma _{ijk}$ ：$P_r(Object)*IOU_{truth}^{pred}$ 2.5 试验结果]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
        <tag>lightweight models</tag>
        <tag>SqueezeDet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-FSSD-论文笔记]]></title>
    <url>%2Farticles%2Ftarget-detection--fssd--paper-notes.html</url>
    <content type="text"><![CDATA[论文：FSSD: Feature Fusion Single Shot Multibox Detector代码：https://github.com/lzx1413/CAFFE_SSD/tree/fssd 这篇文章在FPN的基础上进行了一定的思考，提出了一种新的特征融合方式，并将这种特征融合方式与SSD相结合，在损失一定速度的情况下，提高了检测精度。 这篇文章立足于特征融合。 1 简要介绍首先是模型的拓扑结构： (a)：featurized image pyramids ，使用 image pyramid 构建 feature pyramid，金字塔形状的feature是从不同scale的图片上计算出的。典型的使用这种结构的模型：OverFeat，R-CNN。 (b)：对图片来说只有一个scale，但是为了达到 预测不同scale 的bounding box的目的，Faster R-CNN 提出了 anchor 的机制，解决了这一问题。典型的模型有：Faster R-CNN，R-FCN等。 (d)：先说下d吧，这是SSD中提出的结构，充分利用了CNN自身的 feature pyramid 结构。结合了不同的层，分别预测，预测的时候是独立的，最后生成结果时才通过NMS融合。严格来说不能算是特征融合，只能说是multi-scale 预测。 (c)：这是FPN和DSSD提出的结构，这个才是真正的特征融合，在预测之前先通过一系列上采样额外再生成一系列feature pyramid，并通过lateral/skip connections 与CNN主结构上的feature pyramid 特征进行融合，之后再去预测。 (e)：这是本文的拓扑结构。与FPN结构相似，但实际上跟FPN 有很大的不同。FPN是先上采样，再多个层融合；FSSD是先融合多个层的feature map，后面再跟一个CNN模块，这个CNN模块跟SSD是一样的，采用multi-scale预测。 实际上除了上面的一些拓扑结构之外，还有一种拓扑结构，就是下图的 (b)。 (a)：与前面那个图的(a)是一样的，image pyramid。 (b)：这种结构是前面没有提到的，multi-scale filter ，这个跟SSD比较像，SSD是multi-scale prediction，每一个prediction 都对应一种size的filter。 (c)：Faster R-CNN的anchor机制，从(b)中发展出来的，把multi-scale filter 换成了 multi-scale anchor。 2 模型设计2.1 Feature Fusion Module作者认为 FPN的融合是在多个feature map进行的，这种侧向连接和 element-wise add 是很费时间的： 所以作者提出了一种 lightweight and efficient feature fusion module 特征融合只在一个位置发生 后续的结构跟SSD是一样的 作者提出一种数学模型来表示这种结构： $X_i, i ∈ C$ ：用于融合的 source feature map，SSD中选择的是：conv4 3, fc 7 of the VGG16 and new added layer conv6_2, conv7_2, conv8_2, conv9_2 。这篇文章选用的是conv3 3, conv4 3, fc 7 and conv7_2 $T_i$ ：在融合之前 每一个 source feature map的变换函数，类似于激活函数。在FPN中使用的是 1×1 的卷积来降低输入通道。这篇文章使用的 1×1 卷积 + 线性插值的， 1×1 卷积 同样是为了降低输入通道，线性插值是因为这里的融合是不同level的层或者前后层之间的融合，他们的feature map大小是不一样的，因此需要线性插值将所有的source feature map 转换成统一的大小。FPN中是侧向连接，是同一level的层之间的融合，因此不存在这个问题。 $\phi_f$ ：特征融合函数，FPN中使用的 element-wise summation ，而这个操作不仅要求feature map的大小一样，还要求 feature map的通道数也要统一，这个在FPN中在使用 1×1 卷积时就保证了融合时的通道统一。由于操作麻烦，在FSSD中，1×1 卷积只负责降低通道，而不保证所有feature map的通道统一，融合时选用了concatenation 的操作，简单有效，不需要feature map的通道统一。 $\phi_p$ ：生成 后续 pyramid features的函数，与SSD一样，只不过这个后续结构，作者在这个部分尝试了多种结构的堆叠，见下图。最终通过实验选择了一种简单的方式，这与SSD中的结构是一样的。 $\phi_{c,l}$ ：用于检测和分类的函数，与SSD一样。 2.2 Training两种训练策略： 第一种：由于这是在SSD的基础上改进的，所以可以使用预训练的SSD，然后fintuning 第二种：使用预训练的VGG，然后fintuning SSD300：SSD的训练曲线（VGG+fintuning） FSSD300：前面提到的第二种，VGG+fintuning FSSD300+：前面提到的第三种，SSD+fintuning 两种训练方法训练出的模型的精度相差无几，但是第一种训练方法明显收敛更快。 3 实验一些对照试验，决定了前面提到的很多参数的选取，比如用于融合的source feature map，融合的方法，训练方法，batch normalization等。 PASCAL VOC 注意 在对比速度时，测试的GPU是不一样的，1080ti的性能比TITAN X要好很多。 网络的 backbone network也是不一样的。 同样 注意 在对比速度时，测试的GPU是不一样的 这张图对比 FSSD，与SSD的速度是有意义的，因为其他模型都是在TITAN X上测试的。 COCO]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
        <tag>FSSD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-HyperNet-论文笔记]]></title>
    <url>%2Farticles%2Ftarget-detection--hypernet--paper-notes.html</url>
    <content type="text"><![CDATA[论文： HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection 1 简介这篇文章主要解决了两个问题： 解决 R-CNN 系列 目标检测模型，需要生成几百上千的候选框（Faster R-CNN 300；Fast R-CNN 2000）的问题。HperNet 只需生成 100~200个候选框。而精度比 Faster R-CNN 还要好一点。 将deep ConvNet的高低层特征进行融合，进而利用融合后的特征进行region proposal提取和进一步目标检测。提升了对小目标的检测效果。 主要的贡献： object proposal task 上：使用50个proposals 可以获得95%的recall；使用100个proposals可以获得97%的recall。 注意：这个recall只是用于region proposals的，并不是针对 PASCAL 或 COCO 数据集的最终检测效果的recall。就是说事先生成的 几百个region 与 ground truth 的区域IOU在某一阈值之上的region 占总 的ground truth的比例，这个比例自然是比最终detection的recall要高很多的。因为这个阶段相当于一个弱检测器，约束条件也不高，生成的框里面自然会包含很多 ground truth 的区域。 detection challenges 上：在PASCAL VOC 2007 和 2012 数据集上取得的 平均检测精度（mAP），分别为76.3%和71.4%。比Fast R-CNN分别高了6个和3个百分点。 HperNet 的快速版，可以达到 5FPS的速度。 宏观架构： 整个网络结构也属于两步检测，即先生成 region proposals ，再进行检测。 2 HyperNet 网络结构 图中标注的 写法 比如 5×5×42 指的是，这是一个5×5的卷积核，卷积核的个数为42个，其他的 3×3×4，3×3×63也是一样的写法，写的都是卷积核的参数。 整个网络结构跟 Faster R-CNN 还是比较像的。 主要分为三部分： 第一部分：卷积层提取特征，然后进行特征融合； 第二部分：弱检测器，生成候选区域，卷积层+全连接层, Conv+FC； 第三部分：强检测器，生成最终的检测结构，卷积层 + 全连接层，Conv+FC。 下面分别介绍： 2.1 Hyper Feature Production特征融合，如下图： 将图片resize，使得最短边 为600（比如：resize后的大小为 1000×600）； 首先使用基础卷积神经网络（Alexnet，VGG）提取特征； 在最底层卷积层（conv1）后面加上 max pooling 层，实现降采样； 在最高层卷积层（conv5）后面加上 反卷积 deconv 层，实现上采样； 中间层（conv3）不做处理； 在上一步操作之后每一个 level 后面都再加 一系列卷积层（绿框中的黄色矩形）： 进一步提取语义特征、将不同分辨率的feature map 压缩到相同的分辨率。 卷积后每个 feature map 加局部响应归一化 LRN，之后输出 Hyper Feture maps。这里的Hyper Feture maps的大小不是13×13×126的，图上写的那个 13×13 是经过ROI 的大小（红色框框）。这点作者没说清楚，代码也没有，搞不明白Hyper Feture maps到底是多大。 假设使用VGG做特征提取，输入图片为1000×600。那么Conv1、Conv2、Conv3、Conv4、Conv5层的feature map 输出（不含池化层）分别为 1000×600，500×300，250×150，125×75，62.5×37.5。 最后的feature map会是62×37, 这样会造成信息损失。 因此把最后的Conv5层的feature map通过反卷积上采样到250×150，然后再经过一系列卷积； 把第一层Conv1的feature map做一个max pooling 降采样，然后再经过一系列卷积； 中间层，不做变化，直接经过一系列卷积； 然后把1，3，5层的feature map 进行 LRN之后连在一起。LRN是必要的，因为不同层的feature resolution 不同，如果不做正则，norm大的feature会压制住norm小的feature。 2.2 Region Proposal Generation 如上图，对于Hyper Feture maps 使用 ROI pooling 。ROI pooling 的大小是 13×13的，因此最后输出的特征是 ROI feature 是 13×13×126的。中间那个红色的框框是ROI，但是怎么来的，作者没说清楚。 ROI pooling之后使用了 3×3的4个卷积核。输出的是 13×13×4 的feature map，图上那个空白的矩形就是。之后再经过一个256维的 全连接层；再之后并行经过两个 两个全连接层，分别是分类和回归（用于定位）。 这个阶段每张图片生成 30k 个不同尺寸的 候选框。然后通过NMS，减少到1k 个，然后再取 top-100或top-200个。 不过有个疑问的地方：这里的ROI是怎么来的作者没说清楚。 这也是与Faster R-CNN的不同之处，Faster R-CNN的 ROI Pooling 是在RPN生成 region proposals 之后作用在生成的region上的。如下图Faster R-CNN的结构，对比一下二者在整体结构上的差别。 而在HyperNet中在生成region proposals时就使用了 ROI Pooling ，如下。 Faster R-CNN中的ROI是 RPN生成的 region proposals。 HyperNet中的ROI是怎么来的是个疑问。而且HyperNet中的ROI相当于 Faster R-CNN中的anchor box。 一直不知道HyperNet中的ROI是怎么来的，直到我在知乎上找到了 这个帖子： 对话CVPR2016：目标检测新进展 这个帖子的作者（孔涛,清华大学）好像就是这篇文章的作者 （Tao Kong，Tsinghua University） 里面有一段话： HyperNet：文章的出发点为一个很重要的观察：神经网络的高层信息体现了更强的语义信息，对于识别问题较为有效；而低层的特征由于分辨率较高，对于目标定位有天然的优势，而检测问题恰恰是识别+定位，因此作者的贡献点在于如何将deep ConvNet的高低层特征进行融合，进而利用融合后的特征进行region proposal提取和进一步目标检测。不同于Faster R-CNN，文章的潜在Anchor是用类似于BING[4]的方法通过扫描窗口的方式生成的，但利用的是CNN的特征，因此取得了更好的性能。 通过以上的改进策略，HyperNet可以在产生大约100个region proposal的时候保证较高的recall，同时目标检测的mAP相对于Fast R-CNN也提高了大约6个百分点。 里面有一句： 不同于Faster R-CNN，文章的潜在Anchor是用类似于BING[4]的方法通过扫描窗口的方式生成的，但利用的是CNN的特征，因此取得了更好的性能。 也就是说 HyperNet中的ROI 是使用类似于BING的方法提取的 ，上面提到的 BING[4] 文献是： [4] Cheng M M, Zhang Z, Lin W Y, et al. BING: Binarized normed gradients for objectness estimation at 300fps. In CVPR 2014 这篇文章的项目主页是：http://mmcheng.net/bing/ 大概看了一下论文，是一种利用手工设计的特征进行 object 估计的。这个特征叫做 binarized normed gradients features （BING）. 具体是怎么弄的，没仔细看。 看一下结果好了，图中的SEL是我们熟知的 Selective Search算法。DR是指object detection rate ，不清楚怎么算的。 运行速度如下： 测试条件：Intel i7-3940XM CPU Method [1] OBN [2] CSVM [3] SEL [4] Our BING Time (seconds) 89.2 3.14 1.32 11.2 0.003 可见BING是比 Selective Search效果更好的算法，速度也比 Selective Search 快很多，可达300FPS。 也就是说HyperNet使用了额外的 region 估计算法，使得后面的 region proposals 提取的region更加精确，然后之后再进行object detection。 作者说的是使用的是类似于 BING的方法，具体怎么用的，没有代码，不知道是怎么用的。 2.3 Object Detection 与Faster R-CNN不同之处在于 在两个全连接层之前使用了一个 3×3 卷积层，降低了输入通道（从126到63）。后面是三个全连接层。 还有一个不同是，全连接层之间的 dropout 层的概率使用的是0.25，不是0.5。 2.4 训练策略损失函数： 没啥大变化 训练步骤： 与Faster R-CNN训练步骤一样，都是先训练 region proposals 网络，然后保持卷积层部分权值不动，再训练object detection 网络。然后再保持卷积层特征提取部分权值不动，再训练region proposals 网络。 只不过这里多训练了两次。 2.5 加速 如上图，上边的图中两个空白矩形框，第一个是 ROI Pooling的输出，第二个是 3×3卷积的输出。 下边 的图中，3×3卷积的输出用了一个立方体，后面那个白色矩形框是ROI Pooling的输出。 在生成region proposals 时，将卷积层放在 ROI Pooling层之后，可以实现加速。 先卷积降低了通道数量（由126降到4）； 大量 proposal 的conv操作移至前方，实现计算共享； ROI Pooling 后面分类器的结构变成了全连接层，简化结构，之前是 卷积层＋全连接层。（个人觉得这个作用不大） 使用这个策略获得了 40倍的加速。 3 试验结果分析3.1 region proposals 上图衡量的是 不同的IOU 阈值 对 recall的影响。再说明一下，这个recall不是最终的detection recall。而是region proposals recall。 IOU 阈值越大，条件越严苛，recall势必会下降。对比几条不同方法的结果，HyperNet下降的最慢，比其他方法recall也高。 上图衡量的是 不同的proposals 数量 对 recall的影响。再说明一下，这个recall不是最终的detection recall。而是region proposals recall。 proposals数量越少,条件越严苛,recall势必会下降。对比几条曲线，HyperNet下降的最慢，比其他方法recall也高。 上面这个表，衡量的是IOU=0.7的情况下，region proposals recall达到50%和75%时，需要proposals的数量。很明显 HyperNet需要的最少。 Conv1、Conv2、Conv3、Conv4、Conv5 5个卷积层，为什么选择1,3,5呢？答案在下图中。 3.2 Object Detection结果： VOC 2007 VOC 2012 关于小目标的检测，没有明确标准，只是使用的PASCAL VOC中相对较小的目标（bottle,plant,chair等）做检测。 3.3 Hyper Feature Visualization 3.4 Running Time 4 总结整个网络就是 Faster R-CNN的改进版。最大的改进就是特征融合方法（Hyper Feature）和ROI 的提取方法（BING）。使得region proposals的数量最少降到了100，就可以获得比 Faster F-CNN高的mAP。 另外，虽然作者说对小目标的的预测效果有改善，但只是从定性的方面来表达的，个人认为应该针对 COCO数据集进行定量测试才更有说服力。 5 参考资料 孔涛：对话CVPR2016：目标检测新进展：https://zhuanlan.zhihu.com/p/21533724 BING: http://mmcheng.net/bing/ http://blog.csdn.net/u012361214/article/details/51374012]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
        <tag>HyperNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础DL模型-Deformable Convolutional Networks-论文笔记]]></title>
    <url>%2Farticles%2Fnotes-on-the-basic-dl-model--deformable-convolutional-networks-.html</url>
    <content type="text"><![CDATA[论文 Deformable Convolutional Networks代码 https://github.com/msracver/Deformable-ConvNets 这篇文章是微软亚研院的作品，跟STN的思路基本上是一样的，但是做法还是有很大不同。 1 简介上一篇讲STN（Spatial Transformer Networks）的时候，提到 CNN中池化层的存在带来了空间不变性（旋转不变性，平移不变性等），而 Spatial Transformer Networks 也是为了增加空间不变性。但是二者增加空间不变性的出发点是不同的： CNN是尽力让网络适应物体的形变，而STN是直接通过 Spatial Transformer 将形变的物体给变回到正常的姿态（比如把字摆正）。 也就是说，从模型的几何变换能力（空间变换能力）来说，CNN是比较弱的，因为它主要是通过合适的降采样来减弱目标形变对预测的影响。CNN的几何变换能力不好，是因为它是一个固定的几何结构（固定的卷积核，池化核），所以CNN对几何形变的建模能力是有限的。而STN就是一个比较好的具备对几何形变建模能力的模型。 通常来说为了适应物体的几何形变一般有以下几种做法： 数据扩展 data augmention ：这个在深度学习中很经常使用，比如 multi-view 的方式，选取5个patch等 使用具备平移不变性的特征或算法 transformation-invariant features and algorithms ：特征，比如 SIFT 特征，一般在传统的方法中使用；算法，比如卷积神经网络，目标检测模型中的sliding window，现代计算视觉中使用。 CNN模型中还可以使用池化的方法 缺点： 几何变换能力是固定的，已知的：比如CNN固定的卷积核，固定的池化核，固定的ROI；数据扩展是先验知识的一种利用。对于新的视觉任务（比如换个数据集），之前构建的固定的几何变换模型可能就不能用了，这也是为什么在ImageNet上训练好的分类模型用到检测或分割上时要 fine-tuning。 对于手工设计的 特征比如SIFT，在一些复杂的视觉任务中，表现很差，甚至是不可行的。 CNN在分类，检测，分割领域取得了很大进展，这些模型增强几何变换能力的方法也基本是上面提到的三种方法,但是上面提到的缺点仍然存在。 CNN固定的卷积核，固定的池化核，固定的ROI，导致高层神经元的感受野（receptive field ）是固定的，没有办法有效的编码语义或者位置信息。 如果感受野能够自适应的动态调节，那对精细化的定位问题是很有利的。 这篇文章在STN的基础之上，针对分类和检测提出了两种新的模块 deformable convolution （可变性卷积）和deformable RoI pooling （可变性 ROI池化），来达到动态调节感受野的作用，取得了不错的效果。 2 Deformable Convolutional Networks为方便表示，以下所有都是在2D卷积的基础上示例，对于3D卷积 来说每个通道的变换是一样的。 2.1 Deformable Convolution 先看下可变性卷积的效果，如上图： (a)：标准卷积采样点 (b), (c), (d)：浅绿色是标准卷积采样点，深蓝色是可变形卷积采样点，浅蓝色箭头是偏移（offset） (c), (d)是(b)的特例，其中(c)是 带孔卷积(Atrous convolution )或者叫膨胀卷积(dilated convolution)，这说明可变形卷积是比较具有一般性的卷积形式。 可变形卷积与标准卷积相比，采样点不再是固定的了，而是有个偏移量offset，这个偏移量是2维的，通过网络学习得到，它使得采样点的形状可以根据目标发生变化。 2D卷积： 用 $R$ 表示卷积核中的格点，比如以下是一个 3×3 的卷积核的表示 标准2D卷积的过程： $x$ ：输入 feature map $y$ ： 输出 feature map $p_0$ ：输出feature map $y$ 上的坐标 $p_n$ ： $R$ 中的坐标 采样点坐标： $p_0+p_n$ ，整数 $w$ ：采样参数，CNN中就是权值 $w(p_n)$ ：代表卷积核中的权值 $x(p_0+p_n)$ : 采样点处的像素值 可变形卷积： 要给 $R$ 加上偏移 offset： $\{\Delta p_n | n=1,…,N\},N=|R|$ 现在的采样坐标点变成了 $p_0+p_n+\Delta p_n$ ，$\Delta p_n$ 是小数，所以采样坐标也变成了小数坐标 $x(p_0+p_n+\Delta p_n)$ ：对应到feature map上没有像素值，因此又需要插值了 插值函数： $p=p_0+p_n+\Delta p_n$ : 输入feature map $x$ 上的小数坐标 $q$ : 输入feature map $x$ 上的所有整数坐标 $G(q,p)$ ：插值核函数，可以代表更一般性的插值核函数，比如线性插值，最近邻插值等，不过这里用的是双线性插值 因此插值核函数： $g(a, b) = max(0, 1 - |a - b|)$ 大概说一下线性插值核函数，如下：$$W_{bil}(x,y)=w_{lin}(x) \cdot w_{lin}(y)=\begin{cases}1-x-y-x\cdot y &amp; for \, \, \,0 \le |x|,|y| &lt;1\[2ex]0 &amp; otherwise\end{cases}$$详细的参考：双线性插值 ， 图像处理之插值运算 . 虽然插值需要对输入feature map $x$ 上的所有整数坐标进行遍历和计算，但是由于 $G(q,p)$ 实际上在很多位置都是0，所以插值计算是很快的。 可变形卷积的结构示意图 这个模块有两条 流动路线， 下面一路是一个标准的 3×3 卷积， 上面也是一个 3×3 卷积，步长一样，输出 offset field 与 input feature map 大小一样 通道为 2N 个，代表N个2维的偏移场，一个通道是一维（x 或 y） 剪裁（crop）出于卷积核相对的那一块区域，得到offsets 与下面一路标准卷积核进行相加，然后线性插值，确定采样点坐标 最后进行卷积操作即可 后向传播： 对输入feature map的后向传播与标准卷积一样 对 offset $\Delta p_n$ 的导数如下： 其中可以通过公式4求出。 $\partial \Delta p_n$ 代表 $\partial \Delta p_n^x$ 或 $\partial \Delta p_n^y$ 2.2 Deformable RoI Pooling首先是 ROI Pooling： 回顾一下 ROI 池化： 输入feature map ：$x$ RoI 的大小 ：$w×h$ 左上角点坐标： $p_0$ RoI pooling 将 RoI 分成 $k × k$ ($k$ 是自由参数，可以人为设定) 个网格(bins )，每个网格中像素有多个，大小： $\dfrac{w}{k} × \dfrac{h}{k}$ 输出 $k × k$ 大小的 feature map： y 坐标 $(i, j)$ 处的网格 $(0 ≤ i, j &lt; k)$ 的输出： $p\in bin(i,j)$ 坐标 $(i, j)$ 处的网格的像素跨度：$\lfloor i {w \over k}\rfloor \le x \le \lceil (i+1){w \over k}\rceil$ 和 $\lfloor j {h \over k}\rfloor \le y \le \lceil (j+1){h \over k}\rceil$ ，$\lfloor \cdot \rfloor$ 和 $\lceil \cdot \rceil $ 分别代表下界和上界 $n_{ij}$ : 坐标 $(i, j)$ 处的网格中的像素总数，这是个平均值池化 $p_0+p$ $(i, j)$ ：处的网格中的坐标，即采样点坐标 同样的要给 ROI 加上偏移 offset： $\{\Delta p_{ij} | (0 ≤ i, j &lt; k) \}$ ，如下： 同样的 $\Delta p_{ij} $ 一般都是小数，同样的还是用双线性插值来完成坐标映射，公式3和公式4. 可变形 ROI 池化 结构图 首先从ROI中生成 池化后的 feature map 其后全连接层生成 归一化的（normalized） offsets： $\Delta \hat p_{ij}$ ，这个归一化的向量是必须的，因为ROI的尺寸不一，必须要统一标准，比如归一化到 [-1,1] 归一化的向量 $\Delta \hat p_{ij}$ 再生成 offsets $\Delta p_{ij}$ , 直接与ROI做 点积： $\gamma$ 是一个预定义的系数，用于衡量 偏差offset 的 重要程度，按经验选取，文章使用的 $\gamma =0.1$ . 后向传播： $\dfrac{\partial y(i,g)}{\partial \Delta \hat p_{ij}}$ 可以很轻松的通过 求出。 再来看看Position-Sensitive (PS) RoI Pooling 基本与ROI Pooling相同，因为池化后的feature map 中的每一个像素都来自不同组的score map 的对应位置（颜色对应），所以公式5和6 中的 输入feature map 由 $x$ 改为 $x_{ij}$ 不同的是 offset的生成方式， offset 的生成方式与 分类问题中的 PS Pooling是一样的 从输入feature map另起一路，先生成 $2k^2(C+1)$ 通道的 score map（C个类别，偏移量是2维的），然后PS Pooling 同样也是先生成 归一化的 偏移量，然后再转换成 与ROI 尺寸相关的 offset 与 池化坐标相加，得到采样点坐标。 上面图中的加了偏移量的网格（bins）已经与原来的网格产生了偏移 3 试验3.1 试验准备现有的CNN模型一般都包含两部分，一部分特征提取，一部分分类或检测或分割等。 Deformable Convolution for Feature Extraction ResNet-101 Aligned-Inception-ResNet ，这是 Inception-ResNet 的一个改进版本，因为对于密集预测任务（比如检测和分割） Inception-ResNet 的特征和原图上的目标不能对齐。 这两种网络的降采样比例由原来的32 调整为 16，并使用了 带孔卷积（Atrous convolution ） 或者叫做 膨胀卷积（dilated convolution ） 经过试验验证 deformable convolution 放置于网络最后3层。 Segmentation and Detection Networks DeepLab ：用于分割 Category-Aware RPN ：检测，RPN的分类由二分类换成多分类 Faster R-CNN 使用 FPN（Feature pyramid networks for object detection. ）的配置，ROI Poling 换为 deformable RoI pooling. R-FCN ：PS Pooling 换为 deformable position-sensitive RoI pooling. 3.2 Understanding Deformable ConvNets (a)：标准卷积中固定的感受野和卷积核采样点 (b)：可变形卷积中自适应的感受野和卷积核采样点， 意味着可变形卷积中高层神经元看到原图的区域更接近目标，所以更容易分类 这图表示的是高层的神经元在原图上的感受野和采样点 每个图像三元组显示了三层3x3可变形卷积核的采样点位置（共729个点），对应于三个不同的图像区域（从左至右，背景，小物体，大物体） 绿色点对应高层的一个像素点，红色点对应高层神经元在底层的感受野内的采样点，与figure 5对应 R-FCN模型中 PS Pooling换成可变形池化后，ROI中的网格分布 比较整齐的黄色框是 PS ROI Pooling 红色框是deformable PS ROI Pooling 很明显红色框更接近目标的形状 3.3 Ablation StudyDeformable Convolution 对检测或分割的效果 控制变量：deformable convolution 的数量 和 不同的网络模型 整体上来说 deformable convolution 越多，效果越好；但是从3-6的增益不大，所以文章其他部分都是用的3个deformable convolution 为了说明模型中高层神经元的感受野是动态自适应的，文章做了一个粗略的测量： 对可变形卷积核，加了offset 前后的采样点对，计算他们之间的平均距离，称为 有效膨胀：effective dilation 。这可以粗略的衡量感受野的大小，effective dilation越大，感受野越大 看看效果： 使用包含3个 deformable layers 的R-FCN 在 PASCLA VOC2007数据集上检测，目标按照COCO 的标准分为大，中，小。这三个可变形卷积层的有效膨胀 统计如下： 越大的目标有效膨胀越大，感受野也越大，确实在感受野的区域面积上实现了动态自适应调整 背景的感受野介于 中等目标和大目标之间 当然这个只是对感受野的区域大小作评估，而感受野的区域是如何跟随目标的形状动态变化的，就只能从figure5，figure6，figure7中观察了 ​ 疑问：这怎么跟figure 6上的可视化效果不太一样？figure 6上可是背景的感受野最大, 感觉有效膨胀确实不太准确, 个人认为有效膨胀衡量的是膨胀前后两个相对应的坐标对之间的距离，跟坐标点分布的大小区域的关系不太大，不过文章也说了是粗略测量。 atrous convolution 下图（b）是带孔卷积，可以跳着选，隔一个加一个，即 dilation=1，隔两个的话就是 dilation=2 再来看看带孔卷积的效果，膨胀卷积实际上是本文提出的可变形卷积的特例： dilation 从 2到4，到6，到8，整体上来看，效果在增加，说明默认 default network 的感受野比较小 最优的 dilation value 对于不同的任务和模型也是不同的，6 for DeepLab but 4 for Faster R-CNN deformable convolution 有最好的效果，说明自适应的学习可变形的卷积核或池化核是有效的 对比 Faster RCNN 和 R-FCN，当 deformable convolution 和 deformable RoI Pooling 同时使用时, 能够得到显著的效果提升， 模型复杂度对比： 参数会增加，但是增加的很少，对时间影响不大 说明可变形卷积网络获得的收益不是由于参数增加而获得的 COCO上的效果对比： 无疑 可变形卷积神经网络取得了不错的成绩 将ResNet101换为Aligned-Inception-ResNet ，即便不使用可变形卷积也会提高精度，说明 Aligned-Inception-ResNet 效果比 ResNet101好，Aligned-Inception-ResNet到底是个啥，等会说。 4 与STN的比较整个结构跟STN(Spatial Transformer Networks )还是比较像的。 deformable convolution 中学习到的偏移量 offset 可以认为是一种 轻量化的 spatial transformer 。 spatial transformer 是一种全局几何变换，一次把 目标所在区域全部给变换过去； 而deformable convolution 则是以局部、密集采样的方式完成几何变换的，本质上还是属于卷积操作。 spatial transformer crop出的区域是一个平行四边形，相对来说还是比较规整的； deformable convolution 中的感受野是可以根据目标形状随意形变的。 STN比deformable convolution的参数要多，而且学习变换矩阵比学习偏移量更难。 STN虽然在小的数据集上取得了比较好的效果，不过在大的数据集上的效果好像不怎么好，这篇论文好像有探讨：Inverse compositional spatial transformer networks. 5 Aligned-Inception-ResNetAligned-Inception-ResNet是Inception-ResNet的改进版本，由于 Inception-ResNet网络太深，使用了很多卷积层和池化层，深层神经元（靠近输入层）在原图上的映射，与感受野的中心是不能对齐的，比如下图这样的一层卷积，后一层的每个像素都与感受野的中心是对齐的，但是这样的层堆起来之后，然后再加一些池化层的话，就很有可能对不齐了。 实际上这种情况在深层卷积神经网络中是比较普遍的，一般都是通过控制池化层数量（比如ResNet，GoogLeNet模型中就没有池化层）和总降采样比例来减缓的，而且有些视觉任务是在能够对齐的假设下构建的，比如FCN，最后的feature map中的一个 cell 的预测对应了原图的一个位置。 为了弥补这个缺陷，何凯明还有微软亚研院一干人等 提出了 Aligned-Inception-ResNet ，这是一篇没出版的论文，arxiv网站上也没找到，这里是在 deformable convolutional networks 这篇论文的 附录部分看到的。 看下面这篇论文中的引用：unpublished work 论文中也声明了： 不过网络结构倒是挺清晰的，个别细节不太清楚。 网络结构看table6 和 figure 8： 与原始的Inception-ResNet，相比， Aligned-Inception-ResNet 使用了很多重复的结构，称为 IRB ，如figure 8，整个网络结构比 Inception-ResNet简单了好多； 关于对齐的方式，附录只是说在卷积层和池化层使用了合适的 padding（proper padding in convolutional and pooling layers. ）。具体是怎么做的也没说清楚. ImageNet上的效果如下： 可见效果并没有超越其他的结构，号称是Inception-ResNet的改良，竟然还不如 Inception-ResNet，而且参数也多，IRB那个结构感觉也是将Inception-ResNet 中的模块调整了一下，没有啥太大本质的变化。 唯一有效的恐怕就是在分割或者目标检测中用作基础网络时效果会好点吧，见table5. 这个结构目前只在两篇文章中见到过：一篇 是上面讲的 deformable convolutional networks ，还有一篇是：Flow-Guided Feature Aggregation for Video Object Detection ，都是微软亚研院的工作。 参考资料 双线性差值 图像处理之插值运算 https://mlnotebook.github.io/post/CNN1/]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>basic dl models</tag>
        <tag>Deformable Convolutional Networks</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础DL模型-STN-论文笔记]]></title>
    <url>%2Farticles%2Fnote-taking-of-the-basic-dl-model--stn-.html</url>
    <content type="text"><![CDATA[论文：Spatial Transformer Networks，是Google旗下 DeepMind 公司的研究成果。 这篇论文的试验做的特别好。 1 简介1.2 问题提出CNN在图像分类中取得了显著的成效，主要是得益于 CNN 的深层结构具有 空间不变性（spatially invariance）（平移不变性，旋转不变性），所以图像上的目标物体就算是做了平移或者旋转，CNN仍然能够准确的识别出来，这对于CNN的泛化能力是有益的。 空间不变性主要是由于 Pooling 层 和 步长不为1的卷积层 的存在带来的。实际上主要是池化层的作用，因为大部分的卷积层的步长都是大于1而又小于卷积核大小的，也就是滑动时是有重叠的，而池化层一般不是重叠的。也就是说这些层越多，越深，池化核或卷积核越大，空间不变性也越强；但是随之而来的问题是局部信息丢失，所以这些层越多准确率肯定是下降的，所以主流的CNN分类网络一般都很深，但是池化核都比较小，比如2×2。 比如ResNet，GoogLeNet，VGG，FCN，这些网络的总降采样比例一般是 16或32，基本没有见过 64倍，128倍或者更高倍数的降采样（会损失局部信息降低准确率），也很少见到 2倍或者4倍的降采样比例（空间不变性太弱，泛化能力不好）。不过这个是跟数据集中的图像大小有关的，上述主流图像分类网络基本都是针对于 ImageNet数据集做分类的，ImageNet中的图片都比较大，一般在 256×256 左右。如果数据集中的图像本来就很小，那么降采样比例就也会小，比如 MNIST数据集，图像只有28×28，所以LeNet中的降采样比例是4。总之，降采样比例要根据数据集调整，找到合适的降采样比例，才能保证准确率的情况下，有较强的空间不变性。 那么如何在保证准确率的情况下，即不损失局部信息的前提下，增强网络的空间不变性呢？这篇文章就是为了解决这个问题。 1.2 解决方法对于CNN 来说，即便通过选择合适的降采样比例来保证准确率和空间不变性，但是 池化层 带来的空间不变性是不够的，它受限于预先选定的固定尺寸的池化核（感受野是固定的，局部的）。因为物体的变形包括旋转，平移，扭曲，缩放，混淆噪声等，所以后面feature map中像素点的感受野不一定刚好包含物体或者反映物体的形变。 文章提出了一种 Spatial Transformer Networks，简称 STN，引进了一种可学习的采样模块 Spatial Transformer ，姑且称为空间变换器，Spatial Transformer的学习不需要引入额外的数据标签，它可以在网络中对数据（feature map）进行空间变换操作。这个模块是可微的（后向传播必须），并且可以插入到现有的CNN模型中，使得 feature map具有空间变换能力，也就是说 感受野是动态变化的，feature map的空间变换方向 与 原图片上的目标的空间变换方向（一般认为是数据噪声）是相反的，所以使得整个网络的空间不变性增强。试验结果展示这种方法确实增强了空间不变性，在一些标志性的数据集（benchmark）上取得了先进的水平。 图1 在输入层使用 Spatial Transformer 空说无凭，先看一个简单效果，如图1： (a) ：输入图片 (b) ：框起来的是用于后面网络进行进一步识别分类的部分，这一部分是就是Spatial Transformer的结果 (c) ：输出层的可视化 (d) ：预测结果 整体上来看是一种视觉 attention 机制，也更像一种弱的目标检测机制，就是把图片中物体所在区域送到网络后面的层中，使得后面的分类任务更简单。 CNN是尽力让网络适应物体的形变，而STN是直接通过 Spatial Transformer 将形变的物体给变回到正常的姿态（比如把字摆正），然后再给网络识别。 文章给的 Spatial Transformer 的使用场景： image classification ：如果数据集中的图像上的目标形变很大，噪声很大，位于图片中心较远，那么 Spatial Transformer 可以将物体部分 “剪裁” 出来，并做一定的旋转，缩放变换，使之成为大小统一的图片，便于后续网络识别，并且获得比CNN更好的结果。 co-localisation ：给定输入图片，不确定是否有物体，如果有，可以使用Spatial Transformer做出定位。 spatial attention ：对于使用attention机制的视觉任务，可以很轻松的使用 Spatial Transformer 完成。 看完这篇论文之后，个人觉得目标检测（object detection）也是可以用的，果不其然，真有人将类似的方法用在了 目标检测上，这篇论文就是 Deformable Convolutional Networks ，后面再讲。 2 Spatial Transformer结构文章最重要的一个结构就是 Spatial Transformers ，这个结构的示意图如下： 图2 Spatial Transformers 结构图 这样一个结构相当于 CNN中的一个 卷积层或者池化层： 这个结构又被分为三部分：localisation network ，grid generator和sampler 一些符号意义： $U \in R^{H \times W \times C}$ 为输入 feature map $V \in R^{H’ \times W’ \times C’}$ 为输出 feature map $\theta=f_{loc}(U)$ 是一个回归子网络 $T_{\theta}$ 表示以参数 $\theta$ 为变换矩阵的某种变换，可以是2维仿射变换(2D affine transformation )，平面投影变换(plane projective transformation )，薄板样条变换(thin plate spline ) $ G_i = (x^t_i, y_i^t)$ 代表V中的像素点 $G = \{G_i\} $ 是V中像素点的整体。 $T_{\theta}(G)$ 代表下面图3中，输入U上的绿色区域的坐标。 这个图与图1做个对应，U 相当于 图1 中的 (a) , V相当于 图1 中的(c)，中间那一部分相当于图1 中的(b), 作用就是为了找到那个物体所在的框，或者叫做弱目标检测。 2.1 Localisation network这一部分很简单，可以使用全连接层或者全卷积层，只要保证最后一层是一个回归层即可，最后输出的一个向量是 $\theta$ 。 $\theta$ 的维度下面再说。 2.2 Grid generator前面提到中间那一部分是为了找到那个物体所在的框，并把它给 变换回 “直立的状态”。很自然就能想到使用仿射变换就可以完成，如下图： 图3 (a)恒等变换与采样； (b)仿射变换与采样 我们期望的是输出 V 是 将U中某一部分（比如绿色点覆盖的部分）做了旋转，放缩，平移之后的feature map。 看一下Grid generator是如何进行仿射变换的。 先简单的看一下仿射变换： 仿射变换用于表示旋转，缩放和平移，表示的是两副图之间的关系， 以下 A 为旋转矩阵，B 为平移矩阵，M称为仿射变换矩阵。 假设要对二维向量 进行仿射变换，仿射变换可以写成如下两式，两种写法等价： 输出的结果是： 对于仿射变换来说，一般的用法有两种： 已知 M 和 X，求T; 这个很简单，直接矩阵相乘。 已知 X 和 T , 求M; 可以选取三对点，带入上面的式子中，列方程，6个方程6个未知数； 这里使用的是第一种用法。其中 图3 (b) U 中的被绿点覆盖的那一部分相当于这里的 T，V相当于这里的 X，那不是应该 M也是已知的吗？M哪去了？还记得上面提到的 $\theta$ ？ $\theta$ 就相当于这里的M。因为 M的大小是 2×3 ，所以 $\theta$ 的维度为6。如果使用了别的变换方法，那就根据变换矩阵的大小相应调整。也就是说这里的变换矩阵是学习出来的。 对应于图3的变换公式如下： $(x^t_i, y_i^t)$ are the target coordinates of the regular grid in the output feature map ，代表的是图3输出V中的像素点，即目标像素坐标； $(x^s_i, y_i^s)$ are the source coordinates in the input feature map that define the sample points ,代表的是图3输入U中被绿色点覆盖的像素点，即源像素坐标； $A_{\theta}$ is the affine transformation matrix ，代表的是仿射变换矩阵。其中的成员 $\theta_{ij}$ 由 localisation network 回归生成。图3或图2中的 $T_{\theta}$ 这时指的仿射变换 $A_{\theta}$。 注意他这个仿射变换是 从后向前变换的，就是说这个模块的输出是仿射变换的输入，这个模块的输入的其中一部分（图3(b) 绿点覆盖部分）是仿射变换的输出。 按照一般的做法，应该是从前往后变换，即从 source coordinates 得到 target coordinates 。但是这样做的问题是，如何确定变换的输入？如果是从前往后做变换，U 中绿色部分相当于 X，那怎么确定这一部分是多大，什么形状，位置在哪？ 实际上从后往前变换也就是为了解决这个问题，就是要根据输出V的坐标得到输入U中目标所在的区域的坐标（绿色的区域）。 仿射变换变换的是坐标，既是坐标，那么变换的输入和输出的坐标的参考系应该是一样的，就是说 V 中像素的坐标 和 U 中像素的坐标应该是同一个参考系。这里使用的是针对 宽和高 进行的归一化坐标(height and width normalised coordinates)，把在U和V中的像素坐标归一化到 [-1,1] 之间。U的 尺寸是上一层决定的，V的尺寸是人为固定的，输出 $H’,W’$ 可以分别比 输入$H,W$ 大或者小，或者相等。 可以给仿射变换的变换矩阵添加更多的约束： 这时候，绿色区域已经确定了，相当于V中对应坐标$(x^t_i, y_i^t)$ 的像素都将从U中这块绿色区域中获取。 $H’,W’$ 与$H,W$ 不一定相等；即便是相等，由于变换后的源坐标 $(x^s_i, y_i^s)$ 很有可能不是整数 ，对应U中不是整数像素点，所以没有像素值，没办法直接拷贝。所以V中 $(x^t_i, y_i^t)$ 坐标的像素值如何确定就成了问题。这时就涉及到采样和插值。 2.3 Sampler实际上 CNN中的卷积核 或者 池化核起到的就是采样的作用。 $(x^s_i, y_i^s)$ 是U中绿色区域的坐标，来看看更加具有一般性的采样问题如何描述： $U_{nm}^c$ 是输入feature map上第 $c$ 个通道上坐标为 $(n, m)$ 的像素值； $V_i^c$ 是输出 feature map上第 $c$ 个通道上坐标为 $(x^t_i, y_i^t)$ 的像素值； $k()$ 表示插值核函数； $Φx , Φy$ 代表 x 和 y 方向的插值核函数的参数； $H,W$ 输入U的尺寸; $H’,W’$ 输出 V 的尺寸; 注意上式只是针对一个通道的像素进行采样，实际上每个通道的采样都是一样的，这样可以保留 空间一致性。 卷积的操作也是符合上式的，比如一维卷积： $f(\tau)$ 相当于 $U_{nm}^c$ ； $g(n-\tau)$ 相当于 $k(x_i^s-m; \Phi_x)$ 或 $(y_i^s-m; \Phi_y)$ 因为这里的卷积是 一维的。 理论来说 任意 对 $x^s_i, y_i^s$ 可导或局部可导的采样核函数都是可以使用的. 比如最近邻插值核函数: 其中 $\lfloor x + 0.5\rfloor$ 向下取整 这个插值核函数做的就是把U中 离 当前源坐标 $(x^s_i, y_i^s)$ （小数坐标） 最近的 整数坐标 $(n,m)$ 处的像素值拷贝到V中的 $(x^t_i, y_i^t)$ 坐标处； 不过这篇文章使用的是双线性插值，双线性插值 参考 维基百科 和 图像处理之插值运算，这里放一张示意图吧： 图4 双线性插值（来源于[参考资料 6]） 这里的公式如下： 这个插值核函数做的是利用 U中 离 当前源坐标 $(x^s_i, y_i^s)$ （小数坐标） 最近的 4个整数坐标 $(n,m)$ 处的像素值做双线性插值然后拷贝到V中的 $(x^t_i, y_i^t)$ 坐标处； 我在想他那个通过仿射变换确定绿色区域之后，绿色区域相当于ROI，那采样能不能使用ROI 池化的方式? 2.4 前向传播结合前面的分析，总结一下前向传播的过程，如下图： 实际上首先进行的是 localisation network 的回归，产生 变换矩阵的参数 $\theta$ ，进而resize为 变换矩阵 $T_{\theta}$ ; 根据 V中的 目标坐标 $(x^t_i, y_i^t)$ 做逆向仿射变换变换到源坐标 $(x^s_i, y_i^s)$ ： $Source= T_{\theta} \cdot Target$ ， 源坐标 $(x^s_i, y_i^s)$ 位于U上；对应图中 1,2步； 在U中找到 源坐标 $(x^s_i, y_i^s)$ （小数坐标）附近的四个整数坐标，做双线性插值，插值后的值作为 目标坐标 $(x^t_i, y_i^t)$ 处的像素值；对应图中 3,4步； 图5 前向传播流程（来源于[参考资料 6]） 2.5 梯度流动与反向传播这个函数虽不是 完全可导 但也是局部可导的，求导如下，对 $y_i^s$ 的导数也是类似的： 根据公式(1)很容易求得： $\dfrac{\partial x_i^s}{\partial \theta} $ 和 $\dfrac{\partial y_i^s}{\partial \theta} $ 。 所以反向传播过程，误差可以传播到输入 feature map（公式6），可以传播到 采样格点坐标(sampling grid coordinates )（公式7），还可以传播到变换参数 $\theta$ . 下图是梯度流动的示意图： 图6 反向传播流程（来源于[参考资料 6]） 其中localisation network中的 $\dfrac{\partial x_i^s}{\partial \theta} $ 和 $\dfrac{\partial y_i^s}{\partial \theta} $ 也就是这一股误差流 $\left\{\begin{matrix}\frac{\partial V_{i}^{c}}{\partial x_{i}^{S}}\rightarrow \frac{\partial x_{i}^{S}}{\partial \theta}\ \frac{\partial V_{i}^{c}}{\partial y_{i}^{S}}\rightarrow \frac{\partial y_{i}^{S}}{\partial \theta}\end{matrix}\right.$ ，在定位网络处就断了。 定位网络是一个回归模型，相当于一个子网络，一旦更新完参数，流就断了，独立于主网络。 3 试验3.1 Distorted MNIST这个试验的数据集 是 MNIST，不过与原版的MNIST 不同，这个数据集对图片上的数字做了各种形变操作，比如平移，扭曲，放缩，旋转等。 如下，不同形变操作的简写表示： 旋转：rotation (R), 旋转+缩放+平移：rotation, scale and translation (RTS), 投影变换：projective transformation (P), 弹性变形：elastic warping (E) – note that elastic warping is destructive and can not be inverted in some cases. 文章将 Spatial Transformer 模块嵌入到 两种主流的分类网络，FCN和CNN中（ST-FCN 和 ST-CNN ）。Spatial Transformer 模块嵌入位置在图片输入层与后续分类层之间。 试验也测试了不同的变换函数对结果的影响： 仿射变换：affine transformation (Aff), 投影变换：projective transformation (Proj), 薄板样条变换：16-point thin plate spline transformation (TPS) 其中CNN的模型与 LeNet是一样的，包含两个池化层。为了公平，所有的网络变种都只包含 3 个可学习参数的层，总体网络参数基本一致，训练策略也相同。 试验结果 左侧：不同的形变策略以及不同的 Spatial Transformer网络变种与 baseline的对比； 右侧：一些CNN分错，但是ST-CNN分对的样本 (a)：输入 (b)：Spatial Transformer层 的 源坐标（$T_{\theta}(G)$ ）可视化结果 (c)：Spatial Transformer层输出 很明显：ST-CNN优于CNN, ST-FCN优于FCN，说明Spatial Transformer确实增加了 空间不变性 FCN中由于没有 池化层，所以FCN的空间不变性不如CNN，所以FCN效果不如CNN ST-FCN效果可以达到CNN程度，说明Spatial Transformer确实增加了 空间不变性 ST-CNN效果优于ST-FCN，说明 池化层 确实对 增加 空间不变性很重要 在 Spatial Transformer 中使用 plate spline transformation (TPS) 变换效果是最好的 Spatial Transformer 可以将歪的数字扭正 Spatial Transformer 在输入图片上确定的attention区域很明显利于后续分类层分类，可以更加有效地减少分类损失 作者也做了噪声环境下的试验：将数字 放置在 60×60的图片上，并添加斑点噪声（图1第三行）错误率分别为： FCN ，13.2% error； CNN ， 3.5% error； ST-FCN ，2.0% error； ST-CNN ，1.7% error. 3.2 Street View House NumbersStreet View House Numbers是一个真实的 街景门牌号 数据集，共200k张图片，每张图片包含1-5个数字 ，数字都有形变。 baseline character sequence CNN model ：11层，5个softmax层输出对应位置的预测序列 STCNN Single ：在输入层添加一个Spatial Transformer ST-CNN Multi ：前四层，每一层都添加一个Spatial Transformer 见下面 tabel 2 右侧 localisation networks 子网络：两层32维的全连接层 使用仿射变换和双线性插值 结果： 3.3 Fine-Grained Classification数据集：CUB-200-2011 birds dataset， 6k training images and 5.8k test images, covering 200 species of birds. baseline CNN model ： an Inception architecture with batch normalisation pre-trained on ImageNet and fine-tuned on CUB – which by itself achieves the state-of-the-art accuracy of 82.3% (previous best result is 81.0% [30]). ST-CNN, which contains 2 or 4 parallel spatial transformers, parameterised for attention and acting on the input image. 这里使用了并行的Spatial Transformer ， 效果是可以将图片的不同 部分（part）输入到不同的 Spatial Transformer 层，会产生不同的 part representations 然后经过 inception ，最后再合并起来，经过一个单独的softmax层做分类。 结果： ST-CNN效果最好 右侧上边使用了 2 路 Spatial Transformer并行，可以看到其中一个 spatial transformer(红色) 检测的是鸟的头部, 而另外一个 (绿色) 检测的鸟的身体. 右侧下边使用了 4 路 Spatial Transformer并行，有相似的效果. 此处的Spatial Transformer有点像目标检测的味道 3.4 MNIST Addition这个试验是将任意两张MNIST中的数字独立的进行一系列变形，然后叠加到一块，给网络识别，标签是二者之和。 同样的测试 FCN, CNN, ST-CNN,2×ST-CNN。 2×ST-CNN在输入层使用了两个并行的Spatial Transformer，结构见下面table 4右侧。 由于数据比较复杂，FCN的效果很差，添加了 Spatial Transformer之后，错误率显著下降 CNN有池化层存在所以效果比FCN好 2×ST-CNN效果最好 从右边可视化的图中可以看到虽然每个输入channel都输入到了两个Spatial Transformer中，但是每个Spatial Transformer都是对其中一个channel作用强，而且这两个Spatial Transformer是互补的，所以最后连接起来之后 4个通道的feature map中有两个是完整的数字，所以识别较为有效。 3.5 Co-localisation这个试验将 Spatial Transformer用在了半监督的任务Co-localisation 。 Co-localisation ：给一些图片，假设这些图片包含一些目标（也可能不包含），在不使用目标类别标签和目标位置标签的情况下，定位出常见的目标。 数据集还是 MNIST ，将 28×28大小的 数字图像 随机的放在 84×84 大小的含有噪声的背景上，对每个数字产生100个不同的变形。数据有定位标签，但是在训练时不用，测试时用。 模型还是使用 LeNet CNN模型，在输入层嵌入Spatial Transformer。 文章使用了半监督的方式，监督的学习过程是这样的： 对于一个 包含 N 张图片的 数据集 $I = \{I_n\}$ ，比如table 5 右侧的图。 最下面一行代表在同类别的样本中挑选一张 $I_n$ 做一个随机裁剪，裁剪出的这一块 $I_n^{rand}$ ,认为是目标位置 中间这一行代表将同样的样本 $I_n$输入 Spatial Transformer，输出 $I_n^{T}$ 上面这行代表另选一个样本 $I_m$ 输入Spatial Transformer，输出 $I_m^{T}$ 以上三个输出分别经过一个映射函数 $e()$ ，这个函数由 LeNet CNN模型提供，以便于将上述三个feature map映射成向量，映射成向量后可以计算两两之间的距离 计算 $L1=||e(I_m^T)-e(I_n^T)|_2$ , $L2=||e(I_n^T)-e(I_n^{rand})||_2$ , 训练过程中就是要保证 L1 &lt; L2，L2是一个随机裁剪与经过Spatial Transformer的输出之间的距离，理应大于L1. 经过上面的分析，可以提出如下损失函数: hinge loss (triplet loss) $α$ is a margin ,可以称为 裕度，相当于净赚多少。 半监督是因为这里的标签相当于 L2，而L2是人为构造出来的距离指标。 测试时认为检测出的box与ground truth bounding box的IOU 大于0.5为正确，table5 左侧为测试结果。 在没有噪声时，可以达到100%的准确率，有噪声时在75-93%之间。 下图是优化过程的动态可视化结果，可见随着迭代次数越来越多，模型对目标的定位越来越准确 这个试验使用了一种简单的损失函数，在不使用数据定位标签的情况下，构建了一种距离标签，实现了对目标的检测。这个可以推广到目标检测或追踪问题中去。 作者把前面一些检测的动态效果做成了视频，看起来很清晰明了，看这里：https://goo.gl/qdEhUu 4 总结这篇文章提出的 Spatial Transformer 结构能够很方便的嵌入到现有的CNN模型中去，并且实现端到端（end-to-end）的训练，通过对数据进行反向空间变换来消除图片上目标的变形，从而使得分类网络的识别更加简单高效。现在的CNN的已经非常强大了，但是 Spatial Transformer 仍然能过通过增强空间不变性来提高性能表现。Spatial Transformer实际上是一种attention机制，可以用于目标检测，目标追踪等问题，还可以构建半监督模。 下一篇介绍 Deformable Convolutional Networks ，跟本篇的TSN思路很像，但是又比这个模型简单。 参考资料 opencv中文教程——仿射变换 仿射变换与齐次坐标 知乎——如何通俗易懂的理解高维仿射变换与线性变换 维基百科——双线性插值 图像处理之插值运算 讲STN的一篇博客，不过关于仿射变换那一块写的是错的，但是其中的图还是挺不错的，借用几张图]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>basic dl models</tag>
        <tag>论文笔记</tag>
        <tag>STN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-从RCNN到Mask RCNN两步检测算法总结]]></title>
    <url>%2Farticles%2Ftarget-detection---summary-of-two-step-detection-algorithm-from-rcnn-to-mask-rcnn.html</url>
    <content type="text"><![CDATA[目标检测中两步检测算法的总结对比，持续更新…… R-CNN, SPPNet, Fast R-CNN, Faster R-CNN, R-FCN, Light-head R-CNN, Mask R-CNN R-CNNRbg提出的R-CNN的方法 一张图像先通过selective search的方法，生成1K~2K个候选区域，这个步骤生成的候选区域大小是不一样的，因此需要 warped region，也就是将不同大小的 region 缩放到同样的尺寸，因为CNN后面的全连接层要求输入尺寸固定。 对每个 warped 后的候选区域，使用CNN提取特征 ，提取的特征需存储到磁盘； 读取特征，送入每一类的 SVM 分类器，判别是否属于该类； 最终还有一个位置回归器用于精细修正。 回归方法： region proposals 与 ground truth之间的逻辑关系 可以推出回归目标（也就是标签） 回归函数： 损失函数： 训练方法： 优点： 结构简单明了，容易理解； CNN自动提取特征，省去手工设计特征的复杂操作，以及对经验和运气的依赖性； 使用 selective search方法来生成候选区域，显著减少候选区域的数量，增加候选区域的质量（包含目标的可能性更大），因为这相当于一个弱检测器，相比于sliding window穷举搜索的方式肯定要好很多； 预测精度提高了30%。 缺点： 提取特征时，CNN需要在每一个候选区域上跑一遍；候选区域之间的交叠使得特征被重复提取, 造成了严重的速度瓶颈, 降低了计算效率; 将候选区域直接缩放到固定大小, 破坏了物体的长宽比, 可能导致物体的局部细节损失; R-CNN还不是端到端的模型，训练步骤繁琐multi-stage（先预训练、fine tuning、存储CNN提取的特征；再训练SVM ；再regression）。从fine tuning 到训练SVM时，不能一步到位，要分成两步； 训练SVM时需要将之前CNN提取到的特征全部存储在磁盘上，磁盘读写耗时，且占用空间大，（Pascal 200G）； 使用额外的selective search 算法生成候选区域的过程也很耗时； 预测时间很慢，一张图片要49s。 SPPNet针对R-CNN的两个缺陷： 先生成候选区域，再对区域进行卷积，候选区域之间的交叠使得特征被重复提取, 造成了严重的速度瓶颈, 降低了计算效率; 将候选区域直接缩放到固定大小, 破坏了物体的长宽比, 可能导致物体的局部细节损失; 何凯明提出的改进方法如下： 改变生成候选区域的顺序 SPP池化 改变生成候选区域的顺序。从先生成候选区域再提取特征，变成先提取特征再生成候选区域，实现了特征提取部分的计算共享，极大的减少计算量。生成候选区域的方式还是 selective search算法，在原图上生成候选区域后，映射到特征图上去。 使用SPP池化（Spatial pyramid model, 空间金字塔池化池化）：传统的池化方式是 已知输入尺寸和 固定池化核大小，确定输出尺寸，那么这时候输出尺寸肯定是随输入尺寸变化的，所以这时候就要求输入图片是固定尺寸。而SPP池化是 已知输入尺寸 和 固定输出尺寸，来确定确定池化核的大小。SPP 层用不同大小的池化窗口作用于卷积得到的特征图，池化窗口的大小和步长根据特征图的尺寸进行动态计算，最终可以组合成一个特定维度的特征输出。这里的输入可以是一个feature map（分类问题），也可以是一个window（检测问题）。 训练过程： 优点： SPP-net 对于一幅图像的所有候选区域, 只需要进行一次卷积过程, 避免了重复计算, 显著提高了计算效率。该方法在速度上比 R-CNN 提高 24 ~102 倍 . SPP池化层使得检测网络可以处理任意尺寸的图像, 因此可以采用多尺度图像来训练网络, 从而使得网络对目标的尺度有很好的鲁棒性. 缺点： SPP-net 的训练过程更复杂了，（先预训练、存储SPP特征、使用SPP特征fine tuning全连接层、存储CNN提取的特征；再训练SVM ；再regression）。 CNN 提取的特征存储需要的空间和时间开销增大; 在微调阶段, SPP-net 只能更新空间金字塔池化层后的全连接层, 而不能更新卷积层(好像是梯度不连续), 这限制了检测性能的提升。 Fast R-CNN 为什么 SPPnet和 R-CNN训练很慢？ 主要原因有两点 使用SVM做分类器时，需要将特征事先存储到磁盘上，磁盘交互耗时； 训练步骤繁琐，不能联合训练。 针对 R-CNN 和 SPPNet 的这两个问题, rbg 提出 能够端到端联合训练的 Fast R-CNN 算法 ，如下： 首先在图像中提取感兴趣区域 (Regions of Interest, RoI)，还是使用selective search算法，生成的候选区域这里称为ROI，将ROI映射到feature map上; 然后采用与 SPP-net 相似的处理方式,对每幅图像只进行一次卷积, 在最后一个卷积层输出的特征图上对每个 RoI 进行映射, 得到相应的RoI 的特征图, 并送入 RoI 池化层 (相当于单层的SPP 层, 通过该层把各尺寸的特征图统一到相同的大小); 最后经过全连接层得到两个输出向量, 一个进行 Softmax 分类, 另一个进行边框回归. SPP池化 ROI池化 改进的方法： 串行结构改成并行结构 ：原来的 R-CNN 是先对候选框区域进行分类，判断有没有物体，如果有则对 Bounding Box 进行精修 回归 。这是一个串联式的任务，那么势必没有并联的快，所以 rbg 就将原有结构改成并行，在分类的同时，对 Bbox 进行回归； ROI池化：在这个模型里，ROI就是感兴趣区域(Regions of Interest, RoI) ，也就是之前模型中的候选区域。SPP池化的改进，相当于只用了一种尺寸的 SPP池化； 不用SVM分类，改用SoftMax分类，可以省去特征存储； 使用multi-task loss 多任务损失函数（分类+回归），端到端（end-to-end）训练。 训练方式： multi-task 损失函数： 新的挑战及解决方法： ROI池化： 一句话概括作用：将不同尺寸输入的feature map或者 ROI，降采样成固定尺寸的输出 feature map，再送入全连接层。 做法： 将image中的ROI 映射到feature map 中对应位置的区域，这些区域大小是不统一的（已知输入）； 将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同，固定输出尺寸）；这个过程中，每个ROI分好的section中的像素数量是不一样的（池化核的大小和步长根据输入和输出尺寸进行动态计算），最终可以组合成一个特定维度的特征输出； 对每个sections进行max pooling操作； 参考：Region of interest pooling explained 优点： 精度有提高 通过使用 multi-task loss ，可以实现 end-to-end训练，single-stage，除了预训练之外，其他的都是可以一气呵成的。 分类和回归任务可以共享卷积特征,相互促进. Fast R-CNN 采用 Softmax 分类与边框回归一起进行训练, 省去了特征存储, 提高了空间和时间利用率。 与 R-CNN 相比, 在训练 VGG 网络时,Fast R-CNN 的训练阶段快 9 倍, 测试阶段快 213倍; 与 SPP-net 相比, Fast R-CNN 的训练阶段快 3倍, 测试阶段快 10 倍。 缺点： Fast R-CNN 仍然存在速度上的瓶颈, 就是候选区域生成步骤耗费了整个检测过程的大量时间. Faster R-CNN为了解决候选区域生成步骤消耗大量计算资源, 导致检测速度过慢的问题, 任少卿，何凯明，rbg联合提出区域生成网络 (Region proposal network, RPN), 并且把RPN 和 Fast R-CNN 融合到一个统一的网络 (称为 Faster R-CNN), 二者共享卷积特征. 如下： RPN 将一整幅图像作为输入, 输出一系列的矩形候选区域. 它是一个全卷积网络模型, 通过在与 Fast R-CNN 共享卷积层的最后一层输出的特征图上滑动一个小型网络（sliding window）, 这个网络与特征图上的小窗口全连接, 每个滑动窗口映射到一个低维的特征向量, 再输入给两个并列的全连接层, 即分类层 (cls layer) 和边框回归层(reg layer), 由于网络是以滑动窗的形式来进行操作, 所以全连接层的参数在所有空间位置是共享的. RPN的结构在实现时实际上是一个全卷积网络。 RPN是一个弱检测器，RPN的输出是一些可能包含目标的候选框（ region proposals 或者称为 region of interest ,ROI）,这些ROI 将会输入Fast R-CNN中，用于最后的检测。 训练方式： Anchor的作用 按照两步检测的惯例，应该要先有初步的ROI ，然后才是最终的分类和回归，对于原版的Fast R-CNN来说，它的ROI是由selective search算法提供的；而Faster R-CNN中的 Fast R-CNN的 ROI 则是由RPN网络产生的。 那么RPN既然是一个弱检测器，那么RPN的ROI或者 region proposals从哪来？ 答案是从 anchor 中来。 结合上面的图，RPN在CNN提取特征之后以sliding window的方式在最后一个feature map上提取特征，每个滑动窗口中心都关联着 k个 box，这些box就称为anchor，或者叫anchor box。这些关联的box 可以通过逆向映射对应到原图上，对应到原图上的区域就是region proposals，不过这些region proposals都是位于同一个中心点。就是说sliding window时的window（大小固定）是由这些原图上的不同大小和比例的 region proposals 生成的（类似于ROI池化的功能）。 实际上sliding window时每个 window 起到了一部分region proposals 的作用，但是由于这里的sliding window的尺寸是固定的，所以不能起到多尺度，多尺寸（multiple scales and sizes ）预测的作用，因此提出关联k个不同大小和长宽比的anchor box，这样二者结合即可起到多尺度，多尺寸预测的作用。参考下图： 使用anchor的好处是，RPN最后sliding window 时可以使用 卷积的方式实现（因为 window的大小是固定的），使网络变得很简单。 而后面的 256维向量的输入 由于用卷积层实现所以也由 （1,1,256），变成了（W,H,256）. 但是我们知道后面的输入是固定尺寸的 window ，那么在分类和回归时是如何来反映不同尺寸和比例的 region proposals呢？答案是通过 标签和损失函数。 以下引用自 [参考资料 1]，在那位大兄弟的博客中，一开始他的理解是对的，但是后面的补充他又给改错了，但是不是什么大错，内容是对的，只是因果关系搞反了，这里只把内容贴出来： 从nxn提出的256d特征是被这k种区域共享的，在clc layer和reg layer计算损失的时候，用这共享的256d特征 加上 anchor推算出k种区域的坐标和前景、背景的标签，便可以对这k种区域同时计算loss。 clc layer和reg layer同时预测k个区域的前景、背景概率（1个区域2个scores，所以是2k个scores），以及bounding box（1个区域4个coordinates，所以是4k个coordinates），具体的说： clc layer输出预测区域的2个参数，即预测为前景的概率pa和pb，损失用softmax loss（cross entropy loss）（本来还以为是sigmoid，这样的话只预测pa就可以了？）。需要的监督信息是Y=0,1，表示这个区域是否ground truth reg layer输出预测区域的4个参数：x,y,w,h，用smooth L1 loss。需要的监督信息是 anchor的区域坐标{xa,ya,wa,ha} 和 ground truth的区域坐标{x,y,w,h} 显然，上面的监督信息：Y，{xa,ya,wa,ha}（k个），{x*,y*,w*,h*}（1个），就是通过anchor机制产生的。这几个参数的指定（比如k个anchor区域的Y是怎么得到的）是根据文章中的样本产生规则，很多博客中也都提到了。 参考资料： faster-rcnn中，对RPN的理解 faster rcnn中rpn的anchor，sliding windows，proposals？ R-FCN如果不考虑生成ROI的部分（比如RPN，Region Proposal Network），两步检测模型可以分为两部分子网络（subnetworks ）： 第一部分是共享计算的全卷积基础子网络 base，或称body，trunk ，这一部分是与ROI独立的，主要用于提取特征 第二部分是不共享计算的子网络 head，生成的每一个ROI都要经过head部分，主要用于分类 Faster R-CNN 实现了很多计算的共享：ROI之间的特征提取共享计算，ROI 提取与base部分共享计算，但是ROI通过head部分是不共享计算。 R-FCN就是基于FCN将 head部分也实现了计算共享。但是由于直接将 Faster R-CNN 的head部分也就是全连接层改为全卷积层，然后再使用 R-FCN是一种新的基于区域的全卷积网络检测方法. 为了给网络引入平移变化, 构建对位置敏感的池化方式 (Position sensitive pooling), 编码感兴趣区域的相对空间位置信息. 该网络解决了 Faster R-CNN 由于重复计算全连接层而导致的耗时问题, 实现了让整个网络中所有的计算都可以共享 。 Position sensitive pooling： ROI Pooling 中的每一个网格都来自前面 position-sensitive score maps中不同组通道的 feature map。这个跟分组卷积的意思有点像，这个可以叫做分组ROI池化。是一种选择性ROI池化，主要是为了增强对位置的敏感程度。 Light-head RCNN不管是 Faster R-CNN还是 R-FCN 在 ROI(Region of Interest) 生成前后都是计算量很大的，比如 Faster R-CNN 的head部分包含两个全连接层用于ROI 分类，而全连接层极大地消耗计算；R-FCN虽然比 Faster R-CNN快许多，但是由于生成的 score maps太多，想要达到实时（30FPS, Frame Per Second）还是有点困难的。也就是说这些模型之所以速度慢的原因在于计算量过于繁重的 head部分 (heavy-head design )，即便是将 base部分削减压缩，计算消耗也不能很大程度的削减。 本文提出了一种新的两步检测模型， Light-Head RCNN ，为了解决现在的两步检测普遍存在的 heavy-head的问题。在本文模型的设计中使用了thin feature map 和 cheap R-CNN subnet (pooling and single fully-connected layer)，是对R-FCN的改进。 改进： 引进inception V3中可分离卷积的思想，作者采用large separable convolution生成channel数更少的feature map(从3969减少到490)。 用FC层代替了R-FCN中的global average pool，避免空间信息的丢失。 参考资料： https://zhuanlan.zhihu.com/p/33158548 Mask R-CNN在fatser rcnn的基础上对ROI添加一个分割的分支，预测ROI当中元素所属分类，使用FCN进行预测； 具体步骤：使用fatser rcnn中的rpn网络产生region proposal（ROI），将ROI分两个分支：（1）fatser rcnn操作，即经过ROI pooling 输入fc进行分类和回归；（2）mask操作，即通过ROIAlign校正经过ROI Pooling之后的相同大小的ROI，然后在用fcn进行预测（分割）。 ROIAlign产生的原因：RoI Pooling就是将原图ROI区域映射到feature map上，最后pooling到固定大小的功能。当把原图上的ROI 映射到 feature map上时，存在归一化或者量化（即取整）的过程。在归一化的过程当中，由于存在多次量化过程（卷积步长，池化），会存在ROI与提取到的特征不对准的现象出现 也就是feature map上的ROI再映射会原图时会跟原来的ROI对不准，由于分类问题对平移问题比较鲁棒，所以影响比较小。但是这在预测像素级精度的掩模时会产生一个非常的大的负面影响。作者就提出了这个概念ROIAlign，使用ROIAlign层对提取的特征和输入之间进行校准。 ROI Align的思路很简单：取消量化操作，使用双线性插值的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作，。值得注意的是，在具体的算法操作上，ROI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如 图 所示： 遍历每一个候选区域，保持浮点数边界不做量化。 将候选区域分割成k x k个单元，每个单元的边界也不做量化。 在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。 这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。 参考资料： 详解 ROI Align 的基本原理和实现细节]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>R-CNN</tag>
        <tag>Mask R-CNN</tag>
        <tag>学习总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-R-FCN-论文笔记]]></title>
    <url>%2Farticles%2Ftarget-detection--r-fcn--paper-notes.html</url>
    <content type="text"><![CDATA[这篇文章提出了一种高效简洁的目标检测模型 R-FCN(Region-based Fully Convolutional Networks )，作者将FCN(Fully Convolutional Network)应用于 Faster R-CNN，实现了整个网络的计算共享，极大的提高了 检测速度，同时精度也可以与 Faster R-CNN相媲美。文章提出一种 position-sensitive score maps 用来平衡平移不变性(translation-invariance )和平移可变性(translation-variance )之间的矛盾，模型使用了 全卷积的 ResNet 结构，保证高检测精度的同时，将检测速度提高了 2.5~20倍。 1 从 Faster R-CNN 到 R-FCN目前的目标检测模型一般分为两步检测模型（R-CNN系列）和一步检测模型（YOLO系列和SSD），两步检测精度高，一步检测速度快，各有优势。对于两步检测来说，最成功的模型莫过于R-CNN系列了。 R-CNN系列的检测思路一般是 先生成候选区域 region proposals，或者称为感兴趣区域 ROI (Region of Interest)，然后针对这些 ROI （后面都使用ROI一词）进行分类，也就是说本质上他是把目标检测问题（Object Detection）转化成了分类问题（Classification），不过实际上也是包含回归的，主要用于位置精修，提升准确率。 如果不考虑生成ROI的部分（比如RPN，Region Proposal Network），两步检测模型可以分为两部分子网络（subnetworks ）： 第一部分是共享计算的全卷积基础子网络 base，或称body，trunk ，这一部分是与ROI独立的，主要用于提取特征 第二部分是不共享计算的子网络 head，生成的每一个ROI都要经过head部分，主要用于分类 这个设计拓扑结构实际上是跟经典的用于分类的卷积神经网络很像的，比如用于分类的卷积神经网络前面一部分是用于特征提取的基础网络，后面是用于分类的全连接层，从卷积层到全连接层中间通过一个空间池化层连接。而在目标检测模型中，base与head部分之间使用的是 ROI 池化。 1.1 Faster R-CNN首先回顾一下Faster R-CNN： 最初的 Faster R-CNN的base 部分使用的是VGG的卷积层，head是由全连接层构成的，如图1(a). 后来的使用ResNet网络做base部分的 Faster R-CNN, 它的head是由两部分组成，第一部分是 ResNet 中 的 第5个 卷积block，也就是 Conv5，第二部分是全连接层。这个跟最初提出的 以VGG为特征提取层的 Faster R-CNN不太一样。如图1(b)。 这两种结构，生成的每一个ROI都会经过head部分，head部分是不能共享计算的。这也是 Faster R-CNN速度慢的原因之一，其他的比如 生成的ROI数量等也是影响速度的因素，这里不讨论。 (a) Head of Faster R-CNN - VGG (b) Head of Faster R-CNN -ResNet 101 图1 Faster R-CNN的两种结构 实际上 后来的对于图片分类效果比较好的一些网络比如 GoogLeNet，ResNet都是属于全卷积网络的，他们的结构一般都是将一堆按照特定结构排列的卷积层堆在一起，最后一个feature map经过一个全局平均值池化，然后为了使输出与物体类别数（imagenet，C=1000）相等才会加一个 1000层的全连接层，最后加上softmax层。 之前讲过FCN（Fully Convolution Network）, 对于Faster R-CNN来说它的RPN部分已经是全卷积了，那很自然的会想到 以 ResNet 101 实现的 Faster R-CNN 的 head部分 能不能也使用FCN。如果使用FCN的话，最后一个卷积层的输出是一个代表原图的 heatmap，只需要将 ROI 映射到heatmap上，做一个 ROI Pool ， 然后对最后一个卷积层上的ROI区域 做一个全局平均值池化，像下面图2这样，channel=C对应于不同的类别。 这样的话基本上所有的计算量都能够共享，后面每个ROI经过的都是池化层，而池化层与卷积层和全连接层相比计算量大大减少，几乎可以忽略。 图2 一种可行的全卷积方案 但实际上通过实验验证这么做的效果很差，结果如下： 第三行就是 上面的结构，mAP那一栏写着 fail ，不及格，应该是连60 %都没到。这个表在第2节试验部分详细介绍。 这个原因文章中也分析了： 平移不变性 translation invariance 是深层CNN的特点，这对于图像分类问题来说是很好的一个特性，因为图像分类不管你物体在哪个位置都要求较高的准确性。但是对于目标检测来说，不仅要检测出图上有什么，还需要定位出物体在图片上的位置，也就是说目标检测需要 平移可变性 translation variance 。图2的方案中， 不变性与可变性是一对相互矛盾的问题。因为ROI放在Conv5后面的heatmap上，由于网络太深了，原图的变化已经很难反映在heatmap上了。所以它的效果才不好。 这也就是为什么图1 (b)中的 ROI 池化层不是放在 Conv5 输出的feature map后面，而是放在了Conv4和Conv5两组卷积层之间，Conv5和后面的全连接层构成了head部分。一来可以避免位置信息进一步损失，二来可以使用后面的Conv5卷积层学习位置信息。但是缺点就是 每一个ROI都要再经过Conv5这一组卷积，head部分的计算不能实现共享。图1 (b)可能不太清楚，可以看下图： 图？ 基于ResNet 的 Faster R-CNN 而R-FCN就是着重于解决这一问题的，如何做到head部分使用FCN共享计算的同时，准确率不下降甚至是超过原来？ 只要解决这个问题，即便准确率不比原来高，速度也会提升很多。实际上 R-FCN不仅提高了速度，准确率也提高了。下面详细讲解一下。 1.2 R-FCN1.2.1 R-FCN的整体结构 图3 R-FCN 结构图 网络仍然是包含两部分： region proposal region classification 网络使用了ResNet-101，ResNet-101包含100个卷积层，一个全局平均值池化层，和一个1000维的全连接层。本文去掉了最后的全局平均值池化层和全连接层，最后加上了一个1024个 1×1 卷积核的卷积层用以降维（将Conv5输出的2048维降为1024维）。 看起来跟 图2 好像差不多，但其实是很不一样的，不一样的地方在于 R-FCN 改变了 ROI Pooling的使用方法，称为 Position-Sensitive ROI Pooling，位置敏感的ROI池化，简称PS ROI Pooling，这是为了增加平移可变性而引入的结构 。 1.2.2 Position-Sensitive ROI Pooling 图4 Position-Sensitive ROI Pooling 文章设计了一个 position-sensitive score maps ，做法其实很简单就是通过调整Conv5的卷积核数量，使得输出的通道变为： $k^2(C+1)$ 个。也就是说每一个类别都用 $k^2$ 个通道的feature map来表示，图中不同的颜色就代表一组 $C+1$ 通道的 feature map。 $k×k$ 代表ROI池化的网格大小，以 $k=3$ 为例 。图中最后那个由不同颜色组成的 $C+1$ 通道的 k×k 的 feature map 就是使用 Position-Sensitive Pooling得到的。最后一个PS ROI Pooling之后的feature map（9个格点，每个格点颜色都不一样）中的每一个网格都来自前面 position-sensitive score maps中不同组的 feature map，根据颜色一一对应。这个跟分组卷积的意思有点像，这个可以叫做分组ROI池化。原文称为 选择性池化：our position-sensitive RoI layer conducts selective pooling, and each of the k × k bin aggregates responses from only one score map out of the bank of k × k score maps. 最后那个 颜色混合的 feature map 中每一个网格都是对应着不同的位置信息的，比如 {top-left, top-center,top-right, …, bottom-right} 。随着学习过程的深入，对应的position-sensitive score maps中不同组的（不同颜色的）feature map 的激活值就会对目标的不同位置敏感，从而产生对位置敏感的激活。对目标的位置实现编码（见图5，图6）。原文：With end-to-end training, this RoI layer shepherds the last convolutional layer to learn specialized position-sensitive score maps. vote操作其实就是一个全局平均值池化，生成一个 C+1 维的向量，这个向量就可以用于判断当前ROI是什么类别。 也实现了所有的计算量都能够共享，后面每个ROI经过的都是池化层，而池化层与卷积层和全连接层相比计算量大大减少，几乎可以忽略。 实际这个过程就是人为给网络设定一些模式和期望的结果，然后让网络在训练过程中自己完成。就相当于老师丢给你一道很难的竞赛题，只告诉了你答案，但不告诉你怎么做，你要做的就是绞尽脑汁找到解题方法。这里也是一样的，这个对位置敏感，是人为地希望feature map对位置敏感。至于为什么会敏感，是网络在训练过程中学习到的。 文章也给出了一些可视化的图来解释，如图5，图6： 图5 一个正样本的可视化 图6 一个负样本的可视化 可以看到，以 k=3 为例，这9个 position-sensitive score maps的激活值确实是对位置敏感的，而且如果 某一个ROI如果与 ground truth 的IOU 比较大的话，最后的投票结构是yes（图5），否者是no（图6）。 这个与图2 的结构的不同之处在哪呢? 图2 中那个 channe=C+1 的feature map相当于 图4 中 $k^2$ 组feature map中的一组，也就是其中一个颜色。图2中最后ROI池化后的feature map中的每个像素都是来自上一层的同一组feature map，而图4是来自不同组。 就这么一点差别，其他基本上是一样的。但是就这么一点差别，导致 R-FCN可以做到 精度，速度都有提高。 池化时的公式表达： 上式比较抽象，实际上表达的就是 ROI池化，如下图，表示某一组 score map 中的第c个类别的池化： 图7 某一组 score map 中的第c个类别的池化 图8 ROI 池化 将每个ROI都分为 $k×k$ 大小的网格，对于某个 $w×h$ 大小的 ROI，每一个格点的大小约为 $\dfrac{w}{k} × \dfrac{h}{k}$ ，对于最后一个卷积层的 $k^2$ 组score feature map，PS ROI Pooling 之后的feature map上的 $(i, j)$ $(0 ≤ i, j ≤ k - 1)$位置处的格点的像素值也是由上一层的position-sensitive score feature map的对应通道中的 $(i, j)$ 格点中的多个像素值池化得到的（图4中一一对应的颜色块）。 上式： $r_c(i， j)$ 是 $(i, j)$ 格点处第c个类别的响应值或者是池化输出值； $z_{i,j,c}$ 是 score map 中对应组的 $(i, j)$ 格点中的，第c个类别的，多个像素值之一； $(x_0, y_0)$ 是ROI的左上角在图片中的坐标； n 是 score map 中对应组的 $(i, j)$ 格点中的像素总数 $Θ$ 是网络的学习参数； score map 中 $(i, j)$ 位置处的格点中的坐标跨度为： $\lfloor i {w \over k}\rfloor \le x \le \lceil (i+1){w \over k}\rceil$ 和 $\lfloor j {h \over k}\rfloor \le y \le \lceil (j+1){h \over k}\rceil$ ,$\lfloor \cdot \rfloor$ 和 $\lceil \cdot \rceil $ 分别代表下界和上界。 本文中上式使用的是平均值池化，但是最大值池化也是可以的。 PS ROI Pooling之后的feature map再做一个平均值池化，产生一个 C+1 维的向量，然后计算 softmax 回归值： 看上面的公式好像并没有做平均值处理，只是将不同网格的像素值求和。 同样的网络中也使用了回归层用于位置精修，与 $k^2(C+1)$ 个通道的score map并列，另外再使用 $4k^2$ 个通道的score map，同样是 $k^2$ 组，只不过每一组现在变成了4个通道，代表着坐标的四个值。最后PS ROI Pooling之后的feature map是 4×3×3的，平均值池化之后生成一个 4维的向量，代表着 bounding box位置的四个值 $t = (t_x,t_y,t_w,t_h)$ ,分别为中心坐标，和宽高。这里使用的是类别不明确的回归(class-agnostic bounding box regression) ，也就是对于一个ROI只输出 一个 $t$ 向量，然后与分类的结果结合。实际上也可以使用类别明确的回归(class-specific ) ，这种回归方式对一个 ROI 输出 C个 $t$ 向量，也就是说每一类别都输出一个位置向量，这跟分类时每一个类别都输出一个概率是相对的。 1.2.3 训练(Training )损失函数基本与Faster R-CNN是一样的： $c^∗$ 是RoI的 ground-truth label ($c^∗ = 0$ 意味着是背景). $L_{cls}(s_{c^∗}) = -log(s_{c^∗})$ 是用于分类的交叉熵损失； $L_{reg}$ ：是bounding box 回归损失 , $t^∗$ 代表ground truth box. $[c^∗ &gt; 0]$ 是指示函数，括号中表达式为真则为1，否则为0； 设置 $λ = 1$ ； 定义 正样本为 ROI与ground-truth box的IOU (intersection-over-union) 至少为0.5，否则为负样本。 训练方法与参数设置 It is easy for our method to adopt online hard example mining (OHEM) during training. Ournegligible per-RoI computation enables nearly cost-free example mining. We use a weight decay of 0.0005 and a momentum of 0.9. By default we use single-scale training: images are resized such that the scale (shorter side of image) is 600 pixels [6, 18]. Each GPU holds 1 image and selects B = 128 RoIs for backprop. We train the model with 8 GPUs (so the effective mini-batch size is 8×). We fine-tune R-FCN using a learning rate of 0.001 for 20k mini-batches and 0.0001 for 10k mini-batches on VOC. To have R-FCN share features with RPN (Figure 2), we adopt the 4-step alternating training, alternating between training RPN and training R-FCN. 1.2.4 推理(Inference ) an image with a single scale of 600. During inference we evaluate 300 RoIs. The results are post-processed by non-maximum suppression (NMS) using a threshold of 0.3 IoU ， as standard practice. 1.2.5 带孔卷积(Àtrous algorithme )带孔卷积：与FCN一样，本文也将网络的整体降采样步长改为了16。原先ResNet的Conv1-Conv5每一组都是2倍的降采样，总共是32倍降采样。本文中将Conv5的降采样倍数改为了1，提高了分辨率，这样整个结构是16倍的降采样，为了弥补由于降采样倍数改动导致网络后面的卷积层的感受野发生变化，文章使用了 À trous 技巧，也就是带孔卷积，不过带孔卷积只在Conv5中使用，Conv1-Conv4保持原来的不变，因此RPN不受影响。 带孔卷积的详细说明参考 1 2 . 带孔卷积对整个检测结果的影响还是蛮大的，如下是 R-FCN (k × k = 7 × 7, no hard example mining)的结果对比，带孔卷积提高了2.6个百分点。 表？ 带孔卷积对检测结果的影响 2 试验2.1 Pascal VOC试验条件： 类别C=20 训练集：VOC 2007 trainval and VOC 2012 trainval (“07+12”) 测试集：VOC 2007 test set 性能衡量指标：mean Average Precision (mAP) 首先对比一些用于目标检测的 不同全卷积网络设计策略 的结果，以下是一些 不同的全卷积设计策略： Naïve Faster R-CNN. 使用了Conv1-Conv5作为base部分，用于特征提取，ROI直接映射在Conv5最后输出的feature map上，与图2类似，不一样的地方在于：ROI池化之后加了一个 21维的全连接层。使用了 The àtrous 技巧。是一个近似的全卷积网络。 Class-specific RPN. 与标准的Faster R-CNN中使用的RPN类似，RPN训练方法也一样，不一样的在于：RPN部分不是一个二分类，而是一个类别为21的多分类。 为了对比公平，RPN的head使用的是 ResNet-101的 Conv5 层，也使用了àtrous 技巧。注意这个只是Faster R-CNN中的RPN，是一个全卷积网络。 R-FCN without position-sensitivity. 在图4的结构中设置k = 1，就跟图2是一样的，只是ROI池化的尺寸变成了 1×1 ，相当于全局池化。使用了àtrous 技巧。这是一个全卷积网络。 先展示一个 baseline的结果，如表1。这是使用 ResNet101 的标准 Faster R-CNN的测试结果。 我们只关注与本文本节实验条件相同的项，也就是红色方框框起来的那一行。可以看到mAP是 76.4%。标准的 Faster R-CNN 结构我们上面说过，首先这不是一个全卷积网络，其次 ROI pooling位于Conv4 和 Conv5之间， head 部分没有共享计算。 表1 baseline：使用 ResNet101 的标准 Faster R-CNN 以上提到的三种全卷积设计策略的结果对比如表2： 表2 不同全卷积网络设计策略结果对比 Naïve Faster R-CNN的结果最高只有 68.9%，与标准 Faster R-CNN相比，说明 将ROI pooling放置在Conv4 和 Conv5之间，是有效保证位置信息的关键。而深层的全卷积网络损失了位置信息，对位置不敏感。 Class-specific RPN的结果是67.6%，这实际上相当于特殊版本的 Fast R-CNN，相当于在head部分使用了sliding window的稠密检测方法。 R-FCN without position-sensitivity直接fail，不及格，应该是连50%都不到，因为文中提到这种情况下网络不能收敛，也就是说这种情况下，从ROI中提取不到位置信息。这个跟 Naïve Faster R-CNN 的1×1 ROI输出的版本挺像的，不一样的是 Naïve Faster R-CNN中加了全连接层，而且Naïve Faster R-CNN是可以收敛的，只是精度比标准的低很多。 R-FCN的mAP分别为 75.5和76.6，使用 7×7 ROI输出时，超过了标准的 Faster R-CNN。与R-FCN without position-sensitivity相比 ，说明Position-sensitive RoI pooling 起作用了，它可以对位置信息进行编码。 经过以上的试验分析，基本可以确定了 R-FCN with RoI output size 7 ×7 的效果是最好的。 以下的试验中涉及到 R-FCN的都采用这样的设置。 表3 是 Faster F-CNN 与 R-FCN 的 测试结果对比： 表3 Faster R-CNN与R-FCN的对比 depth of per-RoI subnetwork 指的是 head部分的深度，这里Faster F-CNN使用的是 ResNet101 版本的，Conv5有9个卷积层，再加一个全连接层，共10层。而R-FCN的head就是PS ROI Pooling和全局平均值池化，所以深度为0； online hard example mining 是否使用了OHEM策略进行训练，这个策略并不会额外增加训练时间，这个训练方式有待研究。 可以看到 R-FCN不管是训练还是测试都要比 Faster R-CNN快很多，平均精度也部落下风。 很明显的看到，当 ROI 数量是300时，Faster R-CNN训练平均一张图片需要1.5s，R-FCN需要0.45s；而当ROI 数量为300时，Faster R-CNN训练平均一张图片需要2.9s，R-FCN需要0.46s。Faster R-CNN的训练时间受 ROI是数量影响很大，而R-FCN几乎没有影响，只增加了0.01s。这也是使用全卷积，和 PS ROI Pooling 带来的好处。然而增加 ROI 数量并没有良好的精度收益。所以后面的试验中基本上都是使用的是 300个ROI。 更多的测试结果，见表4，表5. 表4 PASCAL VOC 2007测试结果对比 表5 PASCAL VOC 2012测试结果对比 multi-scale training ：resize the image in each training iteration such that the scale is randomly sampled from {400,500,600,700,800} pixels. And still test a single scale of 600 pixels, so add no test-time cost. 使用这个策略后 mAP80.5。 如果先在COCO 数据集（trainval set ）上训练，然后再 PASCAL VOC数据集上 fine-tune，可以获得 83.6%的mAP。但是精度不敌使用了各种技巧的 Faster R-CNN+++。但也不差。Faster R-CNN+++是ResNet论文中提出的。注意R-FCN并没有使用相应的技巧，比如 iterative box regression, context, multi-scale testing. Faster R-CNN+++虽然精度高，但是速度慢，简直被R-FCN 吊打，R-FCN的0.17s比Faster R-CNN+++快20倍。 表5展示了 PASCAL VOC 2012的测试结果，结论基本差不多，R-FCN精度稍逊Faster R-CNN+++，速度吊打Faster R-CNN+++。不过与 Faster R-CNN相比，精度还是高出很多的。 ResNet深度对检测结果的影响见表6： 表6 ResNet深度对检测结果的影响 从50到101，精度是上升的，但是到152就趋于饱和了。 不同的 region proposal 方法对检测精度的影响，见表7： 表7 不同的 region proposal 方法对检测精度的影响 SS: Selective Search , EB: Edge Boxes , RPN最牛。 2.2 MS COCO这一部分是 MS COCO数据集上的测试结果。 试验条件： 总类别数 C=80; 80k train set, 40k val set, and 20k test-dev set； 评价指标：AP@[0.5; 0.95]，指的是阈值在0.5到0.95之间的平均精度（average precise）；AP@0.5，指的是阈值为0.5的平均精度（average precise）。 训练方法，直接把原文的搬过来： We set the learning rate as 0.001 for 90k iterations and 0.0001 for next 30k iterations, with an effective mini-batch size of 8. We extend the alternating training [18] from 4-step to 5-step (i.e., stopping after one more RPN training step), which slightly improves accuracy on this dataset when the features are shared; we also report that 2-step training is sufficient to achieve comparably good accuracy but the features are not shared. 表8 COCO 数据集上的测试结果 multi-scale testing variant following ResNet’s Faster R-CNN, and use testing scales of {200,400,600,800,1000} 结果也是类似的 精度不敌 Faster R-CNN+++，速度吊打Faster R-CNN+++。注意R-FCN并没有使用 iterative box regression, context等技巧。 3 总结文章结合Faster R-CNN和FCN，提出了一个简单高效的网络 R-FCN，可以达到与 Faster R-CNN几乎同等的精度，而速度比Faster R-CNN快2.5-20倍。 从 R-CNN， Fast/er R-CNN 到 R-FCN，改进的路线主要就是为了实现共享计算： R-CNN ：ROI（Region of Interest）直接在原图上提取（使用 seletive search算法），每个ROI都通过 base和head进行计算。每个ROI的特征提取和最终分类都不共享计算。 Fast R-CNN ：ROI直接在原图上提取（使用 seletive search算法），将ROI映射到base部分最后一个卷积层，然后每个ROI就只通过 head 部分，head由几个全连接层构成。ROI之间的特征提取共享计算，ROI生成与base部分不共享计算，ROI通过head部分也不共享计算。 Faster R-CNN ：ROI 通过RPN（Region Proposal Network）提取，RPN与base共享特征提取层，将ROI映射到base最后一个卷积层，然后每个 ROI 只通过 head部分，head由卷积层 和\或 全连接层构成。ROI之间的特征提取共享计算，ROI 提取与base部分共享计算，ROI通过head部分不共享计算。 R-FCN：ROI通过RPN提取，提取之后的ROI仍然只通过一个网络（FCN），实现计算共享，分类层和回归层直接作用于最后一个卷积层。ROI之间的特征提取共享计算，ROI 提取与base部分共享计算，ROI通过head部分共享计算。所有层计算共享。 参考 文章链接：https://arxiv.org/abs/1605.06409 带孔卷积：http://blog.csdn.net/u012759136/article/details/52434826#t9 带孔卷积：https://www.zhihu.com/question/49630217]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
        <tag>R-FCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测-SSD-论文笔记]]></title>
    <url>%2Farticles%2Ftarget-detection--ssd--paper-notes.html</url>
    <content type="text"><![CDATA[论文地址：https://arxiv.org/abs/1512.02325源码：https://github.com/weiliu89/caffe/tree/ssd 这篇文章提出了一个新的目标检测模型SSD，这是一种 single stage 的检测模型，相比于R-CNN系列模型上要简单许多。 其精度可以与Faster R-CNN相匹敌，而速度达到了惊人的59FPS，速度上完爆 Fster R-CNN。 速度快的根本原因在于移除了 region proposals 步骤以及后续的像素采样或者特征采样步骤。（The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage.） 当然作者还是使用了比较多的trick的。 1 Model 网络模型如上图，前面是一个VGG层用于特征提取，与VGG的区别是把FC6和FC7换成了卷积层，SSD在后面又加了8个卷积层。 最终用于预测的是从这些具有金字塔结构的层中选出的特定层，这些层分别对不同scale（scale的平方是面积，这个参数是假设不考虑纵横比的时候，box的边长）和不同aspect ratios（也就是纵横比）的 bounding box进行预测。 bounding box是 detector/classifier 对 default box 进行回归生成的，而 default box 是由一定规则生成的，这里可以认为 default box 比较像 Faster R-CNN 中的RPN生成的region proposal ，也就是两步检测方案中候选框的作用。 实际上更精确的说 default box 是与 RPN 中的 anchor 机制类似的，而 anchor 实际上在 RPN 中 也就是起到了一种region proposals 的作用。 detector/classifier （图中没有显示）对这些 default box 进行关于 类别 和 位置的 回归，然后得出一个类别得分和位置坐标偏移量。根据坐标偏移量可以计算出bounding box的位置，根据得分确定此bounding box里的物体类别（每个类别都包含 8732个 bounding box，大部分都是背景 或者说共有 8732个bounding box，每个bounding box 都对一个 C 维的得分，C为类别总数）。 最后通过NMS（非最大值抑制）过滤掉背景和得分不是很高的框（这个是为了避免重复预测），得到最终的预测。 上图下半部分也展示了SSD与YOLO两种方案的不同，主要有两点： SSD是在多个feature map上进行的多尺度（multi-scale）预测（每个feature map预测一种 scale）。而YOLO是只在一个feature map上进行 多尺度预测。 两种方案的对比还可以从下面两张图中对比，这图是从FPN的论文里找的。 SSD中用于预测的层也就是 detector/classifier 是全卷积的（上图中没有显示detector，后面有图示），而YOLO中是全连接的。 YOLO SSD 上面的模型结构图可能看着不清楚，看下面这个图[参考资料2]，这个是 inference 时 模型从一张图片中提取目标的过程。 再总结一下，网络模型的主要特征： Multi-scale feature maps for detection VGG中的 conv5_3 以及VGG后面又添加的一些层中的某些层，被用来检测和分类。不同的feature layer 预测的bounding box的scale是不一样的，因此不同feature layer上的卷积模型也是不一样的（体现在参数和回归效果上）。 Convolutional predictors for detection 每一个被选中用于预测的feature layer，是用一个 3×3 的卷积层用于预测的，比如说某个feature layer的是 m×n×p 大小的，那么卷积核就是 3×3×p，这也是某一个 detector/classifier的参数量，它的输出是对应 bounding box中类别的得分或者相对于default box的坐标偏移量。对应于 feature map 上每个位置（cell），都会有 k 个 default box（下面再说怎么生成），那么无疑预测的时候要对每个位置上的每个default box都输出类别得分和坐标偏移。 Default boxes and aspect ratios 每一个被选中预测的feature layer ，其每个位置（cell）都关联k个default box，对每个default box都要输出C个类别得分和4个坐标偏移，因此每个default box有（C+4）个输出，每个位置有 (C+4)k 个输出，对于m×n 大小的feature map输出为 (C+4)kmn 个输出，这个机制跟anchor非常相似，具体可以看后面那个图。 2 Training以上是inference 时模型的结构。接下来介绍模型在训练时需要做的工作。 2.1 Matching strategy匹配策略。分类问题中是不需要匹配策略的，这里之所以要使用匹配策略是由于定位问题引入的。可以简单的认为 检测=分类＋定位，这里的定位使用的是回归方法。那么这里SSD中的回归是怎么弄的？ 我们知道平面上任意两个不重合的框都是可以通过对其中一个框进行一定的变换（比如线性变换+对数变换）使二者重合的。 在SSD中，通俗的说就是先产生一些预选的default box（类似于anchor box），然后标签是 ground truth box，预测的是bounding box，现在有三种框，从default box到ground truth有个变换关系，从default box到prediction bounding box有一个变换关系，如果这两个变换关系是相同的，那么就会导致 prediction bounding box 与 ground truth重合，如下图： 所以回归的就是这两个 变换关系： $l_{*}$ 与 $g_{*}$ ，只要二者接近，就可以使prediction bounding box接近 ground truth box 。上面的 $g_{*}$ 是只在训练的时候才有的，inference 时，就只有 $l_{*}$ 了，但是这时候的 $l_{*}$ 已经相当精确了，所以就可以产生比较准确的定位效果。 现在的问题是生成的 default box （下面讲怎么生成）是有很多的，那么势必会导致只有少部分是包含目标或者是与目标重叠关系比较大的，那当然只有这以少部分才是我们的重点观察对象，我们才能把他用到上述提到的回归过程中去。因为越靠近标签的default box回归的时候越容易，如果二者一个在最上边，一个在最下边，那么回归的时候难度会相当大，而且也会更耗时间。 确定这少部分重点观察对象的过程就是匹配策略。原文是这么所的：ground truth information needs to be assigned to specific outputs in the fixed set of detector outputs ，实际上就是确定正样本的过程，这在YOLO，Faster R-CNN ,Multi-box中也都用到了。 做法是计算default box与任意的ground truth box之间的 杰卡德系数（jaccard overlap ），其实就是IOU，只要二者之间的阈值大于0.5，就认为这是个正样本。 2.2 Training objective损失函数，同样的是multi-task loss，即包含了定位和分类。 N是匹配成功的正样本数量，如果N=0，则令 loss=0. $\alpha$ 是定位损失与分类损失之间的比重，这个值论文中提到是通过交叉验证的方式设置为 1 的。 定位损失，这与Faster R-CNN是一样的，都是用的smooth L1 loss： $l$ 代表预测bounding box与default box之间的变换关系， $g$ 代表的是ground truth box与default box之间的变换关系。 $x_{ij}^p=\{0,1\}$ 代表 第 i 个default box 与类别 p 的 第 j 个ground truth box 是否匹配，匹配为1，否则为0； 分类损失，softmax loss，交叉熵损失： 注意一点：定位损失中是只有正样本的损失的，而分类损失中是包含了正样本和负样本的。因为对于定位问题来说，只要回归出精确的变换关系，在预测的时候是不需要区分正负样本的（或者说是依靠分类来区分的），只需要将这个变换关系应用到所有的default box，最后使用NMS过滤掉得分少的样本框即可。但是分类就不一样了，分类是需要区分出正负样本的。 2.3 Choosing scales and aspect ratios for default boxes首先是 default box 的生成规则。 default box 是 bounding box 的初始参考，也就相当于region proposals，那么为了保证预测时对不同面积大小（等同于scale）和不同纵横比（aspect ratio）的目标进行预测，很明显default box也应该是具有不同scale 和 aspect ratio的。RPN中，每个feature map cell所关联的 9 种 anchor box 也是具有不同scale 和aspect ratio的，他们的作用是类似的。 在其他一些目标检测模型中比如 overfeat 和 SPPNet中，为了达到对不同 scale 和 aspect ratio的目标的预测效果，采用的方法是对原始图片进行不同程度的缩放，以达到数据扩增。这种方式可以称为 Featurized image pyramids ，如下图示，实际上每个尺度都是一个独立的网络和预测结果。 这种方式输入尺寸在变化，然而网络中的全连接层要求固定的输入，所以 overfeat 和 SPPNet 为了解决这个问题，分别使用了 offset pooling和SPP pooling的结构，使得从卷积层输入到全连接层的特征维度固定。 SSD的解决方法比较新颖： 还是看图说话，如上图，每个feature layer预测一种scale，feature layer 上每个位置（cell ）的k个default box 的纵横比（aspect ratio）是不同的，这就实现了 multi-scale 预测的方式。 下面这个图就很好的说明了这种方式，(b)和(c)代表不同的预测feature layer，其中的 default box的 scale 是不一样的，同一个 feature layer 中的同一个cell位置处的 default box的 aspect ratio也是不一样的。 一般对于不同的feature map对原图的 感受野（receptive field sizes ）大小是不一样的，然而不同 feature layer上的 default box 与其对应的感受野是不需要对应的，这个不太理解是什么意思。难道是default box的大小与感受野的大小不需要成固定比例？ （Feature maps from different levels within a network are known to have different (empirical) receptive field sizes [13]. Fortunately, within the SSD framework, the default boxes do not necessary need to correspond to the actual receptive fields of each layer. We design the tiling of default boxes so that specific feature maps learn to be responsive to particular scales of the objects. ） 不过文中还提到： An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work. 所以应该说的是 default box与感受野的对应关系。 假设模型在k个feature layer上进行预测，那么每个feature layer上的default box的scale怎么选呢？如下式： $s_{min}=0.2, s_{max}=0.9$ ，也就是说最底层的scale是0.2，最高层的feature layer的scale是0.9. 中间其它层的scale按照公式依次递增。 每个feature layer都负责对不同面积大小的目标进行预测。 但是我们知道目标不仅有大小之分，长宽比也是有变化的，即便两个目标大小是接近的，他们的长宽比（aspect ratio）也有可能是不同的，也即不可能也不应该把所有的目标都当成正方形的形状。那aspect ratio怎么确定呢？ 这里与Faster R-CNN中的解决方法是一样的：也是人为设定几种 aspect ratio，SSD中设定的几种aspect ratio分别是 $\{1,2,3，{1\over2},{1\over3}\}$ ，然后根据scale和aspect ratio 就可以计算出default box真实的宽和高了，如下两式：$$w^a_k=s_k \sqrt{a_r} \\h^a_k=s_k/\sqrt{a_r}$$这样对于feature layer上的每个cell都有5个不同比例的default box，然后作者在 aspect ratio=1 时，又添加了一种scale， 即 $s^{\prime}_{k}=\sqrt{s_k s_{k+1}}$ ,这样一来每个cell就有6个不同的default box。 那么default box的中心坐标怎么确定呢？如下：$$\left( \frac{i+0.5}{\vert f_k \vert}, \frac{j+0.5}{\vert f_k \vert} \right)$$ $|f_k|$ 是第k个square feature map 的大小 $i，j\in [0,|f_k|]$，这个公式是一个归一化坐标，最终会映射到原图上，以确定原图上对应的区域。之所以加0.5是为了把default box的中心设在与之关联的cell的中心，如下图 这样生成一系列default box之后，就有了预测bounding box的初始参考，最终的bounding box会在default box的基础上产生偏移，也就是说由于不同 scale 和 aspect ratio 的default box的存在 会使网络产生很多的预测bounding box，这些预测包含了不同尺寸和形状的目标物体，如上图，在 4×4 的feature map中只有狗（红色框）是正样本，这是因为 不同的feature map 负责预测的 scale和aspect ratio是不同的，所以在 8×8 的feature map中由于猫的scale不匹配，会被认为是负样本。同理，在 8×8 的feature map中只有猫（蓝色框）是正样本。 关于default box的设计方式，文章提到这是一个开放性的研究内容： An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work. 这应该跟上面提到的关于default box与感受野的对齐是同样的问题。 2.4 Hard negative mining由于存在大量的负样本，所以导致严重的类别不平衡问题，训练时难以收敛。 解决方法是：对负样本根据 confidence loss（也就是分类损失）排序，选取损失最高的一部分保留（也就是最不像目标的那些），使得负样本和正样本的比例保持在 3:1，文章使用这个比例获得了稳定的训练性能加快了优化速度。 2.5 Data augmentation为了使模型对于不同尺寸和纵横比的目标更加鲁棒性，使用了数据扩增。 每张训练图像随机进行如下操作： 使用原图 在原图上采样一个patch，这些patch与目标的杰卡德系数为 0.1, 0.3, 0.5, 0.7, 或者 0.9 随机采样一个patch 采样的patch与原图的比例在[0.1-0.9]之间，aspect ratio在 1/2 到 2之间。经过以上采样之后，采样的patch会被resize到固定大小并以0.5的概率进行水平翻转。 这种扩充数据集的方式增强了模型对目标平移的敏感程度。 2.6 训练流程下图是根据SSD的源码绘制的详细模型结构，caffe自带的 draw_net.py 画图工具生成的图片看起来就跟蜘蛛网一样。建议看图跟看prototxt文件相结合，更容易理解。 [参考资料1] 中还有个简化版的图，这里借鉴一下： 3 Inference下面说一下inference的流程，主要参考 [参考资料2]，有些流程在训练时也是类似的。 整体预测流程如下图，一目了然： 上面图中被红框框起来的 detector和classifier的详细结构如下图： 这是其中一个scale的预测流程 可以看出分类和定位都是通过卷积层预测的，假设某个用于预测的feature map是5×5×256的， default box=3，那么用于定位的卷积层输出是 5×5×12 的（4×3=12，每个通道代表一个位置因素（x,y,w,h）），用于分类的卷积层输出是 5×5×63的（21×3=63，每个通道代表一个类别） 每个用于预测的feature map后面可以认为接了三个并行的层，一个default box generator，一个detector，一个classifier。 上面图中的default box generator如下图： detector 和 classifier对某个 bounding box的回归： 所有的bounding box回归之后，是很乱的，然后再通过NMS过滤掉一大部分： 这只是其中某一个feature map的预测结果 当把所有用于预测的feature map进行预测之后（multi-scale），再通过一个NMS，又过滤掉一部分，保留下来的即是最终的结果 4 Experimental Results这部分不同的数据集上训练策略可能有些不同，具体的训练参数不详细说了。 4.1 PASCAL VOC 4.2 Model analysis对照试验： Data augmentation is crucial ：Fast R-CNN和Faster R-CNN使用的数据扩增方式是原图及其水平翻转。本文使用的数据扩增的方式对预测精度的提升特别明显，使用比不使用提高了8.8%。作者估计本文的数据扩增方式对 Fast R-CNN和Faster R-CNN的预测效果提升不会太多，因为Fast R-CNN和Faster R-CNN中使用的 ROI池化本来就对目标的平移很敏感，这比人为扩充数据集鲁棒性更强。 More default box shapes is better ：更多样性化的default box 显然会产生更好的结果。 Atrous is faster ：论文使用了 atrous algorithm ，这是一种称为 带孔卷积的 卷积方式[参考资料4]。 Multiple output layers at different resolutions is better ：做这个对照实验的时候，为了保证公平，删掉某一层的时候，会把这一层的default box挂载在剩下的层上面并调整scale，以保证总的default box数量不变（8732），堆叠default box时会有很多box是在图像的边缘位置，对这种box的处理方式跟Faster R-CNN的一样，就是直接忽略。 结合高层的粗粒度的feature map，比如 conv11_2(1×1)，进行预测时，效果会下降，这是显然的，网络越深，感受野越大，目标如果没有那么大的话，越到后面目标的信息越少，甚至就缺失了。 4.3 MS COCO 这篇论文对比的还是最初版本的Faster RCNN，即VGG16版的，SSD的效果还是要好于Faster RCNN的，不过后来 Faster RCNN用上了 ResNet后，SSD的精度就比不上Faster RCNN了。可能的原因是Faster R-CNN 第一步关注proposal的效果，第二步关注refine的效果, 提取的ROI区域会有一个zoom in的效果，因而会比SSD混杂在一起单步学习精度更高。而且Faster R-CNN对于小目标的预测更有优势。 4.4 Data Augmentation for Small Object Accurac没有使用特征重采样也就是在feature layer 后面再进行特征提取，这使得SSD对于小目标的预测很困难。前面提到的数据扩增的方式小目标的预测效果有很大的提升作用，特别是对于PASCAL这种小数据集来说。随机剪裁相当于对图片上某一部分进行了zoom in（放大）操作，这使得目标可以变得比较大，也会产生较多的大目标样例。但是不能完全没有小目标，为了产生一种 zoom out（缩小）的效果，文章还将原始图片放置于 16 倍原图大小的画布上，周围空间填充图片均值，然后再进行随机裁剪。使用这个方法之后就有了更多的训练数据。效果如下： 这说明这种数据扩增方式对于提升预测效果以及小目标预测效果都是很重要的技巧。 4.5 Inference time We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz Note that about 80% of the forward time is spent on the base network (VGG16 in our case). Therefore, using a faster base network could even further improve the speed, which can possibly make the SSD512 model real-time as well 。 5 可能的改进方向 An alternative way of improving SSD is to design a better tiling of default boxes sothat its position and scale are better aligned with the receptive field of each position on a feature map. We leave this for future work. 没看到有针对这个做改进的。 Note that about 80% of the forward time is spent on the base network (VGG16 in our case). Therefore, using a faster base network could even further improve the speed, which can possibly make the SSD512 model real-time as well 。 比如Mobilenet-SSD，还有一些基于其他框架的轻量级检测模型：SqueezeDet，PVANet，Tiny YOLO，Tiny SSD等。 特征融合方式，现在已经很多人在做了，DSSD, HyperNet，RON, FPN等。 参考资料 SSD: Single Shot MultiBox Detector 模型fine-tune和网络架构 一个很棒的讲解SSD的PPT 一个汇集了很多目标检测模型的博客 知乎：atrous convolution是什么？]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>object detection</tag>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测评价标准-AP&mAP]]></title>
    <url>%2Farticles%2Ftarget-detection-evaluation-standard--ap%26map.html</url>
    <content type="text"><![CDATA[1 mAP简介目标检测里面经常用到的评估标准是：mAP（mean average precision），计算mAP需要涉及到precision 和 recall的计算，mAP，precision，recall的定义含义以及计算方式，网上很多博客都有说明，本文不打算重述。 阅读本文之前，请先仔细阅读如下资料： 周志华老师 《机器学习》 模型评估标准一节，主要是precision，recall的计算方式，或者自己网上搜博客 多标签图像分类任务的评价方法-mAP 通过一个简单的二分类阐述 mAP的含义与计算 average precision 几种不同形式 AP 的计算方式与异同 以博客 多标签图像分类任务的评价方法-mAP 中的数据为例，下面是这个二分类问题的P-R曲线（precision-recall curve），P-R曲线下面与x轴围成的面积称为 average precision。 那么问题就在于如何计算AP，这里很显然可以通过积分来计算$$AP=\int_0^1 P(r) dr$$但通常情况下都是使用估算或者插值的方式计算： approximated average precision$$AP=\sum_{k=1}^N P(k) \Delta r(k)$$ 这个计算方式称为 approximated 形式的，插值计算的方式里面这个是最精确的，每个样本点都参与了计算 很显然位于一条竖直线上的点对计算AP没有贡献 这里N为数据总量，k为每个样本点的索引， $\Delta r(k)=r(k)-r(k-1)$ Interpolated average precision 这是一种插值计算方式：$$P_{interp}(k)=max_{\hat k \ge k} P(\hat k)$$ $$\sum_{k=1}^N P_{interp(k)} \Delta r(k)$$ k 为每一个样本点的索引，参与计算的是所有样本点 $P_{interp}(k)$ 取第 k 个样本点之后的样本中的最大值 这种方式不常用，所以不画图了 插值方式进一步演变：$$P_{interp}(k)=max_{\hat k \ge k} P(\hat k)$$ $$\sum_{k=1}^K P_{interp}(k) \Delta r(k)$$ 这是通常意义上的 Interpolated 形式的 AP，这种形式使用的是比较多的，因为这个式子跟上面提到的计算方式在最终的计算结果上来说是一样的，上面那个式子的曲线跟这里图中阴影部分的外部轮廓是一样的 当一组数据中的正样本有K个时，那么recall的阈值也有K个，k代表阈值索引，参与计算的只有K个阈值所对应的样本点 $P_{interp}(k)$ 取第 k 个阈值所对应的样本点之后的样本中的最大值 再进一步演变：$$P_{interp}(k)=max_{r(\hat k) \ge R(k)} P(\hat k) \quad R \in \{0,0.1,0.2,…,1.0\}$$ $$\sum_{k=1}^K P_{interp}(k) \Delta r(k)$$ 这是通常意义上的 11points_Interpolated 形式的 AP，选取固定的 $\{0,0.1,0.2,…,1.0\}$ 11个阈值，这个在PASCAL2007中有使用 这里因为参与计算的只有11个点，所以 K=11，称为11points_Interpolated，k为阈值索引 $P_{interp}(k)$ 取第 k 个阈值所对应的样本点之后的样本中的最大值，只不过这里的阈值被限定在了 $\{0,0.1,0.2,…,1.0\}$ 范围内。 从曲线上看，真实 AP&lt; approximated AP &lt; Interpolated AP 11-points Interpolated AP 可能大也可能小，当数据量很多的时候会接近于 Interpolated AP 前面的公式中计算AP时都是对PR曲线的面积估计，然后我看到PASCAL的论文里给出的公式就更加简单粗暴了，如下：$$AP=\frac{1}{11} \sum_{r \in \{ 0,0.1,0.2,…,1.0 \}} P_{intep} (r)$$ $$P_{interp}(r)=MAX_{\hat r: \hat r\ge r} P(\hat r)$$ 直接计算11个阈值处的precision的平均值。 不过我看 Itroduction to Modern Information（中译本：王斌《信息检索导论》）一书中也是直接计算平均值的。 对于Interpolated 形式的 AP，因为recall的阈值变化是等差的（或者recall轴是等分的），所以计算面积和直接计算平均值结果是一样的， 对于11points_Interpolated 来说，虽然recall的阈值也是等差的，但是11points计算平均值时会把recall=0那一点的precision算进去，但实际上那一点是人为添加的，所以计算面积和直接计算平均值会有略微差异。 实际上这是一个极限问题，如果recall轴等分且不考虑初始点，那么计算面积和均值的结果是一样的；如果不等分，只有当分割recall无限多的时候，二者无限趋近，这是一个数学问题。 第 4 节的代码包含这两种计算方式，可以用来验证。 以上几种不同形式的 AP 在第4节会有简单的代码实现。 2 PASCAL数据集mAP计算方式 一定要先看这个博客 多标签图像分类任务的评价方法-mAP 。 PASCAL VOC最终的检测结构是如下这种格式的： 比如comp3_det_test_car.txt: 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 每一行依次为 ： 1&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt; 每一行都是一个bounding box，后面四个数定义了检测出的bounding box的左上角点和右下角点的坐标。 在计算mAP时，如果按照二分类问题理解，那么每一行都应该对应一个标签，这个标签可以通过ground truth计算出来。 但是如果严格按照 ground truth 的坐标来判断这个bounding box是否正确，那么这个标准就太严格了，因为这是属于像素级别的检测，所以PASCAL中规定当一个bounding box与ground truth的 IOU&gt;0.5 时就认为这个框是正样本，标记为1；否则标记为0。这样一来每个bounding box都有个得分，也有一个标签，这时你可以认为前面的文件是这样的，后面多了一个标签项： 12345000004 0.702732 89 112 516 466 1000006 0.870849 373 168 488 229 0000006 0.852346 407 157 500 213 1000006 0.914587 2 161 55 221 0000008 0.532489 175 184 232 201 1 进而你可以认为是这样的，后面的标签实际上是通过坐标计算出来的 12345000004 0.702732 1000006 0.870849 0000006 0.852346 1000006 0.914587 0000008 0.532489 1 这样一来就可以根据前面博客中的二分类方法计算AP了。但这是某一个类别的，将所有类别的都计算出来，再做平均即可得到mAP. 3 COCO数据集AP计算方式COCO数据集里的评估标准比PASCAL 严格许多 COCO检测出的结果是json文件格式，比如下面的： 123456789101112131415[ &#123; "image_id": 139, "category_id": 1, "bbox": [ 431.23001, 164.85001, 42.580002, 124.79 ], "score": 0.16355941 &#125;, …… ……] 我们还是按照前面的形式来便于理解： 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 前面提到可以使用IOU来计算出一个标签，PASCAL用的是 IOU&gt;0.5即认为是正样本，但是COCO要求IOU阈值在[0.5, 0.95]区间内每隔0.05取一次，这样就可以计算出10个类似于PASCAL的mAP，然后这10个还要再做平均，即为最后的AP，COCO中并不将AP与mAP做区分，许多论文中的写法是 AP@[0.5:0.95]。而COCO中的 AP@0.5 与PASCAL 中的mAP是一样的。 4 代码简单实现 一定要先看这个博客 多标签图像分类任务的评价方法-mAP 。 计算AP的代码上，我觉得可以看看sklearn关于AP计算的源码，必要时可以逐步调试以加深理解。 sklearn的average_precision_score API，average_precision_score 源码 sklearn上的一个计算AP的例子 Precision-Recall and average precision compute 另外PASCAL和COCO都有公开的代码用于评估标准的计算 PASCAL development kit code and documentation COCO github 下面是仿照sklearn上的计算AP的例子写的一个简单的代码，与sklearn略有差异并做了一些扩展，这个代码可以计算 approximated，interpolated，11point_interpolated形式的AP，sklearn的API只能计算approximated形式的AP。这几个形式的AP的差异，参考 average precision 这个博客。PASCAL2007的测量标准用的 11point_interpolated形式，而 PASCAL2010往后使用的是 interpolated 形式的。 从计算的精确度上 approximated &gt; interpolated &gt; 11point_interpolated，当然最精确的是积分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248# coding=utf-8from sklearn import svm, datasetsfrom sklearn.model_selection import train_test_splitimport matplotlib.pyplot as pltimport numpy as npdef preprocess_iris_data(): iris = datasets.load_iris() X = iris.data y = iris.target # Add noisy features random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)] # 鸢尾花数据集有三类，取其中两类 # Limit to the two first classes, and split into training and test X_train, X_test, y_train, y_test = train_test_split(X[y &lt; 2], y[y &lt; 2], test_size=.5, random_state=random_state) # Create a simple classifier classifier = svm.LinearSVC(random_state=random_state) classifier.fit(X_train, y_train) y_score = classifier.decision_function(X_test) return y_test, y_scoredef precision_recall_curve(y_true, y_score, pos_label=None): if pos_label is None: pos_label = 1 # 不同的排序方式，其结果也会有略微差别， # 比如 kind="mergesort" 的结果跟kind="quicksort"的结果是不同的， # 这是因为归并排序是稳定的，快速排序是不稳定的，sklearn中使用的是 kind="mergesort" desc_score_indices = np.argsort(y_score, kind="quicksort")[::-1] y_score = y_score[desc_score_indices] y_true = y_true[desc_score_indices] # 确定阈值下标索引，score中可能会有重复的分数，在sklearn中，重复的元素被去掉了 # 本来以为去除重复分数会影响结果呢，因为如果两个样本有重复的分数，一个标签是1，一个是0， # 感觉去掉哪个都会影响结果啊，但是实验发现竟然不影响结果，有点纳闷，以后有时间再分析吧 # distinct_value_indices = np.where(np.diff(y_score))[0] # threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1] # 这里没有去除重复分数 threshold_idxs = np.arange(y_score.size) # 按照阈值依次降低的顺序，确定当前阈值下的true positives 个数，tps[-1]对应于所有的正例数量 tps = np.cumsum(y_true * 1.)[threshold_idxs] # 计算当前阈值下的 false positives 个数， # 它与 tps的关系为fps=1+threshold_idxs-tps，这个关系是比较明显的 fps = 1 + threshold_idxs - tps # y_score[threshold_idxs]把对应下标的阈值取出 thresholds = y_score[threshold_idxs] precision = tps / (tps + fps) recall = tps / tps[-1] # 这里与sklearn有略微不同，即样本点全部输出，令last_ind = tps.size，即可 last_ind = tps.size sl = slice(0, last_ind) return np.r_[1, precision[sl]], np.r_[0, recall[sl]], thresholds[sl]def average_precision_approximated(y_true, y_predict): """ 计算approximated形式的ap，每个样本点的分数都是recall的一个cutoff :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) average_precision = np.sum(np.diff(recall) * np.array(precision)[1:]) return precision, recall, thresholds, average_precisiondef average_precision_interpolated(y_true, y_predict): """ 计算interpolated形式的ap，每个正样本对应的分数都是recalll的一个cutoff :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) # 获取recall轴上的分割，np.insert(recall, 0 , -1, axis=0)是为了保证获取到重复recall的第一个索引值 # 因为重复的recall中只有对应的第一个precision是最大的，我们只需要获取这个最大的precision # 或者说每遇到一个正样本，需要将其对应的recall值作为横轴的切分 recall_cutoff_index = np.where( np.diff(np.insert(recall, 0, -1, axis=0)))[0] # 从recall的cutoff 索引值开始往后获取precision最大值，相同的precision只取索引值最大的那个 # P(r) = max&#123;P(r')&#125; | r'&gt;=r precision_cutoff_index = [] for index in recall_cutoff_index: precision_cutoff_index.append( max([x for x in np.where(precision == np.max(precision[index:]))[0] if x &gt;= index])) # interpolated_idx=np.unique(interpolated_cutoff) # 从原始的precision和recall中截取对应索引的片段，即可得到 interpolated 方式下的precision，recall以及AP precision_interpolated = precision[precision_cutoff_index] recall_interpolated = recall[recall_cutoff_index] # 以上获得的 recall_cutoff_index 和 precision_cutoff_index 切片包含人为添加的0 和 1（为了画图时与坐标轴封闭） # 而计算thresholds_interpolated时要去掉相应索引值的影响 # 阈值不包括recall=0 thresholds_interpolated = thresholds[ [x - 1 for x in recall_cutoff_index if 0 &lt;= x - 1 &lt; thresholds.size]] # 按说ap计算应该按照面积的方式计算，也就是下面被注释的部分，但论文里面是直接计算均值， # 这里也直接计算均值，因为阈值不包括recall=0，所以这种情况下二者结果是一样的 average_precision = np.mean(precision_interpolated[1:]) # average_precision = np.sum( # np.diff(recall_interpolated) * np.array(precision_interpolated)[1:]) return precision_interpolated, recall_interpolated, thresholds_interpolated, average_precisiondef average_precision_11point_interpolated(y_true, y_predict): """ 计算 11point形式的 ap :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) recall_11point_cutoff = np.array( [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) # 计算离11个cutoff最近的样本点 recall_cutoff_index = [] for cutoff in recall_11point_cutoff: recall_cutoff_index.append(np.where(recall &gt;= cutoff)[0][0]) precision_cutoff_index = [] for index in recall_cutoff_index: precision_cutoff_index.append( max([x for x in np.where(precision == np.max(precision[index:]))[0] if x &gt;= index])) precision_11point = precision[precision_cutoff_index] recall_11point = recall[recall_cutoff_index] # 此处阈值包括recall=0，因为是11points thresholds_11point = thresholds[ [x - 1 for x in recall_cutoff_index if -1 &lt;= x - 1 &lt; thresholds.size]] # 此处阈值包括recall=0，因为是11points，所以这种情况下两种计算AP的方式结果不同，有略微差别 average_precision = np.mean(precision_11point) # average_precision = np.sum( # 0.1 * np.array(precision_11point)[1:]) # 此处直接返回 recall_11point_cutoff，实际上返回 recall_11point 也是可以的， # 差别就是图线的转折点不在[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]这11个刻度处 # return precision_11point, recall_11point, thresholds_11point, average_precision return precision_11point, recall_11point_cutoff, thresholds_11point, average_precisiondef main(data=None): y_test = [] y_score = [] if data is None: # 一个简单的示例，这个示例直接给定了y_test和y_score y_test = np.array([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]) y_score = np.array([0.23, 0.76, 0.01, 0.91, 0.13, 0.45, 0.12, 0.03, 0.38, 0.11, 0.03, 0.09, 0.65, 0.07, 0.12, 0.24, 0.1, 0.23, 0.46, 0.08]) if data == 'iris': # 还可以导入鸢尾花数据集并构建一个简单的SVM分类器，通过一个完整的模型来理解 PR曲线的绘制 # 使用鸢尾花数据集 y_test, y_score = preprocess_iris_data() # 计算AP，并画图 precision_approximated, recall_approximated, _, ap_approximated = \ average_precision_approximated(y_test, y_score) precision_interpolated, recall_interpolated, _, ap_interpolated = \ average_precision_interpolated(y_test, y_score) precision_11point, recall_11point, _, ap_11point = \ average_precision_11point_interpolated(y_test, y_score) print('Approximated average precision-recall score: &#123;0:0.5f&#125;'.format( ap_approximated)) print('Interpolated average precision-recall score: &#123;0:0.5f&#125;'.format( ap_interpolated)) print('Interpolated at fixed 11 points average precision-recall score: &#123;0:0.5f&#125;'.format( ap_11point)) # print the AP plot fig1 = plt.figure('fig1') # plt.subplot(311) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.step(recall_approximated, precision_approximated, color='c', where='pre') plt.fill_between(recall_approximated, precision_approximated, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Approximated): AP=&#123;0:0.5f&#125;'.format( ap_approximated)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', 'Approximated-PR-curve', 'Approximated-AP'), loc='upper right') fig2 = plt.figure('fig2') # plt.subplot(312) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.plot(recall_interpolated, precision_interpolated, color='c', marker='o', mec='g', ms=3, alpha=0.5) plt.fill_between(recall_interpolated, precision_interpolated, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Interpolated): AP=&#123;0:0.5f&#125;'.format( ap_interpolated)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', 'Interpolated-PR-curve', 'Interpolated-AP'), loc='upper right') fig3 = plt.figure('fig3') # plt.subplot(313) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.plot(recall_11point, precision_11point, color='c', marker='o', mec='g', ms=3) plt.fill_between(recall_11point, precision_11point, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Interpolated_11point): AP=&#123;0:0.5f&#125;'.format( ap_11point)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', '11point-PR-curve', '11point-AP'), loc='upper right') # plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5, # wspace=0.35) plt.show()if __name__ == '__main__': # main() # 用这个测试博客 “多标签图像分类任务的评价方法-mAP” 中的简单例程 main('iris') 这个例子使用了 鸢尾花数据集中的两类构建了一个SVM分类器，然后对分类结果计算AP. 最终的结果如下所示： 参考资料 周志华老师 《机器学习》 多标签图像分类任务的评价方法-mAP average precision 王斌译 《信息检索导论》 论文： The PASCAL Visual Object Classes (VOC) Challenge 2007 和 2012 http://cocodataset.org/#detection-eval http://host.robots.ox.ac.uk/pascal/VOC/ http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>评价标准</tag>
        <tag>mAP</tag>
        <tag>wheel making</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测数据集MSCOCO简介]]></title>
    <url>%2Farticles%2Fa-brief-introduction-to-the-target-detection-data-set-mscoco.html</url>
    <content type="text"><![CDATA[简介介绍一下目标检测领域另外一个比较有名的数据集 MS COCO (Microsoft COCO: Common Objects in Context) . MSCOCO 数据集是微软构建的一个数据集，其包含 detection, segmentation, keypoints等任务。 MSCOCO主要是为了解决detecting non-iconic views of objects（对应常说的detection）, contextual reasoning between objects and the precise 2D localization of objects（对应常说的分割问题） 这三种场景下的问题。 下面是iconic 图片和 non-iconic 图片之间的对比。 与PASCAL COCO数据集相比，COCO中的图片包含了自然图片以及生活中常见的目标图片，背景比较复杂，目标数量比较多，目标尺寸更小，因此COCO数据集上的任务更难，对于检测任务来说，现在衡量一个模型好坏的标准更加倾向于使用COCO数据集上的检测结果。 数据集的构建过程不说了。主要关注一下统计信息 1 统计信息MSCOCO总共包含91个类别，每个类别的图片数量如下： 图中也标出了PASCAL VOC的统计数据作为对比。 下图展示的是几个不同数据集的总类别数量，以及每个类别的总实例数量，一个实例就是图片上的一个目标，主要关注一下 PASCAL 和 ImageNet。 COCO数据集的类别总数虽然没有 ImageNet 中用于detection的类别总数多，但是每个类别的实例目标总数要比PASCAL和ImageNet都要多。 下图是每张图片上的类别数量或者实例数量的分布，括号中为平均值 PASCAL和ImageNet中，每张图片上的类别或者实例数量普遍都很少。 以PASCAL为例：有多于70%的图片上都只有一个类别，而多于50%的图片上只有一个实例或者目标。PASCAL数据集平均每张图片包含1.4个类别和2.3个实例目标，ImageNet也仅有1.7和3.0个。 COCO数据集平均每张图片包含 3.5个类别和 7.7 个实例目标，仅有不到20%的图片只包含一个类别，仅有10%的图片包含一个实例目标。 COCO数据集不仅数据量大，种类和实例数量也多。从这角度来说 SUN 数据集这两个指标更高一点，但是这个数据集在目标检测里面并不常用。 实例目标的分布 COCO数据集中的小目标数量占比更多 关于数据集的划分，COCO的论文里是这么说的， The 2014 release contains 82,783 training, 40,504 validation, and 40,775 testing images (approximately 1/2 train, 1/4 val, and /4 test). There are nearly 270k segmented people and a total of 886k segmented object instances in the 2014 train+val data alone. The cumulative 2015 release will contain a total of 165,482 train, 81,208 val, and 81,434 test images. 2014年的数据在官网是可以下载的，但是2015年只有test部分，train和val部分的数据没有。另外2017年的数据并没有什么新的图片，只是将数据重新划分，train的数据更多了，如下： 2 评估标准COCO的测试标准比PASCAL VOC更严格： PASCAL 中在测试mAP时，是在IOU=0.5时测的 COCO中的AP 是指在 10个IOU层面 以及 80个类别层面 的平均值 COCO的主要评价指标是AP，指 IOU从0.5到0.95 每变化 0.05 就测试一次 AP，然后求这10次测量结果的平均值作为最终的 AP AP@0.5 跟PASCAL VOC中的mAP是相同的含义 AP@0.75 跟PASCAL VOC中的mAP也相同，只是IOU阈值提高到了0.75，显然这个层面更严格，精度也会更低 IOU越高，AP就越低，所以最终的平均之后的AP要比 AP@0.5 小很多，这也就是为什么COCO的AP 超过 50%的只有寥寥几个而已，因为超过50%太难了。而且由于COCO数据集本身数据的复杂性，所以目前的 AP@0.5 最高也只有 73% 。 COCO数据集还针对 三种不同大小（small，medium，large） 的图片提出了测量标准，COCO中包含大约 41% 的小目标 ($area &lt; 32×32$), 34% 的中等目标 ($32×32 &lt; area &lt; 96×96$), 和 24% 的大目标 ($area &gt; 96×96$). 小目标的AP是很难提升的。 除了AP之外，还提出了 AR 的测量标准 跟AP是类似的。 COCO提供了一些代码，方便对数据集的使用和模型评估 ：https://github.com/cocodataset/cocoapi 3 总结为什么COCO的检测任务那么难？ 图片大多数来源于生活中，背景更复杂 每张图片上的实例目标个数多，平均每张图片7.7个 小目标更多 评估标准更严格 所以现在大家更倾向于使用COCO来评估模型的质量。 参考资料 COCO官网：http://cocodataset.org 论文：Microsoft COCO: Common Objects in Context 论文：What makes for effective detection proposals?]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>dataset</tag>
        <tag>MSCOCO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测数据集PASCAL VOC简介]]></title>
    <url>%2Farticles%2Fa-brief-introduction-to-the-target-detection-data-set-pascal-voc.html</url>
    <content type="text"><![CDATA[简介PASCAL VOC挑战赛 （The PASCAL Visual Object Classes ）是一个世界级的计算机视觉挑战赛, PASCAL全称：Pattern Analysis, Statical Modeling and Computational Learning，是一个由欧盟资助的网络组织。 很多优秀的计算机视觉模型比如分类，定位，检测，分割，动作识别等模型都是基于PASCAL VOC挑战赛及其数据集上推出的，尤其是一些目标检测模型（比如大名鼎鼎的R CNN系列，以及后面的YOLO，SSD等）。 PASCAL VOC从2005年开始举办挑战赛，每年的内容都有所不同，从最开始的分类，到后面逐渐增加检测，分割，人体布局，动作识别（Object Classification 、Object Detection、Object Segmentation、Human Layout、Action Classification）等内容，数据集的容量以及种类也在不断的增加和改善。该项挑战赛催生出了一大批优秀的计算机视觉模型（尤其是以深度学习技术为主的）。 我们知道在 ImageNet挑战赛上涌现了一大批优秀的分类模型，而PASCAL挑战赛上则是涌现了一大批优秀的目标检测和分割模型，这项挑战赛已于2012年停止举办了，但是研究者仍然可以在其服务器上提交预测结果以评估模型的性能。 虽然近期的目标检测或分割模型更倾向于使用MS COCO数据集，但是这丝毫不影响 PASCAL VOC数据集的重要性，毕竟PASCAL对于目标检测或分割类型来说属于先驱者的地位。对于现在的研究者来说比较重要的两个年份的数据集是 PASCAL VOC 2007 与 PASCAL VOC 2012，这两个数据集频频在现在的一些检测或分割类的论文当中出现。 PASCAL主页 与 排行榜 （榜上已几乎看不到传统的视觉模型了，全是基于深度学习的） PASCAL VOC 2007 挑战赛主页 与 PASCAL VOC 2012 挑战赛主页 与 PASCAL VOC Evaluation Server. 以及在两个重要时间点对 PASCAL VOC挑战赛 成绩进行总结的两篇论文 The PASCAL Visual Object Classes Challenge: A Retrospective Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.International Journal of Computer Vision, 111(1), 98-136, 2015Bibtex source | Abstract | PDF 主要总结PASCAL VOC 2012的数据集情况，以及2011年-2013年之间出现的模型及其性能对比 The PASCAL Visual Object Classes (VOC) ChallengeEveringham, M., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.International Journal of Computer Vision, 88(2), 303-338, 2010Bibtex source | Abstract | PDF 主要总结PASCAL VOC 2007的数据集情况，以及2008年之前出现的模型及其性能对比 不过在以上论文中出现的深度学习模型只有一个 R-CNN吧，大部分都是传统方式的模型，毕竟深度学习模型主要在14年以后才大量涌现。 本文也是以PASCAL VOC 2007 和 2012 为例简要介绍VOC数据集的结构。 1 数据集整体概况1.1 层级结构PASCAL VOC 数据集的20个类别及其层级结构： 从2007年开始，PASCAL VOC每年的数据集都是这个层级结构 总共四个大类：vehicle,household,animal,person 总共20个小类，预测的时候是只输出图中黑色粗体的类别 数据集主要关注分类和检测，也就是分类和检测用到的数据集相对规模较大。关于其他任务比如分割，动作识别等，其数据集一般是分类和检测数据集的子集。 1.2 发展历程与使用方法简要提一下在几个关键时间点数据集的一些关键变化，详细的请查看PASCAL VOC主页 。 2005年：还只有4个类别： bicycles, cars, motorbikes, people. Train/validation/test共有图片1578 张，包含2209 个已标注的目标objects. 2007年 ：在这一年PASCAL VOC初步建立成一个完善的数据集。类别扩充到20类，Train/validation/test共有9963张图片，包含24640 个已标注的目标objects. 07年之前的数据集中test部分都是公布的，但是之后的都没有公布。 2009年：从这一年开始，通过在前一年的数据集基础上增加新数据的方式来扩充数据集。比如09年的数据集是包含了08年的数据集的，也就是说08年的数据集是09年的一个子集，以后每年都是这样的扩充方式，直到2012年；09年之前虽然每年的数据集都在变大（08年比07年略少），但是每年的数据集都是不一样的，也就是说每年的数据集都是互斥的，没有重叠的图片。 2012年：从09年到11年，数据量仍然通过上述方式不断增长，11年到12年，用于分类、检测和person layout 任务的数据量没有改变。主要是针对分割和动作识别，完善相应的数据子集以及标注信息。 对于分类和检测来说，也就是下图所示的发展历程，相同颜色的代表相同的数据集： 分割任务的数据集变化略有不同： VOC 2012用于分类和检测的数据包含 2008-2011年间的所有数据，并与VOC2007互斥。 VOC 2012用于分割的数据中train+val包含 2007-2011年间的所有数据，test包含2008-2011年间的数据，没有包含07年的是因为07年的test数据已经公开了。 2012年是最后一次挑战赛，最终用于分类和检测的数据集规模为：train/val ：11540 张图片，包含 27450 个已被标注的 ROI annotated objects ；用于分割的数据集规模为：trainval：2913张图片，6929个分割，用于其他任务的不再细说，参考这里 。 即便挑战赛结束了，但是研究者们仍然可以上传预测结果进行评估。上传入口： PASCAL VOC Evaluation Server. 目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。 论文中针对 VOC2007和VOC2012 的具体用法有以下几种： 只用VOC2007的trainval 训练，使用VOC2007的test测试 只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用 使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12 ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。 使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 07++12 ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12+COCO 。 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 07++12+COCO，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。 在各自数据集上分别进行建模和评测的用法比较少，基本上在早期论文里出现就是起个对照作用；现在的大部分论文都会为了增加数据量而将二者合起来使用。 2 数据量统计由于现在的研究基本上都是在VOC2007和VOC2012上面进行，因此只介绍这两个年份的。 2.1 VOC 2007一些示例图片展示：Classification/detection example images 数据集总体统计： 以上是数据集总体的统计情况，这个里面是包含了测试集的，可见person 类是最多的。 训练集，验证集，测试集划分情况 PASCAL VOC 2007 数据集分为两部分：训练和验证集trainval，测试集test ，两部分各占数据总量的约 50%。其中trainval 又分为训练集和测试集，二者分别各占trainval的50%。 每张图片中有可能包含不只一个目标object。 这里我就只贴出用于分类和检测的划分情况，关于分割或者其他任务的划分方式 点击这里查看 。 2.2 VOC 2012一些示例图片展示：Classification/detection example images 数据集总体统计 这个统计是没有包含 test部分的，仍然是person类最多 trainval部分的数据统计： test部分没有公布，同样的 除了分类和检测之外的数据统计，参考这里 2.3 VOC 2007 与 2012 的对比VOC 2007 与 2012 数据集及二者的并集 数据量对比 黑色字体所示数字是官方给定的，由于VOC2012数据集中 test 部分没有公布，因此红色字体所示数字为估计数据，按照PASCAL 通常的划分方法，即 trainval 与test 各占总数据量的一半。 3 标注信息数据集的标注还是很谨慎的，有专门的标注团队，并遵从统一的标注标准，参考 guidelines 。 标注信息是用 xml 文件组织的如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;annotation&gt; &lt;folder&gt;VOC2007&lt;/folder&gt; &lt;filename&gt;000001.jpg&lt;/filename&gt; &lt;source&gt; &lt;database&gt;The VOC2007 Database&lt;/database&gt; &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt; &lt;image&gt;flickr&lt;/image&gt; &lt;flickrid&gt;341012865&lt;/flickrid&gt; &lt;/source&gt; &lt;owner&gt; &lt;flickrid&gt;Fried Camels&lt;/flickrid&gt; &lt;name&gt;Jinky the Fruit Bat&lt;/name&gt; &lt;/owner&gt; &lt;size&gt; &lt;width&gt;353&lt;/width&gt; &lt;height&gt;500&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;dog&lt;/name&gt; &lt;pose&gt;Left&lt;/pose&gt; &lt;truncated&gt;1&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;48&lt;/xmin&gt; &lt;ymin&gt;240&lt;/ymin&gt; &lt;xmax&gt;195&lt;/xmax&gt; &lt;ymax&gt;371&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;person&lt;/name&gt; &lt;pose&gt;Left&lt;/pose&gt; &lt;truncated&gt;1&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;8&lt;/xmin&gt; &lt;ymin&gt;12&lt;/ymin&gt; &lt;xmax&gt;352&lt;/xmax&gt; &lt;ymax&gt;498&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; filename ：文件名 source，owner：图片来源，及拥有者 size：图片大小 segmented：是否分割 object：表明这是一个目标，里面的内容是目标的相关信息 name：object名称，20个类别 pose：拍摄角度：front, rear, left, right, unspecified truncated：目标是否被截断（比如在图片之外），或者被遮挡（超过15%） difficult：检测难易程度，这个主要是根据目标的大小，光照变化，图片质量来判断 difficult 标签示例：图中白色虚线，被标记为 difficult。 bndbox：bounding box 的左上角点和右下角点的4个坐标值。 4 提交格式4.1 Classification Task每一类都有一个txt文件，里面每一行都是测试集中的一张图片，前面一列是图片名称，后面一列是预测的分数。 comp1_cls_test_car.txt: 12345000004 0.702732000006 0.870849000008 0.532489000018 0.477167000019 0.112426 4.2 Detection Taskcomp3_det_test_car.txt: 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 每一类都有一个txt文件，里面每一行都是测试集中的一张图片，每行的格式按照如下方式组织 1&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt; confidence 用来计算 mAP. 5 评估标准PASCAL的评估标准是 mAP(mean average precision) 关于mAP不再详细解释，参考以下资料： 性能指标（模型评估）之mAP average precision 周志华老师 《机器学习》 模型评估标准一节 这里简单的提一下： 下面是一个二分类的P-R曲线（precision-recall curve），对于PASCAL来说，每一类都有一个这样的 P-R曲线，P-R曲线下面与x轴围成的面积称为 average precision，每个类别都有一个 AP, 20个类别的AP 取平均值就是 mAP。 PASCAL官方给了评估脚本mAP的脚本和示例代码 development kit code and documentation ，是用MATLAB写的。 6 数据集组织结构数据集的下载: 123456789# Download the data.cd $HOME/datawget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar# Extract the data.tar -xvf VOCtrainval_11-May-2012.tartar -xvf VOCtrainval_06-Nov-2007.tartar -xvf VOCtest_06-Nov-2007.tar 或者支直接点击下面链接下载： Download the training/validation data (450MB tar file) Download the annotated test data (430MB tar file) 上面的解压命令会将VOC2007的trainval和test解压到一块，数据会混在一起，如果不想，可以自己指定解压路径。以VOC 2007 为例，解压后的文件： 123456.├── Annotations 进行detection 任务时的 标签文件，xml文件形式├── ImageSets 存放数据集的分割文件，比如train，val，test├── JPEGImages 存放 .jpg格式的图片文件├── SegmentationClass 存放 按照class 分割的图片└── SegmentationObject 存放 按照 object 分割的图片 Annotations 文件夹： 12345678910.├── 000001.xml├── 000002.xml├── 000003.xml├── 000004.xml………………├── 009962.xml└── 009963.xml 以xml 文件的形式，存放标签文件，文件内容如前述，文件名与图片名是一样的，6位整数 ImageSets文件夹： 存放数据集的分割文件 包含三个子文件夹 Layout，Main，Segmentation，其中Main文件夹存放的是用于分类和检测的数据集分割文件，Layout文件夹用于 person layout任务，Segmentation用于分割任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798.├── Layout│ ├── test.txt│ ├── train.txt│ ├── trainval.txt│ └── val.txt├── Main│ ├── aeroplane_test.txt│ ├── aeroplane_train.txt│ ├── aeroplane_trainval.txt│ ├── aeroplane_val.txt│ ├── bicycle_test.txt│ ├── bicycle_train.txt│ ├── bicycle_trainval.txt│ ├── bicycle_val.txt│ ├── bird_test.txt│ ├── bird_train.txt│ ├── bird_trainval.txt│ ├── bird_val.txt│ ├── boat_test.txt│ ├── boat_train.txt│ ├── boat_trainval.txt│ ├── boat_val.txt│ ├── bottle_test.txt│ ├── bottle_train.txt│ ├── bottle_trainval.txt│ ├── bottle_val.txt│ ├── bus_test.txt│ ├── bus_train.txt│ ├── bus_trainval.txt│ ├── bus_val.txt│ ├── car_test.txt│ ├── car_train.txt│ ├── car_trainval.txt│ ├── car_val.txt│ ├── cat_test.txt│ ├── cat_train.txt│ ├── cat_trainval.txt│ ├── cat_val.txt│ ├── chair_test.txt│ ├── chair_train.txt│ ├── chair_trainval.txt│ ├── chair_val.txt│ ├── cow_test.txt│ ├── cow_train.txt│ ├── cow_trainval.txt│ ├── cow_val.txt│ ├── diningtable_test.txt│ ├── diningtable_train.txt│ ├── diningtable_trainval.txt│ ├── diningtable_val.txt│ ├── dog_test.txt│ ├── dog_train.txt│ ├── dog_trainval.txt│ ├── dog_val.txt│ ├── horse_test.txt│ ├── horse_train.txt│ ├── horse_trainval.txt│ ├── horse_val.txt│ ├── motorbike_test.txt│ ├── motorbike_train.txt│ ├── motorbike_trainval.txt│ ├── motorbike_val.txt│ ├── person_test.txt│ ├── person_train.txt│ ├── person_trainval.txt│ ├── person_val.txt│ ├── pottedplant_test.txt│ ├── pottedplant_train.txt│ ├── pottedplant_trainval.txt│ ├── pottedplant_val.txt│ ├── sheep_test.txt│ ├── sheep_train.txt│ ├── sheep_trainval.txt│ ├── sheep_val.txt│ ├── sofa_test.txt│ ├── sofa_train.txt│ ├── sofa_trainval.txt│ ├── sofa_val.txt│ ├── test.txt│ ├── train_test.txt│ ├── train_train.txt│ ├── train_trainval.txt│ ├── train.txt│ ├── train_val.txt│ ├── trainval.txt│ ├── tvmonitor_test.txt│ ├── tvmonitor_train.txt│ ├── tvmonitor_trainval.txt│ ├── tvmonitor_val.txt│ └── val.txt└── Segmentation ├── test.txt ├── train.txt ├── trainval.txt └── val.txt 3 directories, 92 files 主要介绍一下Main文件夹中的组织结构，先来看以下这几个文件： 12345├── Main│ ├── train.txt 写着用于训练的图片名称 共2501个│ ├── val.txt 写着用于验证的图片名称 共2510个│ ├── trainval.txt train与val的合集 共5011个│ ├── test.txt 写着用于测试的图片名称 共4952个 里面的文件内容是下面这样的：以train.txt文件为例 123456789101112131415000012000017000023000026000032000033000034000035000036000042…………009949009959009961 就是对数据库的分割，这一部分图片用于train，其他的用作val，test等。 Main中剩下的文件很显然就是每一类别在train或val或test中的ground truth，这个ground truth是为了方便classification 任务而提供的；如果是detection的话，使用的是上面的xml标签文件。 1234567├── Main│ ├── aeroplane_test.txt 写着用于训练的图片名称 共2501个，指定正负样本│ ├── aeroplane_train.txt 写着用于验证的图片名称 共2510个，指定正负样本│ ├── aeroplane_trainval.txt train与val的合集 共5011个，指定正负样本│ ├── aeroplane_val.txt 写着用于测试的图片名称 共4952个，指定正负样本………… 里面文件是这样的（以aeroplane_train.txt为例）： 123456789101112131415000012 -1000017 -1000023 -1000026 -1000032 1000033 1000034 -1000035 -1000036 -1000042 -1…………009949 -1009959 -1009961 -1 前面一列是训练集中的图片名称，这一列跟train.txt文件中的内容是一样的，后面一列是标签，即训练集中这张图片是不是aeroplane，是的话为1，否则为-1. 其他所有的 (class)_(imgset).txt 文件都是类似的。 (class)_train 存放的是训练使用的数据，每一个class都有2501个train数据。 (class)_val 存放的是验证使用的数据，每一个class都有2510个val数据。 (class)_trainval 将上面两个进行了合并，每一个class有5011个数据。 (class)_test 存放的是测试使用的数据，每一个class有4952个test数据。 所有文件都 指定了正负样本，每个class的实际数量为正样本的数量，train和val两者没有交集。 VOC2012 的数据集组织结构是类似的，不一样的地方在于VOC2012 中没有 test类的图片和以及相关标签和分割文件，因为这部分数据 VOC2012没有公布。 参考资料 http://host.robots.ox.ac.uk/pascal/VOC/ 性能指标（模型评估）之mAP 多标签图像分类任务的评价方法-mAP average precision]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>dataset</tag>
        <tag>PASCAL VOC</tag>
      </tags>
  </entry>
</search>
