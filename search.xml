<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[目标检测评价标准-AP&mAP]]></title>
    <url>%2Farticles%2Ftarget-detection-evaluation-standard--ap%26map.html</url>
    <content type="text"><![CDATA[1 mAP简介目标检测里面经常用到的评估标准是：mAP（mean average precision），计算mAP需要涉及到precision 和 recall的计算，mAP，precision，recall的定义含义以及计算方式，网上很多博客都有说明，本文不打算重述。 阅读本文之前，请先仔细阅读如下资料： 周志华老师 《机器学习》 模型评估标准一节，主要是precision，recall的计算方式，或者自己网上搜博客 多标签图像分类任务的评价方法-mAP 通过一个简单的二分类阐述 mAP的含义与计算 average precision 几种不同形式 AP 的计算方式与异同 以博客 多标签图像分类任务的评价方法-mAP 中的数据为例，下面是这个二分类问题的P-R曲线（precision-recall curve），P-R曲线下面与x轴围成的面积称为 average precision。 那么问题就在于如何计算AP，这里很显然可以通过积分来计算$$AP=\int_0^1 P(r) dr$$但通常情况下都是使用估算或者插值的方式计算： approximated average precision$$AP=\sum_{k=1}^N P(k) \Delta r(k)$$ 这个计算方式称为 approximated 形式的，插值计算的方式里面这个是最精确的，每个样本点都参与了计算 很显然位于一条竖直线上的点对计算AP没有贡献 这里N为数据总量，k为每个样本点的索引， $\Delta r(k)=r(k)-r(k-1)$ Interpolated average precision 这是一种插值计算方式：$$P_{interp}(k)=max_{\hat k \ge k} P(\hat k)$$ $$\sum_{k=1}^N P_{interp(k)} \Delta r(k)$$ k 为每一个样本点的索引，参与计算的是所有样本点 $P_{interp}(k)$ 取第 k 个样本点之后的样本中的最大值 这种方式不常用，所以不画图了 插值方式进一步演变：$$P_{interp}(k)=max_{\hat k \ge k} P(\hat k)$$ $$\sum_{k=1}^K P_{interp}(k) \Delta r(k)$$ 这是通常意义上的 Interpolated 形式的 AP，这种形式使用的是比较多的，因为这个式子跟上面提到的计算方式在最终的计算结果上来说是一样的，上面那个式子的曲线跟这里图中阴影部分的外部轮廓是一样的 当一组数据中的正样本有K个时，那么recall的阈值也有K个，k代表阈值索引，参与计算的只有K个阈值所对应的样本点 $P_{interp}(k)$ 取第 k 个阈值所对应的样本点之后的样本中的最大值 再进一步演变：$$P_{interp}(k)=max_{r(\hat k) \ge R(k)} P(\hat k) \quad R \in \{0,0.1,0.2,…,1.0\}$$ $$\sum_{k=1}^K P_{interp}(k) \Delta r(k)$$ 这是通常意义上的 11points_Interpolated 形式的 AP，选取固定的 $\{0,0.1,0.2,…,1.0\}$ 11个阈值，这个在PASCAL2007中有使用 这里因为参与计算的只有11个点，所以 K=11，称为11points_Interpolated，k为阈值索引 $P_{interp}(k)$ 取第 k 个阈值所对应的样本点之后的样本中的最大值，只不过这里的阈值被限定在了 $\{0,0.1,0.2,…,1.0\}$ 范围内。 从曲线上看，真实 AP&lt; approximated AP &lt; Interpolated AP 11-points Interpolated AP 可能大也可能小，当数据量很多的时候会接近于 Interpolated AP 前面的公式中计算AP时都是对PR曲线的面积估计，然后我看到PASCAL的论文里给出的公式就更加简单粗暴了，如下：$$AP=\frac{1}{11} \sum_{r \in \{ 0,0.1,0.2,…,1.0 \}} P_{intep} (r)$$ $$P_{interp}(r)=MAX_{\hat r: \hat r\ge r} P(\hat r)$$ 直接计算11个阈值处的precision的平均值。 不过我看 Itroduction to Modern Information（中译本：王斌《信息检索导论》）一书中也是直接计算平均值的。 对于Interpolated 形式的 AP，因为recall的阈值变化是等差的（或者recall轴是等分的），所以计算面积和直接计算平均值结果是一样的， 对于11points_Interpolated 来说，虽然recall的阈值也是等差的，但是11points计算平均值时会把recall=0那一点的precision算进去，但实际上那一点是人为添加的，所以计算面积和直接计算平均值会有略微差异。 实际上这是一个极限问题，如果recall轴等分且不考虑初始点，那么计算面积和均值的结果是一样的；如果不等分，只有当分割recall无限多的时候，二者无限趋近，这是一个数学问题。 第 4 节的代码包含这两种计算方式，可以用来验证。 以上几种不同形式的 AP 在第4节会有简单的代码实现。 2 PASCAL数据集mAP计算方式 一定要先看这个博客 多标签图像分类任务的评价方法-mAP 。 PASCAL VOC最终的检测结构是如下这种格式的： 比如comp3_det_test_car.txt: 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 每一行依次为 ： 1&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt; 每一行都是一个bounding box，后面四个数定义了检测出的bounding box的左上角点和右下角点的坐标。 在计算mAP时，如果按照二分类问题理解，那么每一行都应该对应一个标签，这个标签可以通过ground truth计算出来。 但是如果严格按照 ground truth 的坐标来判断这个bounding box是否正确，那么这个标准就太严格了，因为这是属于像素级别的检测，所以PASCAL中规定当一个bounding box与ground truth的 IOU&gt;0.5 时就认为这个框是正样本，标记为1；否则标记为0。这样一来每个bounding box都有个得分，也有一个标签，这时你可以认为前面的文件是这样的，后面多了一个标签项： 12345000004 0.702732 89 112 516 466 1000006 0.870849 373 168 488 229 0000006 0.852346 407 157 500 213 1000006 0.914587 2 161 55 221 0000008 0.532489 175 184 232 201 1 进而你可以认为是这样的，后面的标签实际上是通过坐标计算出来的 12345000004 0.702732 1000006 0.870849 0000006 0.852346 1000006 0.914587 0000008 0.532489 1 这样一来就可以根据前面博客中的二分类方法计算AP了。但这是某一个类别的，将所有类别的都计算出来，再做平均即可得到mAP. 3 COCO数据集AP计算方式COCO数据集里的评估标准比PASCAL 严格许多 COCO检测出的结果是json文件格式，比如下面的： 123456789101112131415[ &#123; "image_id": 139, "category_id": 1, "bbox": [ 431.23001, 164.85001, 42.580002, 124.79 ], "score": 0.16355941 &#125;, …… ……] 我们还是按照前面的形式来便于理解： 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 前面提到可以使用IOU来计算出一个标签，PASCAL用的是 IOU&gt;0.5即认为是正样本，但是COCO要求IOU阈值在[0.5, 0.95]区间内每隔0.05取一次，这样就可以计算出10个类似于PASCAL的mAP，然后这10个还要再做平均，即为最后的AP，COCO中并不将AP与mAP做区分，许多论文中的写法是 AP@[0.5:0.95]。而COCO中的 AP@0.5 与PASCAL 中的mAP是一样的。 4 代码简单实现 一定要先看这个博客 多标签图像分类任务的评价方法-mAP 。 计算AP的代码上，我觉得可以看看sklearn关于AP计算的源码，必要时可以逐步调试以加深理解。 sklearn的average_precision_score API，average_precision_score 源码 sklearn上的一个计算AP的例子 Precision-Recall and average precision compute 另外PASCAL和COCO都有公开的代码用于评估标准的计算 PASCAL development kit code and documentation COCO github 下面是仿照sklearn上的计算AP的例子写的一个简单的代码，与sklearn略有差异并做了一些扩展，这个代码可以计算 approximated，interpolated，11point_interpolated形式的AP，sklearn的API只能计算approximated形式的AP。这几个形式的AP的差异，参考 average precision 这个博客。PASCAL2007的测量标准用的 11point_interpolated形式，而 PASCAL2010往后使用的是 interpolated 形式的。 从计算的精确度上 approximated &gt; interpolated &gt; 11point_interpolated，当然最精确的是积分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248# coding=utf-8from sklearn import svm, datasetsfrom sklearn.model_selection import train_test_splitimport matplotlib.pyplot as pltimport numpy as npdef preprocess_iris_data(): iris = datasets.load_iris() X = iris.data y = iris.target # Add noisy features random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)] # 鸢尾花数据集有三类，取其中两类 # Limit to the two first classes, and split into training and test X_train, X_test, y_train, y_test = train_test_split(X[y &lt; 2], y[y &lt; 2], test_size=.5, random_state=random_state) # Create a simple classifier classifier = svm.LinearSVC(random_state=random_state) classifier.fit(X_train, y_train) y_score = classifier.decision_function(X_test) return y_test, y_scoredef precision_recall_curve(y_true, y_score, pos_label=None): if pos_label is None: pos_label = 1 # 不同的排序方式，其结果也会有略微差别， # 比如 kind="mergesort" 的结果跟kind="quicksort"的结果是不同的， # 这是因为归并排序是稳定的，快速排序是不稳定的，sklearn中使用的是 kind="mergesort" desc_score_indices = np.argsort(y_score, kind="quicksort")[::-1] y_score = y_score[desc_score_indices] y_true = y_true[desc_score_indices] # 确定阈值下标索引，score中可能会有重复的分数，在sklearn中，重复的元素被去掉了 # 本来以为去除重复分数会影响结果呢，因为如果两个样本有重复的分数，一个标签是1，一个是0， # 感觉去掉哪个都会影响结果啊，但是实验发现竟然不影响结果，有点纳闷，以后有时间再分析吧 # distinct_value_indices = np.where(np.diff(y_score))[0] # threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1] # 这里没有去除重复分数 threshold_idxs = np.arange(y_score.size) # 按照阈值依次降低的顺序，确定当前阈值下的true positives 个数，tps[-1]对应于所有的正例数量 tps = np.cumsum(y_true * 1.)[threshold_idxs] # 计算当前阈值下的 false positives 个数， # 它与 tps的关系为fps=1+threshold_idxs-tps，这个关系是比较明显的 fps = 1 + threshold_idxs - tps # y_score[threshold_idxs]把对应下标的阈值取出 thresholds = y_score[threshold_idxs] precision = tps / (tps + fps) recall = tps / tps[-1] # 这里与sklearn有略微不同，即样本点全部输出，令last_ind = tps.size，即可 last_ind = tps.size sl = slice(0, last_ind) return np.r_[1, precision[sl]], np.r_[0, recall[sl]], thresholds[sl]def average_precision_approximated(y_true, y_predict): """ 计算approximated形式的ap，每个样本点的分数都是recall的一个cutoff :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) average_precision = np.sum(np.diff(recall) * np.array(precision)[1:]) return precision, recall, thresholds, average_precisiondef average_precision_interpolated(y_true, y_predict): """ 计算interpolated形式的ap，每个正样本对应的分数都是recalll的一个cutoff :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) # 获取recall轴上的分割，np.insert(recall, 0 , -1, axis=0)是为了保证获取到重复recall的第一个索引值 # 因为重复的recall中只有对应的第一个precision是最大的，我们只需要获取这个最大的precision # 或者说每遇到一个正样本，需要将其对应的recall值作为横轴的切分 recall_cutoff_index = np.where( np.diff(np.insert(recall, 0, -1, axis=0)))[0] # 从recall的cutoff 索引值开始往后获取precision最大值，相同的precision只取索引值最大的那个 # P(r) = max&#123;P(r')&#125; | r'&gt;=r precision_cutoff_index = [] for index in recall_cutoff_index: precision_cutoff_index.append( max([x for x in np.where(precision == np.max(precision[index:]))[0] if x &gt;= index])) # interpolated_idx=np.unique(interpolated_cutoff) # 从原始的precision和recall中截取对应索引的片段，即可得到 interpolated 方式下的precision，recall以及AP precision_interpolated = precision[precision_cutoff_index] recall_interpolated = recall[recall_cutoff_index] # 以上获得的 recall_cutoff_index 和 precision_cutoff_index 切片包含人为添加的0 和 1（为了画图时与坐标轴封闭） # 而计算thresholds_interpolated时要去掉相应索引值的影响 # 阈值不包括recall=0 thresholds_interpolated = thresholds[ [x - 1 for x in recall_cutoff_index if 0 &lt;= x - 1 &lt; thresholds.size]] # 按说ap计算应该按照面积的方式计算，也就是下面被注释的部分，但论文里面是直接计算均值， # 这里也直接计算均值，因为阈值不包括recall=0，所以这种情况下二者结果是一样的 average_precision = np.mean(precision_interpolated[1:]) # average_precision = np.sum( # np.diff(recall_interpolated) * np.array(precision_interpolated)[1:]) return precision_interpolated, recall_interpolated, thresholds_interpolated, average_precisiondef average_precision_11point_interpolated(y_true, y_predict): """ 计算 11point形式的 ap :param y_true: 标签 :param y_predict: 实际预测得分 :return: precision，recall，threshold，average precision """ precision, recall, thresholds = precision_recall_curve( y_true, y_predict, pos_label=1) recall_11point_cutoff = np.array( [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) # 计算离11个cutoff最近的样本点 recall_cutoff_index = [] for cutoff in recall_11point_cutoff: recall_cutoff_index.append(np.where(recall &gt;= cutoff)[0][0]) precision_cutoff_index = [] for index in recall_cutoff_index: precision_cutoff_index.append( max([x for x in np.where(precision == np.max(precision[index:]))[0] if x &gt;= index])) precision_11point = precision[precision_cutoff_index] recall_11point = recall[recall_cutoff_index] # 此处阈值包括recall=0，因为是11points thresholds_11point = thresholds[ [x - 1 for x in recall_cutoff_index if -1 &lt;= x - 1 &lt; thresholds.size]] # 此处阈值包括recall=0，因为是11points，所以这种情况下两种计算AP的方式结果不同，有略微差别 average_precision = np.mean(precision_11point) # average_precision = np.sum( # 0.1 * np.array(precision_11point)[1:]) # 此处直接返回 recall_11point_cutoff，实际上返回 recall_11point 也是可以的， # 差别就是图线的转折点不在[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]这11个刻度处 # return precision_11point, recall_11point, thresholds_11point, average_precision return precision_11point, recall_11point_cutoff, thresholds_11point, average_precisiondef main(data=None): y_test = [] y_score = [] if data is None: # 一个简单的示例，这个示例直接给定了y_test和y_score y_test = np.array([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]) y_score = np.array([0.23, 0.76, 0.01, 0.91, 0.13, 0.45, 0.12, 0.03, 0.38, 0.11, 0.03, 0.09, 0.65, 0.07, 0.12, 0.24, 0.1, 0.23, 0.46, 0.08]) if data == 'iris': # 还可以导入鸢尾花数据集并构建一个简单的SVM分类器，通过一个完整的模型来理解 PR曲线的绘制 # 使用鸢尾花数据集 y_test, y_score = preprocess_iris_data() # 计算AP，并画图 precision_approximated, recall_approximated, _, ap_approximated = \ average_precision_approximated(y_test, y_score) precision_interpolated, recall_interpolated, _, ap_interpolated = \ average_precision_interpolated(y_test, y_score) precision_11point, recall_11point, _, ap_11point = \ average_precision_11point_interpolated(y_test, y_score) print('Approximated average precision-recall score: &#123;0:0.5f&#125;'.format( ap_approximated)) print('Interpolated average precision-recall score: &#123;0:0.5f&#125;'.format( ap_interpolated)) print('Interpolated at fixed 11 points average precision-recall score: &#123;0:0.5f&#125;'.format( ap_11point)) # print the AP plot fig1 = plt.figure('fig1') # plt.subplot(311) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.step(recall_approximated, precision_approximated, color='c', where='pre') plt.fill_between(recall_approximated, precision_approximated, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Approximated): AP=&#123;0:0.5f&#125;'.format( ap_approximated)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', 'Approximated-PR-curve', 'Approximated-AP'), loc='upper right') fig2 = plt.figure('fig2') # plt.subplot(312) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.plot(recall_interpolated, precision_interpolated, color='c', marker='o', mec='g', ms=3, alpha=0.5) plt.fill_between(recall_interpolated, precision_interpolated, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Interpolated): AP=&#123;0:0.5f&#125;'.format( ap_interpolated)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', 'Interpolated-PR-curve', 'Interpolated-AP'), loc='upper right') fig3 = plt.figure('fig3') # plt.subplot(313) plt.plot(recall_approximated, precision_approximated, color='r', marker='o', mec='m', ms=3) plt.plot(recall_11point, precision_11point, color='c', marker='o', mec='g', ms=3) plt.fill_between(recall_11point, precision_11point, step='pre', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1]) plt.title('2-class Precision-Recall curve(Interpolated_11point): AP=&#123;0:0.5f&#125;'.format( ap_11point)) plt.xticks(np.arange(0, 1, 0.1)) plt.yticks(np.arange(0, 1, 0.1)) plt.grid(True) plt.legend(('PR-curve', '11point-PR-curve', '11point-AP'), loc='upper right') # plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5, # wspace=0.35) plt.show()if __name__ == '__main__': # main() # 用这个测试博客 “多标签图像分类任务的评价方法-mAP” 中的简单例程 main('iris') 这个例子使用了 鸢尾花数据集中的两类构建了一个SVM分类器，然后对分类结果计算AP. 最终的结果如下所示： 参考资料 周志华老师 《机器学习》 多标签图像分类任务的评价方法-mAP average precision 王斌译 《信息检索导论》 论文： The PASCAL Visual Object Classes (VOC) Challenge 2007 和 2012 http://cocodataset.org/#detection-eval http://host.robots.ox.ac.uk/pascal/VOC/ http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>评价标准</tag>
        <tag>AP</tag>
        <tag>mAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测数据集MSCOCO简介]]></title>
    <url>%2Farticles%2Fa-brief-introduction-to-the-target-detection-data-set-mscoco.html</url>
    <content type="text"><![CDATA[简介介绍一下目标检测领域另外一个比较有名的数据集 MS COCO (Microsoft COCO: Common Objects in Context) . MSCOCO 数据集是微软构建的一个数据集，其包含 detection, segmentation, keypoints等任务。 MSCOCO主要是为了解决detecting non-iconic views of objects（对应常说的detection）, contextual reasoning between objects and the precise 2D localization of objects（对应常说的分割问题） 这三种场景下的问题。 下面是iconic 图片和 non-iconic 图片之间的对比。 与PASCAL COCO数据集相比，COCO中的图片包含了自然图片以及生活中常见的目标图片，背景比较复杂，目标数量比较多，目标尺寸更小，因此COCO数据集上的任务更难，对于检测任务来说，现在衡量一个模型好坏的标准更加倾向于使用COCO数据集上的检测结果。 数据集的构建过程不说了。主要关注一下统计信息 1 统计信息MSCOCO总共包含91个类别，每个类别的图片数量如下： 图中也标出了PASCAL VOC的统计数据作为对比。 下图展示的是几个不同数据集的总类别数量，以及每个类别的总实例数量，一个实例就是图片上的一个目标，主要关注一下 PASCAL 和 ImageNet。 COCO数据集的类别总数虽然没有 ImageNet 中用于detection的类别总数多，但是每个类别的实例目标总数要比PASCAL和ImageNet都要多。 下图是每张图片上的类别数量或者实例数量的分布，括号中为平均值 PASCAL和ImageNet中，每张图片上的类别或者实例数量普遍都很少。 以PASCAL为例：有多于70%的图片上都只有一个类别，而多于50%的图片上只有一个实例或者目标。PASCAL数据集平均每张图片包含1.4个类别和2.3个实例目标，ImageNet也仅有1.7和3.0个。 COCO数据集平均每张图片包含 3.5个类别和 7.7 个实例目标，仅有不到20%的图片只包含一个类别，仅有10%的图片包含一个实例目标。 COCO数据集不仅数据量大，种类和实例数量也多。从这角度来说 SUN 数据集这两个指标更高一点，但是这个数据集在目标检测里面并不常用。 实例目标的分布 COCO数据集中的小目标数量占比更多 关于数据集的划分，COCO的论文里是这么说的， The 2014 release contains 82,783 training, 40,504 validation, and 40,775 testing images (approximately 1/2 train, 1/4 val, and /4 test). There are nearly 270k segmented people and a total of 886k segmented object instances in the 2014 train+val data alone. The cumulative 2015 release will contain a total of 165,482 train, 81,208 val, and 81,434 test images. 2014年的数据在官网是可以下载的，但是2015年只有test部分，train和val部分的数据没有。另外2017年的数据并没有什么新的图片，只是将数据重新划分，train的数据更多了，如下： 2 评估标准COCO的测试标准比PASCAL VOC更严格： PASCAL 中在测试mAP时，是在IOU=0.5时测的 COCO中的AP 是指在 10个IOU层面 以及 80个类别层面 的平均值 COCO的主要评价指标是AP，指 IOU从0.5到0.95 每变化 0.05 就测试一次 AP，然后求这10次测量结果的平均值作为最终的 AP AP@0.5 跟PASCAL VOC中的mAP是相同的含义 AP@0.75 跟PASCAL VOC中的mAP也相同，只是IOU阈值提高到了0.75，显然这个层面更严格，精度也会更低 IOU越高，AP就越低，所以最终的平均之后的AP要比 AP@0.5 小很多，这也就是为什么COCO的AP 超过 50%的只有寥寥几个而已，因为超过50%太难了。而且由于COCO数据集本身数据的复杂性，所以目前的 AP@0.5 最高也只有 73% 。 COCO数据集还针对 三种不同大小（small，medium，large） 的图片提出了测量标准，COCO中包含大约 41% 的小目标 ($area &lt; 32×32$), 34% 的中等目标 ($32×32 &lt; area &lt; 96×96$), 和 24% 的大目标 ($area &gt; 96×96$). 小目标的AP是很难提升的。 除了AP之外，还提出了 AR 的测量标准 跟AP是类似的。 COCO提供了一些代码，方便对数据集的使用和模型评估 ：https://github.com/cocodataset/cocoapi 3 总结为什么COCO的检测任务那么难？ 图片大多数来源于生活中，背景更复杂 每张图片上的实例目标个数多，平均每张图片7.7个 小目标更多 评估标准更严格 所以现在大家更倾向于使用COCO来评估模型的质量。 参考资料 COCO官网：http://cocodataset.org 论文：Microsoft COCO: Common Objects in Context 论文：What makes for effective detection proposals?]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>dataset</tag>
        <tag>MSCOCO</tag>
        <tag>COCO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测数据集PASCAL VOC简介]]></title>
    <url>%2Farticles%2Fa-brief-introduction-to-the-target-detection-data-set-pascal-voc.html</url>
    <content type="text"><![CDATA[简介PASCAL VOC挑战赛 （The PASCAL Visual Object Classes ）是一个世界级的计算机视觉挑战赛, PASCAL全称：Pattern Analysis, Statical Modeling and Computational Learning，是一个由欧盟资助的网络组织。 很多优秀的计算机视觉模型比如分类，定位，检测，分割，动作识别等模型都是基于PASCAL VOC挑战赛及其数据集上推出的，尤其是一些目标检测模型（比如大名鼎鼎的R CNN系列，以及后面的YOLO，SSD等）。 PASCAL VOC从2005年开始举办挑战赛，每年的内容都有所不同，从最开始的分类，到后面逐渐增加检测，分割，人体布局，动作识别（Object Classification 、Object Detection、Object Segmentation、Human Layout、Action Classification）等内容，数据集的容量以及种类也在不断的增加和改善。该项挑战赛催生出了一大批优秀的计算机视觉模型（尤其是以深度学习技术为主的）。 我们知道在 ImageNet挑战赛上涌现了一大批优秀的分类模型，而PASCAL挑战赛上则是涌现了一大批优秀的目标检测和分割模型，这项挑战赛已于2012年停止举办了，但是研究者仍然可以在其服务器上提交预测结果以评估模型的性能。 虽然近期的目标检测或分割模型更倾向于使用MS COCO数据集，但是这丝毫不影响 PASCAL VOC数据集的重要性，毕竟PASCAL对于目标检测或分割类型来说属于先驱者的地位。对于现在的研究者来说比较重要的两个年份的数据集是 PASCAL VOC 2007 与 PASCAL VOC 2012，这两个数据集频频在现在的一些检测或分割类的论文当中出现。 PASCAL主页 与 排行榜 （榜上已几乎看不到传统的视觉模型了，全是基于深度学习的） PASCAL VOC 2007 挑战赛主页 与 PASCAL VOC 2012 挑战赛主页 与 PASCAL VOC Evaluation Server. 以及在两个重要时间点对 PASCAL VOC挑战赛 成绩进行总结的两篇论文 The PASCAL Visual Object Classes Challenge: A Retrospective Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.International Journal of Computer Vision, 111(1), 98-136, 2015Bibtex source | Abstract | PDF 主要总结PASCAL VOC 2012的数据集情况，以及2011年-2013年之间出现的模型及其性能对比 The PASCAL Visual Object Classes (VOC) ChallengeEveringham, M., Van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A.International Journal of Computer Vision, 88(2), 303-338, 2010Bibtex source | Abstract | PDF 主要总结PASCAL VOC 2007的数据集情况，以及2008年之前出现的模型及其性能对比 不过在以上论文中出现的深度学习模型只有一个 R-CNN吧，大部分都是传统方式的模型，毕竟深度学习模型主要在14年以后才大量涌现。 本文也是以PASCAL VOC 2007 和 2012 为例简要介绍VOC数据集的结构。 1 数据集整体概况1.1 层级结构PASCAL VOC 数据集的20个类别及其层级结构： 从2007年开始，PASCAL VOC每年的数据集都是这个层级结构 总共四个大类：vehicle,household,animal,person 总共20个小类，预测的时候是只输出图中黑色粗体的类别 数据集主要关注分类和检测，也就是分类和检测用到的数据集相对规模较大。关于其他任务比如分割，动作识别等，其数据集一般是分类和检测数据集的子集。 1.2 发展历程与使用方法简要提一下在几个关键时间点数据集的一些关键变化，详细的请查看PASCAL VOC主页 。 2005年：还只有4个类别： bicycles, cars, motorbikes, people. Train/validation/test共有图片1578 张，包含2209 个已标注的目标objects. 2007年 ：在这一年PASCAL VOC初步建立成一个完善的数据集。类别扩充到20类，Train/validation/test共有9963张图片，包含24640 个已标注的目标objects. 07年之前的数据集中test部分都是公布的，但是之后的都没有公布。 2009年：从这一年开始，通过在前一年的数据集基础上增加新数据的方式来扩充数据集。比如09年的数据集是包含了08年的数据集的，也就是说08年的数据集是09年的一个子集，以后每年都是这样的扩充方式，直到2012年；09年之前虽然每年的数据集都在变大（08年比07年略少），但是每年的数据集都是不一样的，也就是说每年的数据集都是互斥的，没有重叠的图片。 2012年：从09年到11年，数据量仍然通过上述方式不断增长，11年到12年，用于分类、检测和person layout 任务的数据量没有改变。主要是针对分割和动作识别，完善相应的数据子集以及标注信息。 对于分类和检测来说，也就是下图所示的发展历程，相同颜色的代表相同的数据集： 分割任务的数据集变化略有不同： VOC 2012用于分类和检测的数据包含 2008-2011年间的所有数据，并与VOC2007互斥。 VOC 2012用于分割的数据中train+val包含 2007-2011年间的所有数据，test包含2008-2011年间的数据，没有包含07年的是因为07年的test数据已经公开了。 2012年是最后一次挑战赛，最终用于分类和检测的数据集规模为：train/val ：11540 张图片，包含 27450 个已被标注的 ROI annotated objects ；用于分割的数据集规模为：trainval：2913张图片，6929个分割，用于其他任务的不再细说，参考这里 。 即便挑战赛结束了，但是研究者们仍然可以上传预测结果进行评估。上传入口： PASCAL VOC Evaluation Server. 目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。 论文中针对 VOC2007和VOC2012 的具体用法有以下几种： 只用VOC2007的trainval 训练，使用VOC2007的test测试 只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用 使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12 ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。 使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 07++12 ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 07+12+COCO 。 先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 07++12+COCO，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。 在各自数据集上分别进行建模和评测的用法比较少，基本上在早期论文里出现就是起个对照作用；现在的大部分论文都会为了增加数据量而将二者合起来使用。 2 数据量统计由于现在的研究基本上都是在VOC2007和VOC2012上面进行，因此只介绍这两个年份的。 2.1 VOC 2007一些示例图片展示：Classification/detection example images 数据集总体统计： 以上是数据集总体的统计情况，这个里面是包含了测试集的，可见person 类是最多的。 训练集，验证集，测试集划分情况 PASCAL VOC 2007 数据集分为两部分：训练和验证集trainval，测试集test ，两部分各占数据总量的约 50%。其中trainval 又分为训练集和测试集，二者分别各占trainval的50%。 每张图片中有可能包含不只一个目标object。 这里我就只贴出用于分类和检测的划分情况，关于分割或者其他任务的划分方式 点击这里查看 。 2.2 VOC 2012一些示例图片展示：Classification/detection example images 数据集总体统计 这个统计是没有包含 test部分的，仍然是person类最多 trainval部分的数据统计： test部分没有公布，同样的 除了分类和检测之外的数据统计，参考这里 2.3 VOC 2007 与 2012 的对比VOC 2007 与 2012 数据集及二者的并集 数据量对比 黑色字体所示数字是官方给定的，由于VOC2012数据集中 test 部分没有公布，因此红色字体所示数字为估计数据，按照PASCAL 通常的划分方法，即 trainval 与test 各占总数据量的一半。 3 标注信息数据集的标注还是很谨慎的，有专门的标注团队，并遵从统一的标注标准，参考 guidelines 。 标注信息是用 xml 文件组织的如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;annotation&gt; &lt;folder&gt;VOC2007&lt;/folder&gt; &lt;filename&gt;000001.jpg&lt;/filename&gt; &lt;source&gt; &lt;database&gt;The VOC2007 Database&lt;/database&gt; &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt; &lt;image&gt;flickr&lt;/image&gt; &lt;flickrid&gt;341012865&lt;/flickrid&gt; &lt;/source&gt; &lt;owner&gt; &lt;flickrid&gt;Fried Camels&lt;/flickrid&gt; &lt;name&gt;Jinky the Fruit Bat&lt;/name&gt; &lt;/owner&gt; &lt;size&gt; &lt;width&gt;353&lt;/width&gt; &lt;height&gt;500&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;dog&lt;/name&gt; &lt;pose&gt;Left&lt;/pose&gt; &lt;truncated&gt;1&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;48&lt;/xmin&gt; &lt;ymin&gt;240&lt;/ymin&gt; &lt;xmax&gt;195&lt;/xmax&gt; &lt;ymax&gt;371&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;person&lt;/name&gt; &lt;pose&gt;Left&lt;/pose&gt; &lt;truncated&gt;1&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;8&lt;/xmin&gt; &lt;ymin&gt;12&lt;/ymin&gt; &lt;xmax&gt;352&lt;/xmax&gt; &lt;ymax&gt;498&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; filename ：文件名 source，owner：图片来源，及拥有者 size：图片大小 segmented：是否分割 object：表明这是一个目标，里面的内容是目标的相关信息 name：object名称，20个类别 pose：拍摄角度：front, rear, left, right, unspecified truncated：目标是否被截断（比如在图片之外），或者被遮挡（超过15%） difficult：检测难易程度，这个主要是根据目标的大小，光照变化，图片质量来判断 difficult 标签示例：图中白色虚线，被标记为 difficult。 bndbox：bounding box 的左上角点和右下角点的4个坐标值。 4 提交格式4.1 Classification Task每一类都有一个txt文件，里面每一行都是测试集中的一张图片，前面一列是图片名称，后面一列是预测的分数。 comp1_cls_test_car.txt: 12345000004 0.702732000006 0.870849000008 0.532489000018 0.477167000019 0.112426 4.2 Detection Taskcomp3_det_test_car.txt: 12345000004 0.702732 89 112 516 466000006 0.870849 373 168 488 229000006 0.852346 407 157 500 213000006 0.914587 2 161 55 221000008 0.532489 175 184 232 201 每一类都有一个txt文件，里面每一行都是测试集中的一张图片，每行的格式按照如下方式组织 1&lt;image identifier&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt; confidence 用来计算 mAP. 5 评估标准PASCAL的评估标准是 mAP(mean average precision) 关于mAP不再详细解释，参考以下资料： 性能指标（模型评估）之mAP average precision 周志华老师 《机器学习》 模型评估标准一节 这里简单的提一下： 下面是一个二分类的P-R曲线（precision-recall curve），对于PASCAL来说，每一类都有一个这样的 P-R曲线，P-R曲线下面与x轴围成的面积称为 average precision，每个类别都有一个 AP, 20个类别的AP 取平均值就是 mAP。 PASCAL官方给了评估脚本mAP的脚本和示例代码 development kit code and documentation ，是用MATLAB写的。 6 数据集组织结构数据集的下载: 123456789# Download the data.cd $HOME/datawget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar# Extract the data.tar -xvf VOCtrainval_11-May-2012.tartar -xvf VOCtrainval_06-Nov-2007.tartar -xvf VOCtest_06-Nov-2007.tar 或者支直接点击下面链接下载： Download the training/validation data (450MB tar file) Download the annotated test data (430MB tar file) 上面的解压命令会将VOC2007的trainval和test解压到一块，数据会混在一起，如果不想，可以自己指定解压路径。以VOC 2007 为例，解压后的文件： 123456.├── Annotations 进行detection 任务时的 标签文件，xml文件形式├── ImageSets 存放数据集的分割文件，比如train，val，test├── JPEGImages 存放 .jpg格式的图片文件├── SegmentationClass 存放 按照class 分割的图片└── SegmentationObject 存放 按照 object 分割的图片 Annotations 文件夹： 12345678910.├── 000001.xml├── 000002.xml├── 000003.xml├── 000004.xml………………├── 009962.xml└── 009963.xml 以xml 文件的形式，存放标签文件，文件内容如前述，文件名与图片名是一样的，6位整数 ImageSets文件夹： 存放数据集的分割文件 包含三个子文件夹 Layout，Main，Segmentation，其中Main文件夹存放的是用于分类和检测的数据集分割文件，Layout文件夹用于 person layout任务，Segmentation用于分割任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798.├── Layout│ ├── test.txt│ ├── train.txt│ ├── trainval.txt│ └── val.txt├── Main│ ├── aeroplane_test.txt│ ├── aeroplane_train.txt│ ├── aeroplane_trainval.txt│ ├── aeroplane_val.txt│ ├── bicycle_test.txt│ ├── bicycle_train.txt│ ├── bicycle_trainval.txt│ ├── bicycle_val.txt│ ├── bird_test.txt│ ├── bird_train.txt│ ├── bird_trainval.txt│ ├── bird_val.txt│ ├── boat_test.txt│ ├── boat_train.txt│ ├── boat_trainval.txt│ ├── boat_val.txt│ ├── bottle_test.txt│ ├── bottle_train.txt│ ├── bottle_trainval.txt│ ├── bottle_val.txt│ ├── bus_test.txt│ ├── bus_train.txt│ ├── bus_trainval.txt│ ├── bus_val.txt│ ├── car_test.txt│ ├── car_train.txt│ ├── car_trainval.txt│ ├── car_val.txt│ ├── cat_test.txt│ ├── cat_train.txt│ ├── cat_trainval.txt│ ├── cat_val.txt│ ├── chair_test.txt│ ├── chair_train.txt│ ├── chair_trainval.txt│ ├── chair_val.txt│ ├── cow_test.txt│ ├── cow_train.txt│ ├── cow_trainval.txt│ ├── cow_val.txt│ ├── diningtable_test.txt│ ├── diningtable_train.txt│ ├── diningtable_trainval.txt│ ├── diningtable_val.txt│ ├── dog_test.txt│ ├── dog_train.txt│ ├── dog_trainval.txt│ ├── dog_val.txt│ ├── horse_test.txt│ ├── horse_train.txt│ ├── horse_trainval.txt│ ├── horse_val.txt│ ├── motorbike_test.txt│ ├── motorbike_train.txt│ ├── motorbike_trainval.txt│ ├── motorbike_val.txt│ ├── person_test.txt│ ├── person_train.txt│ ├── person_trainval.txt│ ├── person_val.txt│ ├── pottedplant_test.txt│ ├── pottedplant_train.txt│ ├── pottedplant_trainval.txt│ ├── pottedplant_val.txt│ ├── sheep_test.txt│ ├── sheep_train.txt│ ├── sheep_trainval.txt│ ├── sheep_val.txt│ ├── sofa_test.txt│ ├── sofa_train.txt│ ├── sofa_trainval.txt│ ├── sofa_val.txt│ ├── test.txt│ ├── train_test.txt│ ├── train_train.txt│ ├── train_trainval.txt│ ├── train.txt│ ├── train_val.txt│ ├── trainval.txt│ ├── tvmonitor_test.txt│ ├── tvmonitor_train.txt│ ├── tvmonitor_trainval.txt│ ├── tvmonitor_val.txt│ └── val.txt└── Segmentation ├── test.txt ├── train.txt ├── trainval.txt └── val.txt 3 directories, 92 files 主要介绍一下Main文件夹中的组织结构，先来看以下这几个文件： 12345├── Main│ ├── train.txt 写着用于训练的图片名称 共2501个│ ├── val.txt 写着用于验证的图片名称 共2510个│ ├── trainval.txt train与val的合集 共5011个│ ├── test.txt 写着用于测试的图片名称 共4952个 里面的文件内容是下面这样的：以train.txt文件为例 123456789101112131415000012000017000023000026000032000033000034000035000036000042…………009949009959009961 就是对数据库的分割，这一部分图片用于train，其他的用作val，test等。 Main中剩下的文件很显然就是每一类别在train或val或test中的ground truth，这个ground truth是为了方便classification 任务而提供的；如果是detection的话，使用的是上面的xml标签文件。 1234567├── Main│ ├── aeroplane_test.txt 写着用于训练的图片名称 共2501个，指定正负样本│ ├── aeroplane_train.txt 写着用于验证的图片名称 共2510个，指定正负样本│ ├── aeroplane_trainval.txt train与val的合集 共5011个，指定正负样本│ ├── aeroplane_val.txt 写着用于测试的图片名称 共4952个，指定正负样本………… 里面文件是这样的（以aeroplane_train.txt为例）： 123456789101112131415000012 -1000017 -1000023 -1000026 -1000032 1000033 1000034 -1000035 -1000036 -1000042 -1…………009949 -1009959 -1009961 -1 前面一列是训练集中的图片名称，这一列跟train.txt文件中的内容是一样的，后面一列是标签，即训练集中这张图片是不是aeroplane，是的话为1，否则为-1. 其他所有的 (class)_(imgset).txt 文件都是类似的。 (class)_train 存放的是训练使用的数据，每一个class都有2501个train数据。 (class)_val 存放的是验证使用的数据，每一个class都有2510个val数据。 (class)_trainval 将上面两个进行了合并，每一个class有5011个数据。 (class)_test 存放的是测试使用的数据，每一个class有4952个test数据。 所有文件都 指定了正负样本，每个class的实际数量为正样本的数量，train和val两者没有交集。 VOC2012 的数据集组织结构是类似的，不一样的地方在于VOC2012 中没有 test类的图片和以及相关标签和分割文件，因为这部分数据 VOC2012没有公布。 参考资料 http://host.robots.ox.ac.uk/pascal/VOC/ 性能指标（模型评估）之mAP 多标签图像分类任务的评价方法-mAP average precision]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>dataset</tag>
        <tag>PASCAL VOC</tag>
      </tags>
  </entry>
</search>
