<!DOCTYPE html>




<html class="theme-next gemini" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>
<meta name="theme-color" content="#222">

<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/kitty_bbook_32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/kitty_bbook_16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="TensorRT,inference," />










<meta name="description" content="TensorRT INT8校准原理">
<meta name="keywords" content="TensorRT,inference">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRT(5)-INT8校准原理">
<meta property="og:url" content="https://arleyzhang.github.io/articles/923e2c40/index.html">
<meta property="og:site_name" content="arleyzhang">
<meta property="og:description" content="TensorRT INT8校准原理">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/no_satuation_int8_quantization.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/distribution-of-different-layers.png.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/satuation_int8_quantization.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/Calibration-process.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/satuation-before-and-after.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/Calibration-result1.jpg">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/Calibration-result2.jpg">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/DP4A.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/performance-auccary.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/performance-speed.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/performance-titanx.png">
<meta property="og:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/performance-dgpu.png">
<meta property="og:updated_time" content="2018-09-02T17:24:55.472Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorRT(5)-INT8校准原理">
<meta name="twitter:description" content="TensorRT INT8校准原理">
<meta name="twitter:image" content="https://arleyzhang.github.io/images/TensorRT-5-int8-calibration.assets/no_satuation_int8_quantization.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://arleyzhang.github.io/articles/923e2c40/"/>





  <title>TensorRT(5)-INT8校准原理 | arleyzhang</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">arleyzhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://arleyzhang.github.io/articles/923e2c40/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="arleyzhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="arleyzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorRT(5)-INT8校准原理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-31T18:47:44+08:00">
                2018-08-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-09-03T01:24:55+08:00">
                2018-09-03
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/articles/923e2c40/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="articles/923e2c40/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          
              <div class="post-description">
                  TensorRT INT8校准原理
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本次讲一下 tensorRT 的 INT8 低精度推理模式。主要参考 <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="external">GTC 2017，Szymon Migacz 的PPT</a> 。</p>
<h1 id="1-Low-Precision-Inference"><a href="#1-Low-Precision-Inference" class="headerlink" title="1 Low Precision Inference"></a>1 Low Precision Inference</h1><p>现有的深度学习框架 比如：TensorFlow，Caffe， MixNet等，在训练一个深度神经网络时，往往都会使用 float 32（Full Precise ，简称FP32）的数据精度来表示，权值、偏置、激活值等。但是如果一个网络很深的话，比如像VGG，ResNet这种，网络参数是极其多的，计算量就更多了（比如VGG 19.6 billion FLOPS, ResNet-152 11.3 billion FLOPS）。如此多的计算量，如果中间值都使用 FP 32的精度来计算的话，势必会很费时间。而这对于嵌入式设备或者移动设备来说，简直就是噩梦，因为他们的计算能力和内存数量是不能与PC相比的。</p>
<p>因此解决此问题的方法之一就是在部署推理时（inference）使用低精度数据，比如INT8。除此之外，当然还有模型压缩之类的方法，不过此处不做探究。注意此处只是针对 推理阶段，训练时仍然使用 FP32的精度。</p>
<p><strong>从经验上来分析一下低精度推理的可行性：</strong> </p>
<p>实际上有些人认为，即便在推理时使用低精度的数据（比如INT8），在提升速度的同时，也并不会造成太大的精度损失，比如  <a href="https://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/" target="_blank" rel="external">Why are Eight Bits Enough for Deep Neural Networks?</a> 以及<a href="https://towardsdatascience.com/low-precision-inference-with-tensorrt-6eb3cda0730b" target="_blank" rel="external">Low Precision Inference with TensorRT</a> 这两篇博文。</p>
<p>文章的作者认为网络在训练的过程中学习到了数据样本的模式可分性，同时由于数据中存在的噪声，使得网络具有较强的鲁棒性，也就是说在输入样本中做轻微的变动并不会过多的影响结果性能。与图像上目标间的位置，姿态，角度等的变化程度相比，这些噪声引进的变动只是很少的一部分，但实际上这些噪声引进的变动同样会使各个层的激活值输出发生变动，然而却对结果影响不大，也就是说训练好的网络对这些噪声具有一定的容忍度（tolerance ）。</p>
<p>正是由于在训练过程中使用高精度（FP32）的数值表示，才使得网络具有一定的容忍度。训练时使用高精度的数值表示，可以使得网络以很小的计算量修正参数，这在网络最后收敛的时候是很重要的，因为收敛的时候要求修正量很小很小（一般训练初始 阶段学习率稍大，越往后学习率越小）。</p>
<p>那么如果使用低精度的数据来表示网络参数以及中间值的话，势必会存在误差，这个误差某种程度上可以认为是一种噪声。那也就是说，使用低精度数据引进的差异是在网络的容忍度之内的，所以对结果不会产生太大影响。</p>
<p>以上分析都是基于经验的，理论上的分析比较少，不过文章提到了两篇 paper，如下：</p>
<ul>
<li><a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/37631.pdf" target="_blank" rel="external">Improving the speed of neural networks on CPUs</a> </li>
<li><a href="https://arxiv.org/abs/1412.7024" target="_blank" rel="external">Training deep neural networks with low precision multiplications</a></li>
</ul>
<p>这里不对这两篇paper做探究。</p>
<p>TensorRT 的INT8模式只支持计算能力为6.1的GPU（Compute Capability 6.1 ），比如： GP102 (Tesla P40 and NVIDIA Titan X), GP104 (<a href="https://devblogs.nvidia.com/parallelforall/new-pascal-gpus-accelerate-inference-in-the-data-center/" target="_blank" rel="external">Tesla P4</a>), and GP106 GPUs，主要根源是这些GPU支持 DP4A硬件指令。DP4A下面会稍微介绍一下。</p>
<h1 id="2-TensorRT-INT8-Inference"><a href="#2-TensorRT-INT8-Inference" class="headerlink" title="2 TensorRT INT8 Inference"></a>2 TensorRT INT8 Inference</h1><p>首先看一下不同精度的动态范围：</p>
<table>
<thead>
<tr>
<th></th>
<th>动态范围</th>
<th>最小正数</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>$-3.4×10^{38} ~ +3.4×10^{38}$</td>
<td>$1.4 × 10^{−45}$</td>
</tr>
<tr>
<td>FP16</td>
<td>$-65504 ~ +65504$</td>
<td>$5.96 × 10^{-8}$</td>
</tr>
<tr>
<td>INT8</td>
<td>$-128 ~ +127$</td>
<td>$1$</td>
</tr>
</tbody>
</table>
<p>实际上将FP32的精度降为INT8还是比较具有挑战性的。</p>
<h2 id="2-1-Quantization"><a href="#2-1-Quantization" class="headerlink" title="2.1 Quantization"></a>2.1 Quantization</h2><p>将FP32降为INT8的过程相当于信息再编码（re-encoding information ），就是原来使用32bit来表示一个tensor，现在使用8bit来表示一个tensor，还要求精度不能下降太多。</p>
<p>将FP32转换为 INT8的操作需要针对每一层的输入张量（tensor）和 网络学习到的参数（learned parameters）进行。</p>
<p>首先能想到的最简单的映射方式就是线性映射（或称线性量化，linear quantization）, 就是说映射前后的关系满足下式：</p>
<p>$$<br>\text{FP32 Tensor (T) = scale_factor(sf) * 8-bit Tensor(t) + FP32_bias (b)}<br>$$</p>
<p>试验证明，偏置实际上是不需要的，因此去掉偏置，也就是<br>$$<br>T = sf * t<br>$$<br>$sf$ 是每一层上每一个tensor的换算系数或称比例因子（scaling factor），因此现在的问题就变成了如何确定比例因子。然后最简单的方法是下图这样的：</p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/no_satuation_int8_quantization.png" alt="no_satuation_int8_quantization"></p>
<ul>
<li>简单的将一个tensor 中的 -|max| 和 |max| FP32 value 映射为 -127 和 127 ，中间值按照线性关系进行映射。</li>
<li>称这种映射关系为不饱和的（No saturation ），对称的。</li>
</ul>
<p>但是试验结果显示这样做会导致比较大的精度损失。</p>
<p>下面这张图展示的是不同网络结构的不同layer的激活值分布，有卷积层，有池化层，他们之间的分布很不一样，因此合理的 量化方式 应该适用于不同的激活值分布，并且减小 信息损失。因为从FP32到INT8其实就是一种信息再编码的过程。</p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/distribution-of-different-layers.png.png" alt="1535654439553"></p>
<p>我个人理解的直接使用线性量化的方式导致精度损失比较大的原因是：</p>
<ul>
<li>上图是一些网络模型中间层的  激活值统计，横坐标是激活值，纵坐标是统计数量的归一化表示，这里是归一化表示，不是绝对数值统计；</li>
<li>这个激活值统计 针对的是一批图片，不同的图片输出的激活值不完全相同。所以图上并不是一条曲线而是多条曲线（一张图片对应一条曲线，或者称为散点图更好一点），只不过前面一部分重复在一块了（红色虚线圈起来的部分），说明对于不同图片生成的大部分激活值其分布是相似的；但是在激活值比较大时（红色实线圈起来的部分），曲线不重复了，一个激活值对应多个不同的统计量，这时的激活值分布就比较乱了。</li>
<li>后面这一部分在整个层中是占少数的（占比很小，比如10^-9, 10^-7, 10^-3），因此后面这一段完全可以不考虑到映射关系中去，保留激活值分布的主方向。开始我以为网络之所以能把不同类别的图片分开是由于后面实线部分的差异导致的，后来想了一下：这个并不包含空间位置的分布，只是数值上的分布，所以后面的应该对结果影响不大。</li>
</ul>
<p>因此TensorRT的做法是：</p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/satuation_int8_quantization.png" alt="satuation_int8_quantization"></p>
<ul>
<li>这种做法不是将 ±|max| 映射为 ±127，而是存在一个 阈值 |T| ，将 ±|T| 映射为±127，显然这里 |T|&lt;|max|。</li>
<li>超出 阈值  ±|T| 外的直接映射为阈值 ±127。比如上图中的三个红色点，直接映射为-127。</li>
<li>称这种映射关系为饱和的（Saturate ），不对称的。</li>
<li>只要 阈值 选取得当，就能将分布散乱的较大的激活值舍弃掉，也就有可能使精度损失不至于降低太多。</li>
</ul>
<p>网络的前向计算涉及到两部分数值：权值和激活值（weights 和activation，二者要做乘法运算），Szymon Migacz 也提到他们曾经做过实验，说对weights 做saturation 没有什么变化，因此 对于weights的int8量化就使用的是不饱和的方式；而对activation做saturation就有比较显著的性能提升，因此对activation使用的是饱和的量化方式。</p>
<p>那现在的问题是 如何确定|T|？我们来思考一下，现在有一个FP32的tensor，FP32肯定是能够表达这个tensor的最佳分布。现在我们要用一个不同的分布（INT8）来表达这个tensor，这个 INT8 分布不是一个最佳的分布。饱和的INT8分布由于阈值 |T|的取值会有很多种情况（$128-|max|$），其中肯定有一种情况是相对其他最接近FP32的，我们就是要把这种情况找出来。</p>
<p>既然如此，我们就需要一个衡量指标来衡量不同的 INT8 分布与原来的FP3F2分布之间的差异程度。这个衡量指标就是 <strong>相对熵</strong>（relative entropy），又称为<strong>KL散度</strong>（<strong>Kullback–Leibler divergence</strong>，简称<strong>KLD</strong>），信息散度（information divergence），信息增益（information gain）。叫法实在太多了，最常见的就是相对熵。跟交叉熵也是有关系的。</p>
<ul>
<li><p>假设我们要给一个信息进行完美编码，那么最短平均编码长度就是信息熵。</p>
</li>
<li><p>如果编码方案不一定完美（由于对概率分布的估计不一定正确），这时的平均编码长度就是交叉熵。 </p>
<p>平均编码长度 = 最短平均编码长度 + 一个增量</p>
<p>交叉熵在深度学习中广泛使用，衡量了测试集标签分布和模型预测分布之间的差异程度。</p>
</li>
<li><p>编码方法不一定完美时，平均编码长度相对于最小值的增加量（即上面那个增量）是相对熵。</p>
</li>
</ul>
<p>即 <strong>交叉熵=信息熵+相对熵</strong> </p>
<p>通俗的理解 信息熵，交叉熵，相对熵，参考：<a href="https://www.zhihu.com/question/41252833" target="_blank" rel="external">知乎：如何通俗的解释交叉熵与相对熵?</a></p>
<p>如何理解信息熵用来表示最短平均编码长度，参考： <a href="http://blog.csdn.net/hearthougan/article/details/77774948" target="_blank" rel="external">如何理解用信息熵来表示最短的平均编码长度</a> </p>
<p>详细的不说了，请看参考链接。</p>
<p>在这里，FP32的tensor就是我们要表达的信息量，FP32也是最佳分布（可以认为最短编码长度32bit），现在要做的是使用INT8 来编码FP32的信息，同时要求INT8编码后差异尽可能最小。考虑两个分布 P（FP32）、Q（INT8）KL散度计算如下：<br>$$<br>\text{KL_divergence(P,Q):= SUM(P[i] * log(P[i] / Q[i] ), i)}<br>$$<br>P，Q分别称为 reference_distribution、 quantize _distribution </p>
<p>实际上这里也说明了每一层的tensor 的 |T| 值都是不一样的。</p>
<p>确定每一层的 |T|值的过程称为 校准（Calibration ）。</p>
<h2 id="2-2-Calibration"><a href="#2-2-Calibration" class="headerlink" title="2.2 Calibration"></a>2.2 Calibration</h2><p>上面已经说了 KL散度越小代表 INT8编码后的信息损失越少。这一节来看看如何根据KL散度寻找最佳INT8分布。其实前面我们也已经提到了，如果要让最后的精度损失不大，是要考虑一些先验知识的，这个先验知识就是每一层在 FP32精度下的激活值分布，只有根据这个才能找到更加合理的 阈值|T|。也就是说首先得有一个以FP32精度训练好的模型。基本上现有的深度学习框架都是默认 FP32精度的，有些模型还支持FP16精度训练，貌似 Caffe2和MXNet是支持FP16的，其他的不太清楚。所以基本上只要没有特别设定，训练出来的模型肯定是 FP32 的。</p>
<p>那激活值分布如何得到？难道我们要将FP32的模型先在所有的测试集（或验证集）上跑一边记录下每一层的FP32激活值，然后再去推断 |T|?</p>
<p>这里的做法是 从验证集 选取一个子集作为校准集（Calibration Dataset ），校准集应该具有代表性，多样性，最好是验证集的一个子集，不应该只是分类类别的一小部分。激活值分布就是从校准集中得到的。</p>
<p>按照<a href="http://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#calibrationset" target="_blank" rel="external">NVIDIA 官方的说法</a>：</p>
<blockquote>
<p>Note: The calibration set must be representative of the input provided to TensorRT at runtime; for example, for image classification networks, it should not consist of images from just a small subset of categories. For ImageNet networks, around 500 calibration images is adequate.</p>
</blockquote>
<p>对于ImageNet 数据集来说 校准集大小一般500张图片就够了（Szymon Migacz的演讲说用1000张），这里有点怀疑也有点震惊，没想到 ImageNet 1000个分类，100多万张图片，500张就够了，不过从2.5节的图表中的结果可以看出500张确实够了。</p>
<p>然后要做的是：</p>
<ul>
<li>首先在 校准集上 进行 FP32 inference 推理；</li>
<li>对于网络的每一层（遍历）：<ul>
<li>收集这一层的激活值，并做 直方图（histograms ），分成几个组别（bins）（官方给的一个说明使用的是2048组），分组是为了下面遍历 |T| 时，减少遍历次数；</li>
<li>对于不同的 阈值 |T| 进行遍历，因为这里 |T|的取值肯定在 第128-2047 组之间，所以就选取每组的中间值进行遍历；<ul>
<li>选取使得 KL_divergence(ref_distr, quant_distr) 取得最小值的 |T|。</li>
</ul>
</li>
</ul>
</li>
<li>返回一系列 |T|值，每一层都有一个 |T|。创建 <strong>CalibrationTable</strong> 。</li>
</ul>
<p>上面解释一下：假设 最后 使得 KL散度最小的|T|值是第200组的中间值，那么就把原来 第 0-200组的 数值线性映射到 0-128之间，超出范围的直接映射到128。</p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/Calibration-process.png" alt="1516636471289"></p>
<p>校准的过程可以参考一下这个：<a href="https://www.jianshu.com/p/43318a3dc715，" target="_blank" rel="external">https://www.jianshu.com/p/43318a3dc715，</a> 这篇文章提供了一个详细的根据KL散度来将原始信息进行编码的例子，包括直方图的使用。跟这里的校准过程极为相像。</p>
<p>下面是一个官方 <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="external">GTC2017 PPT</a> 中给的校准的伪代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//首先分成 2048个组，每组包含多个数值（基本都是小数）</span></div><div class="line">Input: FP32 histogram H with <span class="number">2048</span> bins: bin[ <span class="number">0</span> ], …, bin[ <span class="number">2047</span> ] </div><div class="line">  </div><div class="line">For i in range( 128 , 2048 ): // |T|的取值肯定在 第128-2047 组之间,取每组的中点</div><div class="line">	reference_distribution_P = [ bin[ <span class="number">0</span> ] , ..., bin[ i<span class="number">-1</span> ] ] <span class="comment">// 选取前 i 组构成P，i&gt;=128</span></div><div class="line">	outliers_count = sum( bin[ i ] , bin[ i+<span class="number">1</span> ] , … , bin[ <span class="number">2047</span> ] ) <span class="comment">//边界外的组</span></div><div class="line">	reference_distribution_P[ i<span class="number">-1</span> ] += outliers_count <span class="comment">//边界外的组加到边界P[i-1]上，没有直接丢掉</span></div><div class="line">	P /= sum(P) <span class="comment">// 归一化</span></div><div class="line">      </div><div class="line">    <span class="comment">// 将前面的P（包含i个组，i&gt;=128），映射到 0-128 上，映射后的称为Q，Q包含128个组，</span></div><div class="line">    <span class="comment">// 一个整数是一组</span></div><div class="line">	candidate_distribution_Q = quantize [ bin[ <span class="number">0</span> ], …, bin[ i<span class="number">-1</span> ] ] into <span class="number">128</span> levels</div><div class="line">	</div><div class="line">	<span class="comment">//这时的P（包含i个组，i&gt;=128）和Q向量（包含128个组）的大小是不一样的，无法直接计算二者的KL散度</span></div><div class="line">	<span class="comment">//因此需要将Q扩展为 i 个组，以保证跟P大小一样</span></div><div class="line">	expand candidate_distribution_Q to ‘ i ’ bins </div><div class="line">	</div><div class="line">	Q /= sum(Q) <span class="comment">// 归一化</span></div><div class="line">	<span class="comment">//计算P和Q的KL散度</span></div><div class="line">	divergence[ i ] = KL_divergence( reference_distribution_P, candidate_distribution_Q)</div><div class="line">End For</div><div class="line"></div><div class="line"><span class="comment">//找出 divergence[ i ] 最小的数值，假设 divergence[m] 最小，</span></div><div class="line"><span class="comment">//那么|T|=( m + 0.5 ) * ( width of a bin )</span></div><div class="line">Find index ‘m’ <span class="keyword">for</span> which divergence[ m ] is minimal</div><div class="line"></div><div class="line">threshold = ( m + <span class="number">0.5</span> ) * ( width of a bin )</div></pre></td></tr></table></figure>
<p>解释一下第16行：</p>
<ul>
<li>计算KL散度 KL_divergence(P, Q) 时，要求序列P和Q的长度一致，即 len(P) == len(Q)；</li>
<li>Candidate_distribution_Q 是将 P 线性映射到 128个bins得到的，长度为128。而reference_distribution_P 包含 i （i&gt;=128）个 bins （bin[0] - bin[i-1] ），二者长度不等；</li>
<li>需要将 candidate_distribution_Q 扩展回 i 个bins 然后才能与 i个bins 的 reference_distribution_P计算KL散度。</li>
</ul>
<p>举个简单的栗子：</p>
<ul>
<li><p>假设reference_distribution_P 包含 8 个bins（这里一个bin就只包含一个数据）:</p>
<p>P = [ 1, 0, 2, 3, 5, 3, 1, 7]</p>
</li>
<li><p>我们想把它映射为 2 个bins，于是 4个一组合并：</p>
<p>[1 + 0 + 2 + 3 , 5 + 3 + 1 + 7] = [6, 16]</p>
</li>
<li><p>然后要成比例的 扩展回到 8个组，保留原来是0的组：</p>
<p>Q = [ 6/3, 0, 6/3, 6/3, 16/4, 16/4, 16/4, 16/4] = [ 2, 0, 2, 2, 4, 4, 4, 4]</p>
</li>
<li><p>然后对 P和Q进行标准化：</p>
<p>P /= sum(P) 、Q /= sum(Q)</p>
</li>
<li><p>最后计算散度：</p>
<p>result = KL_divergence(P, Q) </p>
</li>
</ul>
<p>我们来看看 ResNet-152中 res4b30层校准前后的结果对比：</p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/satuation-before-and-after.png" alt="1516638878836"></p>
<ul>
<li>图中那个白线就是 |T|的取值，不过怎么还小于128了，有点没搞明白。</li>
</ul>
<p>再看看其他几种网络的校准情况：</p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/Calibration-result1.jpg" alt="result_1"></p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/Calibration-result2.jpg" alt="result_2"></p>
<h2 id="2-3-DP4A-Dot-Product-of-4-8-bits-Accumulated-to-a-32-bit"><a href="#2-3-DP4A-Dot-Product-of-4-8-bits-Accumulated-to-a-32-bit" class="headerlink" title="2.3 DP4A(Dot Product of 4 8-bits Accumulated to a 32-bit)"></a>2.3 DP4A(<strong>D</strong>ot <strong>P</strong>roduct of <strong>4</strong> 8-bits <strong>A</strong>ccumulated to a 32-bit)</h2><p>TensorRT 进行优化的方式是 DP4A (<strong>D</strong>ot <strong>P</strong>roduct of <strong>4</strong> 8-bits <strong>A</strong>ccumulated to a 32-bit)，如下图：</p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/DP4A.png" alt="1516642345023"></p>
<p>这是PASCAL 系列GPU的硬件指令，INT8卷积就是使用这种方式进行的卷积计算。</p>
<p>这个没搞太明白是怎么回事，参考这篇博客获取详细信息<a href="https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/" target="_blank" rel="external">Mixed-Precision Programming with CUDA 8</a> </p>
<p>下面是 官方 <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="external">GTC2017 PPT</a> 中给的INT8卷积计算的伪代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// I8 input tensors: I8_input, I8_weights, INT8输入tensor</span></div><div class="line"><span class="comment">// I8 output tensors: I8_output， INT8输出tensor</span></div><div class="line"><span class="comment">// F32 bias (original bias from the F32 model),FP32的偏置</span></div><div class="line"><span class="comment">// F32 scaling factors: input_scale, output_scale, weights_scale[K], 这个是前面说的缩放因子sf</span></div><div class="line">I32_gemm_out = I8_input * I8_weights <span class="comment">// Compute INT8 GEMM (DP4A)，卷积计算，INT32输出</span></div><div class="line">F32_gemm_out = (<span class="keyword">float</span>)I32_gemm_out <span class="comment">// Cast I32 GEMM output to F32 float，强制转换为FP32</span></div><div class="line"></div><div class="line"><span class="comment">//前面计算I8_input * I8_weights时，总的缩放系数为 input_scale * weights_scale[K]</span></div><div class="line"><span class="comment">//但是输出的缩放系数为output_scale，所以为了保证缩放程度匹配，要将F32_gemm_out乘以 </span></div><div class="line"><span class="comment">//output_scale / (input_scale * weights_scale[ i ] )</span></div><div class="line">  </div><div class="line"><span class="comment">// At this point we have F32_gemm_out which is scaled by ( input_scale * weights_scale[K] ),</span></div><div class="line"><span class="comment">// but to store the final result in int8 we need to have scale equal to "output_scale", so we have to rescale:</span></div><div class="line"><span class="comment">// (this multiplication is done in F32, *_gemm_out arrays are in NCHW format)</span></div><div class="line">For i in <span class="number">0</span>, ... K<span class="number">-1</span>:</div><div class="line">rescaled_F32_gemm_out[ :, i, :, :] = F32_gemm_out[ :, i, :, :] * [ output_scale /(input_scale * weights_scale[ i ] ) ]</div><div class="line">  </div><div class="line"><span class="comment">//将FP32精度的偏置 乘上缩放因子，加到前面的计算结果中</span></div><div class="line"><span class="comment">// Add bias, to perform addition we have to rescale original F32 bias so that it's scaled with "output_scale"</span></div><div class="line">rescaled_F32_gemm_out _with_bias = rescaled_F32_gemm_out + output_scale * bias</div><div class="line"></div><div class="line"><span class="comment">//ReLU 激活</span></div><div class="line"><span class="comment">// Perform ReLU (in F32)</span></div><div class="line">F32_result = ReLU(rescaled_F32_gemm_out _with_bias)</div><div class="line"></div><div class="line"><span class="comment">//重新转换为 INT8</span></div><div class="line"><span class="comment">// Convert to INT8 and save to global</span></div><div class="line">I8_output = Saturate( Round_to_nearest_integer( F32_result ) )</div></pre></td></tr></table></figure>
<p>它这个INT8卷积的计算是这样的，虽然输入的tensor已经降为 INT8，但是在卷积计算的时候用了DP4A的计算模式，卷积计算完之后是INT32的，然后又要转成 FP32，然后激活，最后再将FP32的转为INT8.</p>
<p>只知道这么计算会快很多，但不知道为什么，详情还是看<a href="https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/" target="_blank" rel="external">Mixed-Precision Programming with CUDA 8</a> 这个吧，我看的也是糊里糊涂的。</p>
<p>不过这个对于tensorRT的使用没啥影响，这个是很底层的东西，涉及到硬件优化。</p>
<h2 id="2-4-Typical-workflow-in-TensorRT"><a href="#2-4-Typical-workflow-in-TensorRT" class="headerlink" title="2.4 Typical workflow in TensorRT"></a>2.4 Typical workflow in TensorRT</h2><p>典型的工作流还是直接使用 <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="external">GTC2017 PPT</a> 原文说法吧：</p>
<ul>
<li>You will need:<ul>
<li>Model trained in FP32.</li>
<li>Calibration dataset.</li>
</ul>
</li>
<li>TensorRT will:<ul>
<li>Run inference in FP32 on calibration dataset.</li>
<li>Collect required statistics.</li>
<li>Run calibration algorithm → optimal scaling factors.</li>
<li>Quantize FP32 weights → INT8.</li>
<li>Generate “CalibrationTable” and INT8 execution engine. </li>
</ul>
</li>
</ul>
<h2 id="2-5-Results-Accuracy-amp-Performance"><a href="#2-5-Results-Accuracy-amp-Performance" class="headerlink" title="2.5 Results - Accuracy &amp; Performance"></a>2.5 Results - Accuracy &amp; Performance</h2><p><strong>精度并没有损失太多</strong></p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/performance-auccary.png" alt="1516641328187"></p>
<p><strong>速度提升还蛮多的，尤其是当 batch_size 大于1时，提升更明显</strong> </p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/performance-speed.png" alt="1516641398718"></p>
<p><strong>TITAN X GPU优化效果</strong></p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/performance-titanx.png" alt="1516642501791"></p>
<p><strong>DRIVE PX 2, dGPU 优化效果</strong></p>
<p><img src="/images/TensorRT-5-int8-calibration.assets/performance-dgpu.png" alt="1516642517725"></p>
<h2 id="2-6-Open-challenges-improvements"><a href="#2-6-Open-challenges-improvements" class="headerlink" title="2.6 Open challenges / improvements"></a>2.6 Open challenges / improvements</h2><p>一些开放式的提升和挑战：</p>
<ul>
<li>Unsigned int8 for activations after ReLU. 无符号 INT8 的映射。</li>
<li>RNNs → open research problem. TensorRT 3.0开始已经支持RNN了。</li>
<li>Fine tuning of saturation thresholds. 对阈值 |T|的 微调方法。</li>
<li>Expose API for accepting custom, user provided scale factors.  开放API，使用户可以自定义 换算系数（比例因子）</li>
</ul>
<p>这几个开放问题还是很值得研究的。</p>
<h1 id="3-Conclusion"><a href="#3-Conclusion" class="headerlink" title="3 Conclusion"></a>3 Conclusion</h1><ul>
<li>介绍了一种自动化，无参数的 FP32  到 INT8 的转换方法；</li>
<li>对称的，不饱和的线性量化，会导致精度损失较大；</li>
<li>通过最小化 KL散度来选择 饱和量化中的 阈值 |T|;</li>
<li>FP32完全可以降低为INT8推理，精度几乎持平，速度有很大提升。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/" target="_blank" rel="external">Why are Eight Bits Enough for Deep Neural Networks?</a></li>
<li><a href="https://towardsdatascience.com/low-precision-inference-with-tensorrt-6eb3cda0730b" target="_blank" rel="external">Low Precision Inference with TensorRT</a></li>
<li><a href="http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=s7310&amp;searchItems=session_id&amp;sessionTopic=&amp;sessionEvent=&amp;sessionYear=&amp;sessionFormat=&amp;submit=&amp;select=" target="_blank" rel="external">GTC 2017 Presentation: 8-Bit Inference with TensorRT PPT和演讲视频</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5#cite_note-1" target="_blank" rel="external">维基百科中文版：相对熵</a> </li>
<li><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank" rel="external">Kullback-Leibler Divergence Explained</a></li>
<li><a href="https://www.jianshu.com/p/43318a3dc715" target="_blank" rel="external">引用4的中文版：如何理解K-L散度（相对熵）</a></li>
<li><a href="https://www.zhihu.com/question/22178202/answer/124805730" target="_blank" rel="external">知乎：信息熵是什么？</a></li>
<li><a href="http://blog.csdn.net/acdreamers/article/details/44657745" target="_blank" rel="external">相对熵（KL散度）</a></li>
<li><a href="https://www.zhihu.com/question/41252833" target="_blank" rel="external">知乎：如何通俗的解释交叉熵与相对熵?</a></li>
<li><a href="http://blog.csdn.net/hearthougan/article/details/77774948" target="_blank" rel="external">如何理解用信息熵来表示最短的平均编码长度</a></li>
<li><a href="http://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html" target="_blank" rel="external">TensorRT Developer Guide</a> </li>
</ol>

      
    </div>
    
    
    

    
      <div>
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">
  <p><strong>本文标题：</strong><a href="/articles/923e2c40/">TensorRT(5)-INT8校准原理</a></p>
  <p><strong>本文作者：</strong><a href="/" title="访问 arleyzhang 的个人博客">arleyzhang</a></p>
  <p><strong>发布时间：</strong>2018年08月31日 - 18:08</p>
  <p><strong>最后更新：</strong>2018年09月03日 - 01:09</p>
  <p><strong>本文链接：</strong><a href="/articles/923e2c40/" title="TensorRT(5)-INT8校准原理">https://arleyzhang.github.io/articles/923e2c40/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://arleyzhang.github.io/articles/923e2c40/"  aria-label="复制成功！"></i></span>
  </p>
  <p><strong>版权声明：</strong> 本文由 arleyzhang 原创，采用 <i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">保留署名-非商业性使用-禁止演绎 4.0-国际许可协议</a>，转载请保留原文链接及作者!</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>

      </div>
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/TensorRT/" rel="tag"><i class="fa fa-tag"></i> TensorRT</a>
          
            <a href="/tags/inference/" rel="tag"><i class="fa fa-tag"></i> inference</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/articles/fda11be6/" rel="next" title="TensorRT(4)-Profiling and 16-bit Inference">
                <i class="fa fa-chevron-left"></i> TensorRT(4)-Profiling and 16-bit Inference
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/articles/95d15d89/" rel="prev" title="TensorRT(6)-INT8 inference">
                TensorRT(6)-INT8 inference <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">arleyzhang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/arleyzhang" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:1186197274@qq.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Low-Precision-Inference"><span class="nav-text">1 Low Precision Inference</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-TensorRT-INT8-Inference"><span class="nav-text">2 TensorRT INT8 Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Quantization"><span class="nav-text">2.1 Quantization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Calibration"><span class="nav-text">2.2 Calibration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-DP4A-Dot-Product-of-4-8-bits-Accumulated-to-a-32-bit"><span class="nav-text">2.3 DP4A(Dot Product of 4 8-bits Accumulated to a 32-bit)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-Typical-workflow-in-TensorRT"><span class="nav-text">2.4 Typical workflow in TensorRT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-Results-Accuracy-amp-Performance"><span class="nav-text">2.5 Results - Accuracy & Performance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-Open-challenges-improvements"><span class="nav-text">2.6 Open challenges / improvements</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Conclusion"><span class="nav-text">3 Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">arleyzhang</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://arleyzhang.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://arleyzhang.github.io/articles/923e2c40/';
          this.page.identifier = 'articles/923e2c40/';
          this.page.title = 'TensorRT(5)-INT8校准原理';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://arleyzhang.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
